{
  "scan_date": "2026-01-14T19:47:03.279386",
  "apps_scanned": 11,
  "results": [
    {
      "app": "open-webui",
      "description": "ChatGPT-like UI",
      "repo": "https://github.com/open-webui/open-webui",
      "python_files": 205,
      "files_scanned": 0,
      "total_findings": 310,
      "critical": 193,
      "high": 54,
      "potential_vulns": [
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/contribution_stats.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/hatch_build.py",
          "line": 23,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'subprocess.run' is used in 'run(' on line 23 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/hatch_build.py",
          "line": 11,
          "category": "LLM08: Excessive Agency",
          "title": "Direct execution of LLM-generated code in 'initialize'",
          "description": "Function 'initialize' on line 11 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM outputs ",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/hatch_build.py",
          "line": 11,
          "category": "LLM09: Overreliance",
          "title": "Direct execution of LLM output in 'initialize'",
          "description": "Function 'initialize' on line 11 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/hatch_build.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/functions.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/tasks.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/config.py",
          "line": 2461,
          "category": "LLM05: Supply Chain Vulnerabilities",
          "title": "Code execution on external content",
          "description": "eval() on non-literal content on line 2461. File fetches external content - HIGH RISK.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/config.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/env.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/constants.py",
          "line": 34,
          "category": "LLM06: Sensitive Information Disclosure",
          "title": "Hardcoded secret detected: Secret Keyword",
          "description": "detect-secrets found a potential Secret Keyword on line 34. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/constants.py",
          "line": 67,
          "category": "LLM06: Sensitive Information Disclosure",
          "title": "Hardcoded secret detected: Secret Keyword",
          "description": "detect-secrets found a potential Secret Keyword on line 67. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/constants.py",
          "line": 80,
          "category": "LLM06: Sensitive Information Disclosure",
          "title": "Hardcoded secret detected: Secret Keyword",
          "description": "detect-secrets found a potential Secret Keyword on line 80. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/constants.py",
          "line": 81,
          "category": "LLM06: Sensitive Information Disclosure",
          "title": "Hardcoded secret detected: Secret Keyword",
          "description": "detect-secrets found a potential Secret Keyword on line 81. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/constants.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/__init__.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/routers/functions.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/routers/files.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/routers/tasks.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/routers/configs.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/routers/users.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/routers/tools.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/routers/groups.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/routers/evaluations.py",
          "line": 156,
          "category": "LLM04: Model Denial of Service",
          "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting, No input length validation, No timeout configuration, No token/context limits",
          "description": "Function '_compute_similarities' on line 156 has 5 DoS risk(s): LLM calls in loops, No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing prote",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/routers/notes.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/routers/prompts.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/routers/auths.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/routers/openai.py",
          "line": 506,
          "category": "LLM04: Model Denial of Service",
          "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits",
          "description": "Function 'get_merged_models' on line 506 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhau",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/routers/scim.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/routers/utils.py",
          "line": 94,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/routers/retrieval.py",
          "line": 1562,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output flows to sql_injection sink",
          "description": "LLM output variable 'items' flows to 'VECTOR_DB_CLIENT.insert' on line 1562 via direct flow. This creates a sql_injection vulnerability.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/routers/retrieval.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/routers/images.py",
          "line": 699,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/routers/chats.py",
          "line": 240,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/routers/chats.py",
          "line": 241,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/routers/chats.py",
          "line": 107,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/routers/chats.py",
          "line": 108,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/routers/memories.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/routers/audio.py",
          "line": 603,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/routers/folders.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/routers/knowledge.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/routers/ollama.py",
          "line": 309,
          "category": "LLM04: Model Denial of Service",
          "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits",
          "description": "Function 'merge_ollama_models_lists' on line 309 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers ",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/routers/pipelines.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/migrations/env.py",
          "line": 34,
          "category": "LLM09: Overreliance",
          "title": "Critical decision without oversight in 'run_migrations_offline'",
          "description": "Function 'run_migrations_offline' on line 34 makes critical financial decisions based on LLM output without human oversight or verification. Action edges detected (HTTP/file/DB/subprocess) - risk of a",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/migrations/env.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/tools/builtin.py",
          "line": 836,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/tools/builtin.py",
          "line": 772,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/internal/db.py",
          "line": 62,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'router.run' is used in 'run(' on line 62 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/internal/wrappers.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/auth.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/misc.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/files.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/task.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/code_interpreter.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/audit.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/models.py",
          "line": 340,
          "category": "LLM04: Model Denial of Service",
          "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
          "description": "Function 'check_model_access' on line 340 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/models.py",
          "line": 364,
          "category": "LLM04: Model Denial of Service",
          "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting, No input length validation, No timeout configuration, No token/context limits",
          "description": "Function 'get_filtered_models' on line 364 has 5 DoS risk(s): LLM calls in loops, No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protect",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/webhook.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/pdf_generator.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/security_headers.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/tools.py",
          "line": 341,
          "category": "LLM05: Supply Chain Vulnerabilities",
          "title": "Network fetch combined with code execution",
          "description": "This file downloads external content (lines [773, 970]) and executes code (lines [341, 536, 867]). This pattern enables remote code execution attacks if the fetched content is not properly validated.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/tools.py",
          "line": 412,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/response.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/logger.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/payload.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/chat.py",
          "line": 266,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/chat.py",
          "line": 271,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/chat.py",
          "line": 227,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/chat.py",
          "line": 217,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/embeddings.py",
          "line": 75,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/redis.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/rate_limit.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/plugin.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/filter.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/channels.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/access_control.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/middleware.py",
          "line": 953,
          "category": "LLM04: Model Denial of Service",
          "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
          "description": "Function 'add_file_context' on line 953 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers t",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/oauth.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/models/functions.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/models/tags.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/models/files.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/models/models.py",
          "line": 415,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output flows to sql_injection sink",
          "description": "LLM output variable 'data' flows to 'update' on line 415 via direct flow. This creates a sql_injection vulnerability.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/models/models.py",
          "line": 414,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/models/models.py",
          "line": 461,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/models/models.py",
          "line": 463,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/models/models.py",
          "line": 471,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/models/users.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/models/tools.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/models/oauth_sessions.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/models/groups.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/models/notes.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/models/prompts.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/models/auths.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/models/messages.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/models/feedbacks.py",
          "line": 257,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/models/chats.py",
          "line": 426,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/models/chats.py",
          "line": 419,
          "category": "LLM04: Model Denial of Service",
          "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
          "description": "Function 'get_message_by_id_and_message_id' on line 419 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections en",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/models/chats.py",
          "line": 410,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/models/chats.py",
          "line": 417,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/models/chats.py",
          "line": 1489,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/models/memories.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/models/channels.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/models/folders.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/models/knowledge.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/storage/provider.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/socket/utils.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/socket/main.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/web/serpapi.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/web/bocha.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/web/searchapi.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/web/mojeek.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/web/tavily.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/web/brave.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/web/serper.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/web/exa.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/web/perplexity_search.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/web/azure.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/web/searxng.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/web/perplexity.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/web/bing.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/web/sougou.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/web/google_pse.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/web/utils.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/web/serpstack.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/web/jina_search.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/web/main.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/web/yacy.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/web/external.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/web/duckduckgo.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/web/ollama.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/web/kagi.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/web/firecrawl.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/web/serply.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/models/colbert.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/models/external.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/vector/type.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/loaders/mistral.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/loaders/youtube.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/loaders/tavily.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/loaders/external_web.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/loaders/mineru.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/loaders/external_document.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/loaders/main.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/loaders/datalab_marker.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/vector/dbs/elasticsearch.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/vector/dbs/qdrant_multitenancy.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/vector/dbs/milvus.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/vector/dbs/opensearch.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/vector/dbs/qdrant.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/vector/dbs/s3vector.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/vector/dbs/pgvector.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/vector/dbs/opengauss.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/vector/dbs/weaviate.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/vector/dbs/milvus_multitenancy.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/vector/dbs/oracle23ai.py",
          "line": 10,
          "category": "LLM06: Sensitive Information Disclosure",
          "title": "Hardcoded secret detected: Secret Keyword",
          "description": "detect-secrets found a potential Secret Keyword on line 10. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/vector/dbs/oracle23ai.py",
          "line": 16,
          "category": "LLM06: Sensitive Information Disclosure",
          "title": "Hardcoded secret detected: Secret Keyword",
          "description": "detect-secrets found a potential Secret Keyword on line 16. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/vector/dbs/oracle23ai.py",
          "line": 20,
          "category": "LLM06: Sensitive Information Disclosure",
          "title": "Hardcoded secret detected: Secret Keyword",
          "description": "detect-secrets found a potential Secret Keyword on line 20. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/vector/dbs/oracle23ai.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/vector/dbs/chroma.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/images/comfyui.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/mcp/client.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/db/access_control.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/telemetry/metrics.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/telemetry/constants.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/telemetry/setup.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/telemetry/instrumentors.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/telemetry/logs.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/internal/migrations/010_migrate_modelfiles_to_models.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/internal/migrations/012_add_tools.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/internal/migrations/014_add_files.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/internal/migrations/001_initial_schema.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/internal/migrations/008_add_memory.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/internal/migrations/015_add_functions.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/internal/migrations/009_add_models.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/migrations/versions/3ab32c4b8f59_update_tags.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/migrations/versions/c29facfe716b_update_file_table_path.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/migrations/versions/b10670c03dd5_update_user_table.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/migrations/versions/38d63c18f30f_add_oauth_session_table.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/migrations/versions/3781e22d8b01_update_message_table.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/migrations/versions/57c599a3cb57_add_channel_table.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/migrations/versions/1af9b942657b_migrate_tags.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/migrations/versions/37f288994c47_add_group_member_table.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/migrations/versions/6a39f3d8e55c_add_knowledge_table.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/migrations/versions/018012973d35_add_indexes.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/migrations/versions/4ace53fd72c8_update_folder_table_datetime.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/migrations/versions/7826ab40b532_update_file_table.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/migrations/versions/af906e964978_add_feedback_table.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/migrations/versions/c69f45358db4_add_folder_table.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/migrations/versions/242a2047eae0_update_chat_table.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/migrations/versions/3e0e00844bb0_add_knowledge_file_table.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/migrations/versions/d31026856c01_update_folder_table_data.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/migrations/versions/922e7a387820_add_group_table.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/migrations/versions/9f0c9cd09105_add_note_table.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/__init__.py",
          "line": 34,
          "category": "LLM08: Excessive Agency",
          "title": "High-risk write/execute operation without confirmation in 'serve'",
          "description": "Function 'serve' on line 34 performs high-risk write/execute operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially ",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/__init__.py",
          "line": 88,
          "category": "LLM08: Excessive Agency",
          "title": "High-risk execute operation without confirmation in 'dev'",
          "description": "Function 'dev' on line 88 performs high-risk execute operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destruct",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/routers/models.py",
          "line": 346,
          "category": "LLM10: Model Theft",
          "title": "Model API without rate limiting in 'get_model_profile_image'",
          "description": "API endpoint 'get_model_profile_image' on line 346 provides model access without rate limiting. This allows attackers to make unlimited queries to extract model knowledge, potentially stealing intelle",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/routers/chats.py",
          "line": 90,
          "category": "LLM10: Model Theft",
          "title": "Model API without rate limiting in 'get_session_user_chat_usage_stats'",
          "description": "API endpoint 'get_session_user_chat_usage_stats' on line 90 provides model access without rate limiting. This allows attackers to make unlimited queries to extract model knowledge, potentially stealin",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/routers/audio.py",
          "line": 954,
          "category": "LLM06: Sensitive Information Disclosure",
          "title": "Sensitive data passed into LLM prompt",
          "description": "Variable 'api_key' containing sensitive data is included in a prompt string on line 954. This can lead to data leakage through model outputs, logs, or training data.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/routers/audio.py",
          "line": 998,
          "category": "LLM06: Sensitive Information Disclosure",
          "title": "Sensitive data passed into LLM prompt",
          "description": "Variable 'api_key' containing sensitive data is included in a prompt string on line 998. This can lead to data leakage through model outputs, logs, or training data.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/migrations/env.py",
          "line": 34,
          "category": "LLM08: Excessive Agency",
          "title": "High-risk execute/network operation without confirmation in 'run_migrations_offline'",
          "description": "Function 'run_migrations_offline' on line 34 performs high-risk execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously e",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/migrations/env.py",
          "line": 58,
          "category": "LLM08: Excessive Agency",
          "title": "High-risk delete/write/execute operation without confirmation in 'run_migrations_online'",
          "description": "Function 'run_migrations_online' on line 58 performs high-risk delete/write/execute operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomous",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/migrations/env.py",
          "line": 58,
          "category": "LLM09: Overreliance",
          "title": "Automated action without confidence threshold in 'run_migrations_online'",
          "description": "Function 'run_migrations_online' on line 58 automatically executes actions based on LLM output without checking confidence thresholds or validating output. Action edges detected - risk of automated ex",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/internal/db.py",
          "line": 55,
          "category": "LLM08: Excessive Agency",
          "title": "High-risk execute/network operation without confirmation in 'handle_peewee_migration'",
          "description": "Function 'handle_peewee_migration' on line 55 performs high-risk execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously ",
          "is_semantic_taint": true
        }
      ]
    },
    {
      "app": "AutoGPT",
      "description": "Autonomous agent",
      "repo": "https://github.com/Significant-Gravitas/AutoGPT",
      "python_files": 769,
      "files_scanned": 0,
      "total_findings": 742,
      "critical": 464,
      "high": 154,
      "potential_vulns": [
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/classic/cli.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/run_tests.py",
          "line": 110,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'subprocess.run' is used in 'subprocess.' on line 110 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application securit",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/run_tests.py",
          "line": 116,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'subprocess.run' is used in 'subprocess.' on line 116 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application securit",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/run_tests.py",
          "line": 42,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'subprocess.run' is used in 'subprocess.' on line 42 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/run_tests.py",
          "line": 10,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'subprocess.run' is used in 'subprocess.' on line 10 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/run_tests.py",
          "line": 7,
          "category": "LLM04: Model Denial of Service",
          "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits",
          "description": "Function 'wait_for_postgres' on line 7 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/run_tests.py",
          "line": 7,
          "category": "LLM08: Excessive Agency",
          "title": "Direct execution of LLM-generated code in 'wait_for_postgres'",
          "description": "Function 'wait_for_postgres' on line 7 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM ou",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/run_tests.py",
          "line": 40,
          "category": "LLM08: Excessive Agency",
          "title": "Direct execution of LLM-generated code in 'run_command'",
          "description": "Function 'run_command' on line 40 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM outputs",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/run_tests.py",
          "line": 48,
          "category": "LLM08: Excessive Agency",
          "title": "Direct execution of LLM-generated code in 'test'",
          "description": "Function 'test' on line 48 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM outputs can ex",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/run_tests.py",
          "line": 7,
          "category": "LLM09: Overreliance",
          "title": "Direct execution of LLM output in 'wait_for_postgres'",
          "description": "Function 'wait_for_postgres' on line 7 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/run_tests.py",
          "line": 40,
          "category": "LLM09: Overreliance",
          "title": "Direct execution of LLM output in 'run_command'",
          "description": "Function 'run_command' on line 40 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/run_tests.py",
          "line": 48,
          "category": "LLM09: Overreliance",
          "title": "Direct execution of LLM output in 'test'",
          "description": "Function 'test' on line 48 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/linter.py",
          "line": 15,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'subprocess.run' is used in 'subprocess.' on line 15 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/linter.py",
          "line": 12,
          "category": "LLM08: Excessive Agency",
          "title": "Direct execution of LLM-generated code in 'run'",
          "description": "Function 'run' on line 12 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM outputs can exe",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/linter.py",
          "line": 12,
          "category": "LLM09: Overreliance",
          "title": "Direct execution of LLM output in 'run'",
          "description": "Function 'run' on line 12 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/autogpt_libs/autogpt_libs/api_key/keysmith.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/autogpt_libs/autogpt_libs/auth/config.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/autogpt_libs/autogpt_libs/auth/jwt_utils.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/autogpt_libs/autogpt_libs/auth/dependencies.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/autogpt_libs/autogpt_libs/rate_limit/middleware.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/autogpt_libs/autogpt_libs/rate_limit/limiter.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/autogpt_libs/autogpt_libs/logging/config.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/autogpt_libs/autogpt_libs/logging/formatters.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/test/load_store_agents.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/cli.py",
          "line": 239,
          "category": "LLM04: Model Denial of Service",
          "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits",
          "description": "Function 'websocket' on line 239 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/check_db.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/check_store_data.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/iteration.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/ai_shortform_video_block.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/google_maps.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/text_to_speech_block.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/spreadsheet.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/rss.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/talking_head.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/code_executor.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/media.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/persistence.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/youtube.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/pinecone.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/ai_image_customizer.py",
          "line": 153,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/codex.py",
          "line": 165,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/code_extraction_block.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/xml_parser.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/io.py",
          "line": 682,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous sql_injection sink",
          "description": "LLM output from 'super().generate_schema' is used in 'SELECT' on line 682 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application securit",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/io.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/medium.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/perplexity.py",
          "line": 149,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential indirect prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Indirect prompt injection: Malicious instructions from external data sources\n\n**Location:** LLM call with pot",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/__init__.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/llm.py",
          "line": 405,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'llm_model.startswith' is used in 'call(' on line 405 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application securit",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/human_in_the_loop.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/screenshotone.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/email_block.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/flux_kontext.py",
          "line": 141,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/data_manipulation.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/basic.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/text.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/sampling.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/agent.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/search.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/ai_condition.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/branching.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/block.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/maths.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/time_blocks.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/reddit.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/mem0.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/ideogram.py",
          "line": 210,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/util/cloud_storage.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/util/timezone_utils.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/util/logging.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/util/metrics.py",
          "line": 75,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/util/truncate.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/util/service.py",
          "line": 136,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'asyncio.run_coroutine_threadsafe(coro, self.shared_event_loop).result' is used in 'run(' on line 136 without sanitization. This creates a command_injection vulnerability where malicio",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/util/service.py",
          "line": 130,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'self.shared_event_loop.run_forever' is used in 'run(' on line 130 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise applic",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/util/encryption.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/util/feature_flag.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/util/decorator.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/util/request.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/util/cache.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/util/type.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/util/test.py",
          "line": 149,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/util/test.py",
          "line": 164,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/util/retry.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/util/file.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/util/dynamic_fields.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/util/text.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/util/clients.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/util/settings.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/util/process.py",
          "line": 65,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'self.run' is used in 'run(' on line 65 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/util/prompt.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/util/virus_scanner.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/util/json.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/util/data.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/cli/oauth_tool.py",
          "line": 391,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output flows to sql_injection sink",
          "description": "LLM output variable 'creds' flows to 'format_sql_insert' on line 391 via direct flow. This creates a sql_injection vulnerability.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/integrations/credentials_store.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/integrations/providers.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/integrations/ayrshare.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/integrations/creds_manager.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/sdk/provider.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/sdk/registry.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/sdk/cost_integration.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/sdk/__init__.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/sdk/builder.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/api/conn_manager.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/api/ws_api.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/api/model.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/api/rest_api.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/executor/database.py",
          "line": 91,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/executor/database.py",
          "line": 96,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/executor/activity_status_generator.py",
          "line": 338,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/executor/cluster_lock.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/executor/manager.py",
          "line": 1060,
          "category": "LLM01: Prompt Injection",
          "title": "User input 'log_metadata' embedded in LLM prompt",
          "description": "User input parameter 'log_metadata' is directly passed to LLM API call 'asyncio.run_coroutine_threadsafe'. This is a high-confidence prompt injection vector.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/executor/manager.py",
          "line": 1551,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output flows to command_injection sink",
          "description": "LLM output variable 'run_channel' flows to 'RuntimeError' on line 1551 via direct flow. This creates a command_injection vulnerability.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/executor/manager.py",
          "line": 1639,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output flows to command_injection sink",
          "description": "LLM output variable 'channel' flows to 'channel.connection.add_callback_threadsafe' on line 1639 via direct flow. This creates a command_injection vulnerability.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/executor/manager.py",
          "line": 1635,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output flows to command_injection sink",
          "description": "LLM output variable 'channel' flows to 'channel.connection.add_callback_threadsafe' on line 1635 via direct flow. This creates a command_injection vulnerability.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/executor/manager.py",
          "line": 1854,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output flows to command_injection sink",
          "description": "LLM output variable 'run_channel' flows to 'run_channel.connection.add_callback_threadsafe' on line 1854 via direct flow. This creates a command_injection vulnerability.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/executor/manager.py",
          "line": 876,
          "category": "LLM08: Excessive Agency",
          "title": "Direct execution of LLM-generated code in '_on_graph_execution'",
          "description": "Function '_on_graph_execution' on line 876 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LL",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/executor/manager.py",
          "line": 876,
          "category": "LLM09: Overreliance",
          "title": "Direct execution of LLM output in '_on_graph_execution'",
          "description": "Function '_on_graph_execution' on line 876 directly executes LLM-generated code using exec(. This is extremely dangerous and allows arbitrary code execution.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/monitoring/instrumentation.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/monitoring/block_error_monitor.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/monitoring/late_execution_monitor.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/monitoring/accuracy_monitor.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/data/db.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/data/user.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/data/execution.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/data/graph.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/data/includes.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/data/human_review.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/data/model.py",
          "line": 509,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/data/event_bus.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/data/redis_client.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/data/generate_data.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/data/rabbitmq.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/data/dynamic_fields.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/data/notifications.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/data/analytics.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/data/integrations.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/data/credit.py",
          "line": 1311,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/usecases/reddit_marketing.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/usecases/sample.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/usecases/block_autogen.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/notifications/email.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/data/auth/api_key.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/data/auth/oauth.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/executor/automod/manager.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/api/features/v1.py",
          "line": 513,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/api/features/v1.py",
          "line": 529,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/api/features/v1.py",
          "line": 540,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/api/features/v1.py",
          "line": 565,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/api/features/v1.py",
          "line": 658,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/api/features/v1.py",
          "line": 676,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/api/features/v1.py",
          "line": 959,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/api/features/v1.py",
          "line": 500,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/api/features/v1.py",
          "line": 568,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/api/features/v1.py",
          "line": 570,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/api/features/v1.py",
          "line": 639,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/api/features/analytics.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/api/features/oauth.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/api/utils/cors.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/api/utils/api_key_auth.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/api/external/middleware.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/api/external/v1/tools.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/api/external/v1/integrations.py",
          "line": 289,
          "category": "LLM06: Sensitive Information Disclosure",
          "title": "Hardcoded secret detected: Secret Keyword",
          "description": "detect-secrets found a potential Secret Keyword on line 289. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/api/external/v1/integrations.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/api/external/v1/routes.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/api/features/postmark/models.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/api/features/postmark/postmark.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/api/features/chat/service.py",
          "line": 359,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/api/features/chat/config.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/api/features/chat/model.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/api/features/chat/response_model.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/api/features/chat/routes.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/api/features/admin/execution_analytics_routes.py",
          "line": 373,
          "category": "LLM05: Supply Chain Vulnerabilities",
          "title": "Dynamic model path from request input",
          "description": "Model path determined by request input on line 373. Allowing external control of model paths enables attackers to load malicious models or access unauthorized model files.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/api/features/admin/store_admin_routes.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/api/features/library/model.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/api/features/otto/service.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/api/features/builder/db.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/api/features/builder/routes.py",
          "line": 197,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/api/features/store/media.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/api/features/store/image_gen.py",
          "line": 76,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/api/features/store/image_gen.py",
          "line": 138,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/api/features/executions/review/model.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/api/features/executions/review/routes.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/api/features/library/routes/presets.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/api/features/library/routes/agents.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/api/features/chat/tools/find_agent.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/api/features/chat/tools/models.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/api/features/chat/tools/run_agent.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/api/features/chat/tools/utils.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/integrations/oauth/discord.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/integrations/oauth/__init__.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/integrations/oauth/google.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/integrations/oauth/todoist.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/integrations/oauth/twitter.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/integrations/oauth/notion.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/integrations/oauth/github.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/integrations/webhooks/_base.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/integrations/webhooks/graph_lifecycle_hooks.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/integrations/webhooks/slant3d.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/integrations/webhooks/utils.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/integrations/webhooks/compass.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/integrations/webhooks/github.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/twitter/_builders.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/twitter/_mappers.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/twitter/_serializer.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/twitter/_auth.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/bannerbear/text_overlay.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/discord/_api.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/discord/bot_blocks.py",
          "line": 155,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/discord/_auth.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/dataforseo/_api.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/dataforseo/related_keywords.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/dataforseo/_config.py",
          "line": 12,
          "category": "LLM06: Sensitive Information Disclosure",
          "title": "Hardcoded secret detected: Secret Keyword",
          "description": "detect-secrets found a potential Secret Keyword on line 12. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/dataforseo/keyword_suggestions.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/wordpress/_oauth.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/wordpress/_config.py",
          "line": 17,
          "category": "LLM06: Sensitive Information Disclosure",
          "title": "Hardcoded secret detected: Secret Keyword",
          "description": "detect-secrets found a potential Secret Keyword on line 17. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/firecrawl/_api.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/baas/bots.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/baas/_api.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/google/sheets.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/google/docs.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/google/_drive.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/google/calendar.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/google/gmail.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/google/_auth.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/linear/issues.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/linear/_api.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/linear/_oauth.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/linear/_config.py",
          "line": 53,
          "category": "LLM06: Sensitive Information Disclosure",
          "title": "Hardcoded secret detected: Secret Keyword",
          "description": "detect-secrets found a potential Secret Keyword on line 53. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/linear/_config.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/exa/webhook_blocks.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/exa/websets_items.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/exa/_webhook.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/exa/contents.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/exa/research.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/exa/websets_monitor.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/exa/websets.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/exa/websets_enrichment.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/exa/websets_import_export.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/exa/search.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/exa/similar.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/exa/websets_polling.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/exa/helpers.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/exa/websets_search.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/exa/code_context.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/fal/ai_video_generator.py",
          "line": 214,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/fal/_auth.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/ayrshare/post_to_instagram.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/ayrshare/post_to_threads.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/ayrshare/post_to_telegram.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/ayrshare/post_to_bluesky.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/ayrshare/post_to_youtube.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/ayrshare/post_to_gmb.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/ayrshare/post_to_tiktok.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/ayrshare/post_to_linkedin.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/ayrshare/post_to_facebook.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/ayrshare/post_to_snapchat.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/ayrshare/post_to_x.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/ayrshare/post_to_pinterest.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/ayrshare/post_to_reddit.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/slant3d/_api.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/slant3d/order.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/slant3d/base.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/notion/read_database.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/notion/_api.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/notion/create_page.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/notion/search.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/notion/read_page_markdown.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/notion/_auth.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/hubspot/company.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/hubspot/_auth.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/hubspot/contact.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/hubspot/engagement.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/enrichlayer/_api.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/enrichlayer/linkedin.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/airtable/_webhook.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/airtable/_api.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/airtable/records.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/airtable/_oauth.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/airtable/_config.py",
          "line": 29,
          "category": "LLM06: Sensitive Information Disclosure",
          "title": "Hardcoded secret detected: Secret Keyword",
          "description": "detect-secrets found a potential Secret Keyword on line 29. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/airtable/bases.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/airtable/schema.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/todoist/labels.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/todoist/_types.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/todoist/tasks.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/todoist/sections.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/todoist/projects.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/todoist/_auth.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/todoist/comments.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/system/library_operations.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/system/store_operations.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/stagehand/blocks.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/nvidia/deepfake.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/nvidia/_auth.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/github/issues.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/github/reviews.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/github/_api.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/github/ci.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/github/checks.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/github/statuses.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/github/repo.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/github/pull_requests.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/github/_auth.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/apollo/models.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/apollo/_api.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/apollo/people.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/apollo/_auth.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/smartlead/models.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/smartlead/_api.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/smartlead/campaign.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/smartlead/_auth.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/replicate/_helper.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/replicate/flux_advanced.py",
          "line": 158,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/replicate/_auth.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/jina/chunking.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/jina/fact_checker.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/jina/embeddings.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/jina/search.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/jina/_auth.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/zerobounce/_auth.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/generic_webhook/_webhook.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/twitter/lists/list_tweets_lookup.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/twitter/lists/list_members.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/twitter/lists/list_follows.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/twitter/lists/pinned_lists.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/twitter/lists/manage_lists.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/twitter/lists/list_lookup.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/twitter/spaces/spaces_lookup.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/twitter/spaces/search_spaces.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/twitter/tweets/timeline.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/twitter/tweets/tweet_lookup.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/twitter/tweets/quote.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/twitter/tweets/like.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/twitter/tweets/bookmark.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/twitter/tweets/retweet.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/twitter/tweets/hide.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/twitter/tweets/manage.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/twitter/users/follows.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/twitter/users/mutes.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/twitter/users/blocks.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/twitter/users/user_lookup.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/.github/workflows/scripts/check_actions_status.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/.github/workflows/scripts/get_package_version_from_lockfile.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/classic/original_autogpt/setup.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/classic/forge/forge/app.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/classic/forge/forge/file_storage/gcs.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/classic/forge/forge/file_storage/local.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/classic/forge/forge/file_storage/__init__.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/classic/forge/forge/file_storage/s3.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/classic/forge/forge/file_storage/base.py",
          "line": 270,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous sql_injection sink",
          "description": "LLM output from 'asyncio.get_event_loop().run_until_complete' is used in 'DELETE' on line 270 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/classic/forge/forge/file_storage/base.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/classic/forge/forge/utils/exceptions.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/classic/forge/forge/utils/file_operations.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/classic/forge/forge/utils/url_validator.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/classic/forge/forge/agent/components.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/classic/forge/forge/agent/base.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/classic/forge/forge/models/config.py",
          "line": 183,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/classic/forge/forge/models/config.py",
          "line": 202,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/classic/forge/forge/models/config.py",
          "line": 230,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/classic/forge/forge/models/action.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/classic/forge/forge/models/providers.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/classic/forge/forge/models/json_schema.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/classic/forge/forge/speech/macos_tts.py",
          "line": 18,
          "category": "LLM01: Prompt Injection",
          "title": "User input 'text' embedded in LLM prompt",
          "description": "User input parameter 'text' is directly passed to LLM API call 'subprocess.run'. This is a high-confidence prompt injection vector.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/classic/forge/forge/speech/macos_tts.py",
          "line": 20,
          "category": "LLM01: Prompt Injection",
          "title": "User input 'text' embedded in LLM prompt",
          "description": "User input parameter 'text' is directly passed to LLM API call 'subprocess.run'. This is a high-confidence prompt injection vector.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/classic/forge/forge/speech/macos_tts.py",
          "line": 22,
          "category": "LLM01: Prompt Injection",
          "title": "User input 'text' embedded in LLM prompt",
          "description": "User input parameter 'text' is directly passed to LLM API call 'subprocess.run'. This is a high-confidence prompt injection vector.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/classic/forge/forge/speech/macos_tts.py",
          "line": 20,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'subprocess.run' is used in 'subprocess.' on line 20 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/classic/forge/forge/speech/macos_tts.py",
          "line": 22,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'subprocess.run' is used in 'subprocess.' on line 22 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/classic/forge/forge/speech/macos_tts.py",
          "line": 15,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** Function with user ",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/classic/forge/forge/speech/say.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/classic/forge/forge/speech/stream_elements_speech.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/classic/forge/forge/speech/eleven_labs.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/classic/forge/forge/content_processing/text.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/classic/forge/forge/json/parsing.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/classic/forge/forge/command/command.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/classic/forge/forge/command/decorator.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/classic/forge/forge/agent_protocol/api_router.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/classic/forge/forge/agent_protocol/agent.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/classic/forge/forge/logging/config.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/classic/forge/forge/logging/formatters.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/classic/forge/forge/agent_protocol/database/db.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/classic/forge/forge/agent_protocol/models/task.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/classic/forge/forge/components/image_gen/image_gen.py",
          "line": 97,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/classic/forge/forge/components/image_gen/image_gen.py",
          "line": 103,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/classic/forge/forge/components/image_gen/image_gen.py",
          "line": 108,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/classic/forge/forge/components/image_gen/image_gen.py",
          "line": 196,
          "category": "LLM01: Prompt Injection",
          "title": "User input 'prompt' embedded in LLM prompt",
          "description": "User input parameter 'prompt' is directly passed to LLM API call 'OpenAI(api_key=self.openai_credentials.api_key.get_secret_value(), organization=self.openai_credentials.organization.get_secret_value(",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/classic/forge/forge/components/image_gen/image_gen.py",
          "line": 80,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** Function with user ",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/classic/forge/forge/components/image_gen/image_gen.py",
          "line": 171,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** Function with user ",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/classic/forge/forge/components/context/context_item.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/classic/forge/forge/components/context/context.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/classic/forge/forge/components/web/selenium.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/classic/forge/forge/components/web/search.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/classic/forge/forge/components/user_interaction/user_interaction.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/classic/forge/forge/components/action_history/action_history.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/classic/forge/forge/components/action_history/model.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/classic/forge/forge/components/watchdog/watchdog.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/classic/forge/forge/components/file_manager/file_manager.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/classic/forge/forge/components/code_executor/code_executor.py",
          "line": 219,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output flows to code_execution sink",
          "description": "LLM output variable 'result' flows to 'CodeExecutionError' on line 219 via direct flow. This creates a code_execution vulnerability.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/classic/forge/forge/components/code_executor/code_executor.py",
          "line": 179,
          "category": "LLM07: Insecure Plugin Design",
          "title": "Insecure tool function 'execute_python_file' executes dangerous operations",
          "description": "Tool function 'execute_python_file' on line 179 takes LLM output as a parameter and performs dangerous operations (shell_exec) without proper validation. Attackers can craft malicious LLM outputs to e",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/classic/forge/forge/components/code_executor/code_executor.py",
          "line": 179,
          "category": "LLM08: Excessive Agency",
          "title": "Direct execution of LLM-generated code in 'execute_python_file'",
          "description": "Function 'execute_python_file' on line 179 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LL",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/classic/forge/forge/components/code_executor/code_executor.py",
          "line": 261,
          "category": "LLM08: Excessive Agency",
          "title": "Direct execution of LLM-generated code in 'execute_shell'",
          "description": "Function 'execute_shell' on line 261 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM outp",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/classic/forge/forge/components/code_executor/code_executor.py",
          "line": 179,
          "category": "LLM09: Overreliance",
          "title": "Direct execution of LLM output in 'execute_python_file'",
          "description": "Function 'execute_python_file' on line 179 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/classic/forge/forge/components/code_executor/code_executor.py",
          "line": 261,
          "category": "LLM09: Overreliance",
          "title": "Direct execution of LLM output in 'execute_shell'",
          "description": "Function 'execute_shell' on line 261 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/classic/forge/forge/components/git_operations/git_operations.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/classic/forge/forge/llm/prompting/utils.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/classic/forge/forge/llm/providers/_openai_base.py",
          "line": 348,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/classic/forge/forge/llm/providers/multi.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/classic/forge/forge/llm/providers/groq.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/classic/forge/forge/llm/providers/openai.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/classic/forge/forge/llm/providers/anthropic.py",
          "line": 453,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/classic/forge/forge/llm/providers/schema.py",
          "line": 222,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous sql_injection sink",
          "description": "LLM output from 'self.usage_per_model.values' is used in 'UPDATE' on line 222 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application sec",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/classic/forge/forge/llm/providers/schema.py",
          "line": 222,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/classic/forge/forge/llm/providers/schema.py",
          "line": 226,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/classic/forge/forge/llm/providers/llamafile/llamafile.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/classic/original_autogpt/scripts/git_log_to_release_notes.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/classic/original_autogpt/agbenchmark_config/analyze_reports.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/classic/original_autogpt/scripts/llamafile/serve.py",
          "line": 126,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'subprocess.run' is used in 'subprocess.' on line 126 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application securit",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/classic/original_autogpt/scripts/llamafile/serve.py",
          "line": 105,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'subprocess.run' is used in 'subprocess.' on line 105 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application securit",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/classic/original_autogpt/scripts/llamafile/serve.py",
          "line": 115,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'subprocess.run' is used in 'subprocess.' on line 115 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application securit",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/classic/original_autogpt/scripts/llamafile/serve.py",
          "line": 105,
          "category": "LLM05: Supply Chain Vulnerabilities",
          "title": "Network fetch combined with code execution",
          "description": "This file downloads external content (lines [148]) and executes code (lines [105, 110, 115]). This pattern enables remote code execution attacks if the fetched content is not properly validated.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/classic/original_autogpt/scripts/llamafile/serve.py",
          "line": 44,
          "category": "LLM08: Excessive Agency",
          "title": "Direct execution of LLM-generated code in 'main'",
          "description": "Function 'main' on line 44 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM outputs can ex",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/classic/original_autogpt/scripts/llamafile/serve.py",
          "line": 44,
          "category": "LLM09: Overreliance",
          "title": "Direct execution of LLM output in 'main'",
          "description": "Function 'main' on line 44 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/classic/original_autogpt/autogpt/agent_factory/profile_generator.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/classic/original_autogpt/autogpt/agent_factory/configurators.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/classic/original_autogpt/autogpt/app/config.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/classic/original_autogpt/autogpt/app/setup.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/classic/original_autogpt/autogpt/app/utils.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/classic/original_autogpt/autogpt/app/main.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/classic/original_autogpt/autogpt/app/agent_protocol_server.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/classic/original_autogpt/autogpt/agents/agent_manager.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/classic/original_autogpt/autogpt/agents/prompt_strategies/one_shot.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/classic/benchmark/agbenchmark/agent_api_interface.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/classic/benchmark/agbenchmark/config.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/classic/benchmark/agbenchmark/app.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/classic/benchmark/agbenchmark/main.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/classic/benchmark/agbenchmark/__main__.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/classic/benchmark/reports/send_to_googledrive.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/classic/benchmark/reports/json_to_base_64.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/classic/benchmark/reports/match_records.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/classic/benchmark/agbenchmark/challenges/__init__.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/classic/benchmark/agbenchmark/challenges/builtin.py",
          "line": 405,
          "category": "LLM01: Prompt Injection",
          "title": "User input 'content' embedded in LLM prompt",
          "description": "User input 'content' flows to LLM call via format_call in variable 'prompt'. Function 'score_result_with_llm' may be vulnerable to prompt injection attacks.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/classic/benchmark/agbenchmark/challenges/builtin.py",
          "line": 332,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'subprocess.run' is used in 'subprocess.' on line 332 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application securit",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/classic/benchmark/agbenchmark/challenges/builtin.py",
          "line": 304,
          "category": "LLM08: Excessive Agency",
          "title": "Direct execution of LLM-generated code in 'get_outputs_for_eval'",
          "description": "Function 'get_outputs_for_eval' on line 304 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy L",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/classic/benchmark/agbenchmark/challenges/builtin.py",
          "line": 304,
          "category": "LLM09: Overreliance",
          "title": "Direct execution of LLM output in 'get_outputs_for_eval'",
          "description": "Function 'get_outputs_for_eval' on line 304 directly executes LLM-generated code using eval(, subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/classic/benchmark/agbenchmark/utils/logging.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/classic/benchmark/agbenchmark/utils/get_data_from_helicone.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/classic/benchmark/agbenchmark/utils/data_types.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/classic/benchmark/agbenchmark/utils/prompts.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/classic/benchmark/agbenchmark/utils/utils.py",
          "line": 133,
          "category": "LLM04: Model Denial of Service",
          "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits",
          "description": "Function 'pretty_print_model' on line 133 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exha",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/classic/benchmark/agbenchmark/reports/ReportManager.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/classic/benchmark/agbenchmark/reports/processing/get_files.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/classic/benchmark/agbenchmark/reports/processing/process_report.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/classic/benchmark/agbenchmark/reports/processing/graphs.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/classic/benchmark/agbenchmark/reports/processing/gen_combined_chart.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/classic/benchmark/agbenchmark/utils/dependencies/util.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/classic/benchmark/agbenchmark/utils/dependencies/graphs.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/classic/benchmark/agbenchmark/challenges/verticals/code/4_url_shortener/artifacts_out/url_shortener.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/classic/benchmark/agbenchmark/challenges/verticals/code/6_battleship/artifacts_out/battleship.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/classic/benchmark/agbenchmark/challenges/verticals/code/1_three_sum/custom_python/test.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/classic/benchmark/agbenchmark/challenges/verticals/code/3_file_organizer/artifacts_out/organize_files.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/classic/benchmark/agbenchmark/challenges/verticals/code/2_password_generator/custom_python/test.py",
          "line": 8,
          "category": "LLM04: Model Denial of Service",
          "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits",
          "description": "Function 'test_password_length' on line 8 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exha",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/classic/benchmark/agbenchmark/challenges/verticals/code/2_password_generator/custom_python/test.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/classic/benchmark/agbenchmark/challenges/verticals/code/5_tic_tac_toe/artifacts_out/tic_tac_toe.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/classic/benchmark/agbenchmark/challenges/library/ethereum/check_price/artifacts_in/test.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/classic/benchmark/agbenchmark/challenges/library/ethereum/check_price/artifacts_out/test.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/cli.py",
          "line": 239,
          "category": "LLM08: Excessive Agency",
          "title": "High-risk execute/network operation without confirmation in 'websocket'",
          "description": "Function 'websocket' on line 239 performs high-risk execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/talking_head.py",
          "line": 110,
          "category": "LLM06: Sensitive Information Disclosure",
          "title": "Hardcoded secret detected: Base64 High Entropy String",
          "description": "detect-secrets found a potential Base64 High Entropy String on line 110. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/medium.py",
          "line": 131,
          "category": "LLM06: Sensitive Information Disclosure",
          "title": "Hardcoded secret detected: Hex High Entropy String",
          "description": "detect-secrets found a potential Hex High Entropy String on line 131. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/perplexity.py",
          "line": 140,
          "category": "LLM06: Sensitive Information Disclosure",
          "title": "Sensitive information in system prompt",
          "description": "System prompt contains sensitive keyword 'secret' on line 140. Confidential business logic, pricing, or internal policies in system prompts can be extracted through prompt injection attacks.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/llm.py",
          "line": 166,
          "category": "LLM06: Sensitive Information Disclosure",
          "title": "Hardcoded Generic API Key detected in assignment",
          "description": "Hardcoded Generic API Key found in assignment on line 166. Hardcoded secrets in source code pose a critical security risk as they can be extracted by anyone with access to the codebase, version contro",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/blocks/llm.py",
          "line": 514,
          "category": "LLM06: Sensitive Information Disclosure",
          "title": "Sensitive information in system prompt",
          "description": "System prompt contains sensitive keyword 'secret' on line 514. Confidential business logic, pricing, or internal policies in system prompts can be extracted through prompt injection attacks.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/util/service.py",
          "line": 128,
          "category": "LLM08: Excessive Agency",
          "title": "High-risk execute operation without confirmation in '_run_shared_event_loop'",
          "description": "Function '_run_shared_event_loop' on line 128 performs high-risk execute operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute ",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/util/service.py",
          "line": 135,
          "category": "LLM08: Excessive Agency",
          "title": "High-risk execute operation without confirmation in 'run_and_wait'",
          "description": "Function 'run_and_wait' on line 135 performs high-risk execute operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentiall",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/util/service.py",
          "line": 295,
          "category": "LLM08: Excessive Agency",
          "title": "High-risk execute operation without confirmation in '__start_fastapi'",
          "description": "Function '__start_fastapi' on line 295 performs high-risk execute operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potenti",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpqrk971uv/AutoGPT/autogpt_platform/backend/backend/util/service.py",
          "line": 243,
          "category": "LLM10: Model Theft",
          "title": "Model API without rate limiting in '_create_fastapi_endpoint'",
          "description": "API endpoint '_create_fastapi_endpoint' on line 243 provides model access without rate limiting. This allows attackers to make unlimited queries to extract model knowledge, potentially stealing intell",
          "is_semantic_taint": true
        }
      ]
    },
    {
      "app": "gpt-engineer",
      "description": "Code generation agent",
      "repo": "https://github.com/gpt-engineer-org/gpt-engineer",
      "python_files": 84,
      "files_scanned": 0,
      "total_findings": 75,
      "critical": 60,
      "high": 4,
      "potential_vulns": [
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpdpty9v74/gpt-engineer/docs/create_api_rst.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpdpty9v74/gpt-engineer/docs/conf.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpdpty9v74/gpt-engineer/scripts/print_chat.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpdpty9v74/gpt-engineer/scripts/legacy_benchmark.py",
          "line": 83,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'subprocess.run' is used in 'run(' on line 83 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpdpty9v74/gpt-engineer/scripts/legacy_benchmark.py",
          "line": 22,
          "category": "LLM08: Excessive Agency",
          "title": "Direct execution of LLM-generated code in 'main'",
          "description": "Function 'main' on line 22 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM outputs can ex",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpdpty9v74/gpt-engineer/scripts/legacy_benchmark.py",
          "line": 22,
          "category": "LLM09: Overreliance",
          "title": "Direct execution of LLM output in 'main'",
          "description": "Function 'main' on line 22 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpdpty9v74/gpt-engineer/scripts/legacy_benchmark.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpdpty9v74/gpt-engineer/gpt_engineer/benchmark/run.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpdpty9v74/gpt-engineer/gpt_engineer/benchmark/__main__.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpdpty9v74/gpt-engineer/gpt_engineer/core/files_dict.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpdpty9v74/gpt-engineer/gpt_engineer/core/git.py",
          "line": 27,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'subprocess.run' is used in 'run(' on line 27 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpdpty9v74/gpt-engineer/gpt_engineer/core/git.py",
          "line": 55,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'subprocess.run' is used in 'run(' on line 55 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpdpty9v74/gpt-engineer/gpt_engineer/core/git.py",
          "line": 59,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'subprocess.run' is used in 'run(' on line 59 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpdpty9v74/gpt-engineer/gpt_engineer/core/git.py",
          "line": 63,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'subprocess.run' is used in 'subprocess.' on line 63 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpdpty9v74/gpt-engineer/gpt_engineer/core/git.py",
          "line": 16,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'subprocess.run' is used in 'run(' on line 16 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpdpty9v74/gpt-engineer/gpt_engineer/core/git.py",
          "line": 32,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'subprocess.run' is used in 'run(' on line 32 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpdpty9v74/gpt-engineer/gpt_engineer/core/git.py",
          "line": 45,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'subprocess.run' is used in 'run(' on line 45 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpdpty9v74/gpt-engineer/gpt_engineer/core/git.py",
          "line": 14,
          "category": "LLM08: Excessive Agency",
          "title": "Direct execution of LLM-generated code in 'is_git_repo'",
          "description": "Function 'is_git_repo' on line 14 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM outputs",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpdpty9v74/gpt-engineer/gpt_engineer/core/git.py",
          "line": 26,
          "category": "LLM08: Excessive Agency",
          "title": "Direct execution of LLM-generated code in 'init_git_repo'",
          "description": "Function 'init_git_repo' on line 26 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM outpu",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpdpty9v74/gpt-engineer/gpt_engineer/core/git.py",
          "line": 30,
          "category": "LLM08: Excessive Agency",
          "title": "Direct execution of LLM-generated code in 'has_uncommitted_changes'",
          "description": "Function 'has_uncommitted_changes' on line 30 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpdpty9v74/gpt-engineer/gpt_engineer/core/git.py",
          "line": 41,
          "category": "LLM08: Excessive Agency",
          "title": "Direct execution of LLM-generated code in 'filter_files_with_uncommitted_changes'",
          "description": "Function 'filter_files_with_uncommitted_changes' on line 41 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where mali",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpdpty9v74/gpt-engineer/gpt_engineer/core/git.py",
          "line": 54,
          "category": "LLM08: Excessive Agency",
          "title": "Direct execution of LLM-generated code in 'stage_files'",
          "description": "Function 'stage_files' on line 54 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM outputs",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpdpty9v74/gpt-engineer/gpt_engineer/core/git.py",
          "line": 58,
          "category": "LLM08: Excessive Agency",
          "title": "Direct execution of LLM-generated code in 'filter_by_gitignore'",
          "description": "Function 'filter_by_gitignore' on line 58 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpdpty9v74/gpt-engineer/gpt_engineer/core/git.py",
          "line": 14,
          "category": "LLM09: Overreliance",
          "title": "Direct execution of LLM output in 'is_git_repo'",
          "description": "Function 'is_git_repo' on line 14 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpdpty9v74/gpt-engineer/gpt_engineer/core/git.py",
          "line": 26,
          "category": "LLM09: Overreliance",
          "title": "Direct execution of LLM output in 'init_git_repo'",
          "description": "Function 'init_git_repo' on line 26 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpdpty9v74/gpt-engineer/gpt_engineer/core/git.py",
          "line": 30,
          "category": "LLM09: Overreliance",
          "title": "Critical decision without oversight in 'has_uncommitted_changes'",
          "description": "Function 'has_uncommitted_changes' on line 30 makes critical data_modification decisions based on LLM output without human oversight or verification. Action edges detected (HTTP/file/DB/subprocess) - ",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpdpty9v74/gpt-engineer/gpt_engineer/core/git.py",
          "line": 41,
          "category": "LLM09: Overreliance",
          "title": "Critical decision without oversight in 'filter_files_with_uncommitted_changes'",
          "description": "Function 'filter_files_with_uncommitted_changes' on line 41 makes critical data_modification decisions based on LLM output without human oversight or verification. Action edges detected (HTTP/file/DB/",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpdpty9v74/gpt-engineer/gpt_engineer/core/git.py",
          "line": 54,
          "category": "LLM09: Overreliance",
          "title": "Direct execution of LLM output in 'stage_files'",
          "description": "Function 'stage_files' on line 54 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpdpty9v74/gpt-engineer/gpt_engineer/core/git.py",
          "line": 58,
          "category": "LLM09: Overreliance",
          "title": "Direct execution of LLM output in 'filter_by_gitignore'",
          "description": "Function 'filter_by_gitignore' on line 58 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpdpty9v74/gpt-engineer/gpt_engineer/core/git.py",
          "line": 59,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpdpty9v74/gpt-engineer/gpt_engineer/core/chat_to_files.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpdpty9v74/gpt-engineer/gpt_engineer/core/ai.py",
          "line": 287,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpdpty9v74/gpt-engineer/gpt_engineer/core/project_config.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpdpty9v74/gpt-engineer/gpt_engineer/core/linting.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpdpty9v74/gpt-engineer/gpt_engineer/core/token_usage.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpdpty9v74/gpt-engineer/gpt_engineer/core/diff.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpdpty9v74/gpt-engineer/gpt_engineer/applications/cli/collect.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpdpty9v74/gpt-engineer/gpt_engineer/applications/cli/cli_agent.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpdpty9v74/gpt-engineer/gpt_engineer/applications/cli/file_selector.py",
          "line": 236,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'subprocess.run' is used in 'run(' on line 236 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpdpty9v74/gpt-engineer/gpt_engineer/applications/cli/file_selector.py",
          "line": 243,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'subprocess.run' is used in 'run(' on line 243 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpdpty9v74/gpt-engineer/gpt_engineer/applications/cli/file_selector.py",
          "line": 212,
          "category": "LLM04: Model Denial of Service",
          "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits",
          "description": "Function 'open_with_default_editor' on line 212 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers t",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpdpty9v74/gpt-engineer/gpt_engineer/applications/cli/file_selector.py",
          "line": 212,
          "category": "LLM08: Excessive Agency",
          "title": "Direct execution of LLM-generated code in 'open_with_default_editor'",
          "description": "Function 'open_with_default_editor' on line 212 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or bug",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpdpty9v74/gpt-engineer/gpt_engineer/applications/cli/file_selector.py",
          "line": 212,
          "category": "LLM09: Overreliance",
          "title": "Direct execution of LLM output in 'open_with_default_editor'",
          "description": "Function 'open_with_default_editor' on line 212 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpdpty9v74/gpt-engineer/gpt_engineer/applications/cli/file_selector.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpdpty9v74/gpt-engineer/gpt_engineer/applications/cli/learning.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpdpty9v74/gpt-engineer/gpt_engineer/applications/cli/main.py",
          "line": 256,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'subprocess.run' is used in 'run(' on line 256 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpdpty9v74/gpt-engineer/gpt_engineer/applications/cli/main.py",
          "line": 254,
          "category": "LLM08: Excessive Agency",
          "title": "Direct execution of LLM-generated code in 'get_installed_packages'",
          "description": "Function 'get_installed_packages' on line 254 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpdpty9v74/gpt-engineer/gpt_engineer/applications/cli/main.py",
          "line": 254,
          "category": "LLM09: Overreliance",
          "title": "Direct execution of LLM output in 'get_installed_packages'",
          "description": "Function 'get_installed_packages' on line 254 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpdpty9v74/gpt-engineer/gpt_engineer/applications/cli/main.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpdpty9v74/gpt-engineer/gpt_engineer/core/default/steps.py",
          "line": 267,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'execution_env.upload(files_dict).run' is used in 'run(' on line 267 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise appl",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpdpty9v74/gpt-engineer/gpt_engineer/core/default/simple_agent.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpdpty9v74/gpt-engineer/gpt_engineer/core/default/file_store.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpdpty9v74/gpt-engineer/gpt_engineer/core/default/paths.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpdpty9v74/gpt-engineer/gpt_engineer/core/default/disk_memory.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpdpty9v74/gpt-engineer/gpt_engineer/core/default/disk_execution_env.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpdpty9v74/gpt-engineer/gpt_engineer/benchmark/benchmarks/mbpp/load.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpdpty9v74/gpt-engineer/gpt_engineer/benchmark/benchmarks/apps/load.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpdpty9v74/gpt-engineer/docs/examples/open_llms/langchain_interface.py",
          "line": 17,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpdpty9v74/gpt-engineer/projects/example-improve/model.py",
          "line": 15,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous sql_injection sink",
          "description": "LLM output from 'self.generate_food' is used in 'UPDATE' on line 15 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application security.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpdpty9v74/gpt-engineer/projects/example-improve/model.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpdpty9v74/gpt-engineer/gpt_engineer/benchmark/benchmarks/mbpp/load.py",
          "line": 58,
          "category": "LLM05: Supply Chain Vulnerabilities",
          "title": "Unsafe model loading pattern",
          "description": "trust_remote_code=True enables arbitrary code execution on line 58.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpdpty9v74/gpt-engineer/gpt_engineer/benchmark/benchmarks/apps/load.py",
          "line": 58,
          "category": "LLM05: Supply Chain Vulnerabilities",
          "title": "Unsafe model loading pattern",
          "description": "trust_remote_code=True enables arbitrary code execution on line 58.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpdpty9v74/gpt-engineer/projects/example-improve/model.py",
          "line": 13,
          "category": "LLM08: Excessive Agency",
          "title": "High-risk execute operation without confirmation in '__init__'",
          "description": "Function '__init__' on line 13 performs high-risk execute operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially des",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpdpty9v74/gpt-engineer/projects/example-improve/model.py",
          "line": 21,
          "category": "LLM08: Excessive Agency",
          "title": "High-risk write/execute operation without confirmation in 'update'",
          "description": "Function 'update' on line 21 performs high-risk write/execute operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially",
          "is_semantic_taint": true
        }
      ]
    },
    {
      "app": "aider",
      "description": "AI pair programmer",
      "repo": "https://github.com/paul-gauthier/aider",
      "python_files": 146,
      "files_scanned": 0,
      "total_findings": 256,
      "critical": 169,
      "high": 20,
      "potential_vulns": [
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/benchmark/over_time.py",
          "line": 101,
          "category": "LLM04: Model Denial of Service",
          "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits",
          "description": "Function 'plot_model_series' on line 101 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhau",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/benchmark/over_time.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/benchmark/benchmark.py",
          "line": 1027,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'subprocess.run' is used in 'run(' on line 1027 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/benchmark/benchmark.py",
          "line": 863,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'coder.run' is used in 'run(' on line 863 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/benchmark/benchmark.py",
          "line": 679,
          "category": "LLM08: Excessive Agency",
          "title": "Direct execution of LLM-generated code in 'run_test_real'",
          "description": "Function 'run_test_real' on line 679 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM outp",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/benchmark/benchmark.py",
          "line": 981,
          "category": "LLM09: Overreliance",
          "title": "Critical decision without oversight in 'run_unit_tests'",
          "description": "Function 'run_unit_tests' on line 981 makes critical data_modification decisions based on LLM output without human oversight or verification. Action edges detected (HTTP/file/DB/subprocess) - risk of ",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/benchmark/problem_stats.py",
          "line": 62,
          "category": "LLM04: Model Denial of Service",
          "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits",
          "description": "Function 'analyze_exercise_solutions' on line 62 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers ",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/benchmark/refactor_tools.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/benchmark/swe_bench.py",
          "line": 21,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous sql_injection sink",
          "description": "LLM output from 'model.strip' is used in 'INSERT' on line 21 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application security.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/benchmark/swe_bench.py",
          "line": 29,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous sql_injection sink",
          "description": "LLM output from 'model.replace' is used in 'INSERT' on line 29 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application security.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/benchmark/swe_bench.py",
          "line": 23,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous sql_injection sink",
          "description": "LLM output from 'model.split' is used in 'INSERT' on line 23 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application security.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/benchmark/swe_bench.py",
          "line": 30,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous sql_injection sink",
          "description": "LLM output from 'model.strip' is used in 'INSERT' on line 30 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application security.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/benchmark/swe_bench.py",
          "line": 11,
          "category": "LLM04: Model Denial of Service",
          "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits",
          "description": "Function 'plot_swe_bench' on line 11 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust m",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/benchmark/swe_bench.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/benchmark/rungrid.py",
          "line": 56,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'subprocess.run' is used in 'run(' on line 56 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/benchmark/rungrid.py",
          "line": 42,
          "category": "LLM08: Excessive Agency",
          "title": "Direct execution of LLM-generated code in 'run'",
          "description": "Function 'run' on line 42 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM outputs can exe",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/benchmark/rungrid.py",
          "line": 42,
          "category": "LLM09: Overreliance",
          "title": "Direct execution of LLM output in 'run'",
          "description": "Function 'run' on line 42 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/benchmark/rungrid.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/benchmark/plots.py",
          "line": 62,
          "category": "LLM04: Model Denial of Service",
          "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits",
          "description": "Function 'plot_outcomes' on line 62 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust mo",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/aider/repomap.py",
          "line": 88,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** Function with user ",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/aider/versioncheck.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/aider/format_settings.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/aider/models.py",
          "line": 1192,
          "category": "LLM04: Model Denial of Service",
          "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits",
          "description": "Function 'fuzzy_match_models' on line 1192 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exh",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/aider/models.py",
          "line": 950,
          "category": "LLM04: Model Denial of Service",
          "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
          "description": "Function 'send_completion' on line 950 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/aider/models.py",
          "line": 902,
          "category": "LLM06: Sensitive Information Disclosure",
          "title": "Hardcoded secret detected: Secret Keyword",
          "description": "detect-secrets found a potential Secret Keyword on line 902. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/aider/sendchat.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/aider/openrouter.py",
          "line": 89,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous sql_injection sink",
          "description": "LLM output from 'model.startswith' is used in 'UPDATE' on line 89 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application security.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/aider/openrouter.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/aider/onboarding.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/aider/deprecated.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/aider/io.py",
          "line": 1093,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'subprocess.run' is used in 'run(' on line 1093 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/aider/io.py",
          "line": 1088,
          "category": "LLM08: Excessive Agency",
          "title": "Direct execution of LLM-generated code in 'ring_bell'",
          "description": "Function 'ring_bell' on line 1088 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM outputs",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/aider/io.py",
          "line": 1088,
          "category": "LLM09: Overreliance",
          "title": "Direct execution of LLM output in 'ring_bell'",
          "description": "Function 'ring_bell' on line 1088 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/aider/__init__.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/aider/llm.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/aider/diffs.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/aider/prompts.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/aider/dump.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/aider/mdstream.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/aider/waiting.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/aider/utils.py",
          "line": 217,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'subprocess.run' is used in 'run(' on line 217 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/aider/utils.py",
          "line": 210,
          "category": "LLM08: Excessive Agency",
          "title": "Direct execution of LLM-generated code in 'run_install'",
          "description": "Function 'run_install' on line 210 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM output",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/aider/utils.py",
          "line": 210,
          "category": "LLM09: Overreliance",
          "title": "Direct execution of LLM output in 'run_install'",
          "description": "Function 'run_install' on line 210 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/aider/utils.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/aider/run_cmd.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/aider/repo.py",
          "line": 355,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/aider/repo.py",
          "line": 361,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/aider/repo.py",
          "line": 356,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/aider/exceptions.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/aider/analytics.py",
          "line": 56,
          "category": "LLM06: Sensitive Information Disclosure",
          "title": "Hardcoded secret detected: Base64 High Entropy String",
          "description": "detect-secrets found a potential Base64 High Entropy String on line 56. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/aider/watch.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/aider/editor.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/aider/main.py",
          "line": 786,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous sql_injection sink",
          "description": "LLM output from 'args.model.startswith' is used in 'SELECT' on line 786 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application security.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/aider/main.py",
          "line": 1054,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'main_model.get_repo_map_tokens' is used in 'commands.' on line 1054 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise appl",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/aider/main.py",
          "line": 1061,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'main_model.get_repo_map_tokens' is used in 'commands.' on line 1061 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise appl",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/aider/main.py",
          "line": 1063,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'main_model.get_repo_map_tokens' is used in 'run(' on line 1063 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise applicati",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/aider/main.py",
          "line": 1069,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'main_model.get_repo_map_tokens' is used in 'commands.' on line 1069 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise appl",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/aider/main.py",
          "line": 1130,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'main_model.get_repo_map_tokens' is used in 'run(' on line 1130 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise applicati",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/aider/main.py",
          "line": 1140,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'main_model.get_repo_map_tokens' is used in 'run(' on line 1140 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise applicati",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/aider/main.py",
          "line": 1162,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'main_model.get_repo_map_tokens' is used in 'run(' on line 1162 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise applicati",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/aider/main.py",
          "line": 775,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous sql_injection sink",
          "description": "LLM output from 'model.strip' is used in 'SELECT' on line 775 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application security.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/aider/main.py",
          "line": 451,
          "category": "LLM04: Model Denial of Service",
          "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits",
          "description": "Function 'main' on line 451 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model reso",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/aider/help.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/aider/urls.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/aider/linter.py",
          "line": 151,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'subprocess.run' is used in 'run(' on line 151 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/aider/linter.py",
          "line": 136,
          "category": "LLM08: Excessive Agency",
          "title": "Direct execution of LLM-generated code in 'flake8_lint'",
          "description": "Function 'flake8_lint' on line 136 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM output",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/aider/linter.py",
          "line": 136,
          "category": "LLM09: Overreliance",
          "title": "Direct execution of LLM output in 'flake8_lint'",
          "description": "Function 'flake8_lint' on line 136 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/aider/commands.py",
          "line": 1003,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'self.coder.main_model.token_count' is used in 'run(' on line 1003 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise applic",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/aider/commands.py",
          "line": 1142,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'coder.run' is used in 'run(' on line 1142 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/aider/commands.py",
          "line": 1203,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'coder.run' is used in 'run(' on line 1203 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/aider/commands.py",
          "line": 405,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'lint_coder.run' is used in 'run(' on line 405 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/aider/commands.py",
          "line": 964,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'subprocess.run' is used in 'run(' on line 964 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/aider/commands.py",
          "line": 1470,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'self.run' is used in 'run(' on line 1470 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/aider/commands.py",
          "line": 445,
          "category": "LLM04: Model Denial of Service",
          "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits",
          "description": "Function 'cmd_tokens' on line 445 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust mode",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/aider/commands.py",
          "line": 1447,
          "category": "LLM04: Model Denial of Service",
          "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits",
          "description": "Function 'cmd_load' on line 1447 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/aider/commands.py",
          "line": 957,
          "category": "LLM08: Excessive Agency",
          "title": "Direct execution of LLM-generated code in 'cmd_git'",
          "description": "Function 'cmd_git' on line 957 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM outputs ca",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/aider/commands.py",
          "line": 957,
          "category": "LLM09: Overreliance",
          "title": "Direct execution of LLM output in 'cmd_git'",
          "description": "Function 'cmd_git' on line 957 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/aider/reasoning_tags.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/aider/special.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/aider/gui.py",
          "line": 394,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'self.messages.chat_message' is used in 'run(' on line 394 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application se",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/aider/gui.py",
          "line": 412,
          "category": "LLM04: Model Denial of Service",
          "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits",
          "description": "Function 'process_chat' on line 412 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust mo",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/aider/gui.py",
          "line": 301,
          "category": "LLM09: Overreliance",
          "title": "Critical decision without oversight in 'do_messages_container'",
          "description": "Function 'do_messages_container' on line 301 makes critical financial, security decisions based on LLM output without human oversight or verification. Action edges detected (HTTP/file/DB/subprocess) -",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/aider/args_formatter.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/aider/report.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/aider/scrape.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/aider/history.py",
          "line": 98,
          "category": "LLM04: Model Denial of Service",
          "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting, No input length validation, No timeout configuration, No token/context limits",
          "description": "Function 'summarize_all' on line 98 has 5 DoS risk(s): LLM calls in loops, No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections en",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/scripts/issues.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/scripts/logo_svg.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/scripts/clean_metadata.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/scripts/recording_audio.py",
          "line": 77,
          "category": "LLM01: Prompt Injection",
          "title": "User input 'input_file' embedded in LLM prompt",
          "description": "User input parameter 'input_file' is directly passed to LLM API call 'subprocess.run'. This is a high-confidence prompt injection vector.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/scripts/recording_audio.py",
          "line": 64,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'subprocess.run' is used in 'run(' on line 64 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/scripts/recording_audio.py",
          "line": 77,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'subprocess.run' is used in 'run(' on line 77 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/scripts/recording_audio.py",
          "line": 70,
          "category": "LLM04: Model Denial of Service",
          "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
          "description": "Function 'compress_audio' on line 70 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to e",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/scripts/recording_audio.py",
          "line": 64,
          "category": "LLM05: Supply Chain Vulnerabilities",
          "title": "Network fetch combined with code execution",
          "description": "This file downloads external content (lines [109]) and executes code (lines [64, 77]). This pattern enables remote code execution attacks if the fetched content is not properly validated.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/scripts/recording_audio.py",
          "line": 61,
          "category": "LLM08: Excessive Agency",
          "title": "Direct execution of LLM-generated code in 'check_ffmpeg'",
          "description": "Function 'check_ffmpeg' on line 61 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM output",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/scripts/recording_audio.py",
          "line": 70,
          "category": "LLM08: Excessive Agency",
          "title": "Direct execution of LLM-generated code in 'compress_audio'",
          "description": "Function 'compress_audio' on line 70 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM outp",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/scripts/recording_audio.py",
          "line": 61,
          "category": "LLM09: Overreliance",
          "title": "Direct execution of LLM output in 'check_ffmpeg'",
          "description": "Function 'check_ffmpeg' on line 61 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/scripts/recording_audio.py",
          "line": 70,
          "category": "LLM09: Overreliance",
          "title": "Direct execution of LLM output in 'compress_audio'",
          "description": "Function 'compress_audio' on line 70 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/scripts/recording_audio.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/scripts/redact-cast.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/scripts/blame.py",
          "line": 84,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'subprocess.run' is used in 'run(' on line 84 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/scripts/blame.py",
          "line": 82,
          "category": "LLM08: Excessive Agency",
          "title": "Direct execution of LLM-generated code in 'run'",
          "description": "Function 'run' on line 82 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM outputs can exe",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/scripts/blame.py",
          "line": 82,
          "category": "LLM09: Overreliance",
          "title": "Direct execution of LLM output in 'run'",
          "description": "Function 'run' on line 82 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/scripts/blame.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/scripts/30k-image.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/scripts/homepage.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/scripts/dl_icons.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/scripts/versionbump.py",
          "line": 15,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'subprocess.run(['git', 'rev-parse', '--abbrev-ref', 'HEAD'], capture_output=True, text=True).stdout.strip' is used in 'run(' on line 15 without sanitization. This creates a command_in",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/scripts/versionbump.py",
          "line": 33,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'subprocess.run' is used in 'run(' on line 33 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/scripts/versionbump.py",
          "line": 34,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'subprocess.run(['git', 'rev-parse', 'main'], capture_output=True, text=True).stdout.strip' is used in 'run(' on line 34 without sanitization. This creates a command_injection vulnerab",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/scripts/versionbump.py",
          "line": 38,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'subprocess.run(['git', 'rev-parse', 'origin/main'], capture_output=True, text=True).stdout.strip' is used in 'run(' on line 38 without sanitization. This creates a command_injection v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/scripts/versionbump.py",
          "line": 69,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'subprocess.run' is used in 'run(' on line 69 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/scripts/versionbump.py",
          "line": 25,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'subprocess.run' is used in 'run(' on line 25 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/scripts/versionbump.py",
          "line": 43,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'subprocess.run(['git', 'show', '-s', '--format=%ci', 'main'], capture_output=True, text=True).stdout.strip' is used in 'run(' on line 43 without sanitization. This creates a command_i",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/scripts/versionbump.py",
          "line": 46,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'subprocess.run(['git', 'show', '-s', '--format=%ci', 'origin/main'], capture_output=True, text=True).stdout.strip' is used in 'run(' on line 46 without sanitization. This creates a co",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/scripts/versionbump.py",
          "line": 136,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'subprocess.run' is used in 'run(' on line 136 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/scripts/versionbump.py",
          "line": 164,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'subprocess.run' is used in 'run(' on line 164 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/scripts/versionbump.py",
          "line": 78,
          "category": "LLM04: Model Denial of Service",
          "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits",
          "description": "Function 'main' on line 78 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resou",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/scripts/versionbump.py",
          "line": 14,
          "category": "LLM08: Excessive Agency",
          "title": "Direct execution of LLM-generated code in 'check_branch'",
          "description": "Function 'check_branch' on line 14 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM output",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/scripts/versionbump.py",
          "line": 24,
          "category": "LLM08: Excessive Agency",
          "title": "Direct execution of LLM-generated code in 'check_working_directory_clean'",
          "description": "Function 'check_working_directory_clean' on line 24 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/scripts/versionbump.py",
          "line": 32,
          "category": "LLM08: Excessive Agency",
          "title": "Direct execution of LLM-generated code in 'check_main_branch_up_to_date'",
          "description": "Function 'check_main_branch_up_to_date' on line 32 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or ",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/scripts/versionbump.py",
          "line": 67,
          "category": "LLM08: Excessive Agency",
          "title": "Direct execution of LLM-generated code in 'check_ok_to_push'",
          "description": "Function 'check_ok_to_push' on line 67 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM ou",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/scripts/versionbump.py",
          "line": 78,
          "category": "LLM08: Excessive Agency",
          "title": "Direct execution of LLM-generated code in 'main'",
          "description": "Function 'main' on line 78 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM outputs can ex",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/scripts/versionbump.py",
          "line": 14,
          "category": "LLM09: Overreliance",
          "title": "Direct execution of LLM output in 'check_branch'",
          "description": "Function 'check_branch' on line 14 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/scripts/versionbump.py",
          "line": 24,
          "category": "LLM09: Overreliance",
          "title": "Direct execution of LLM output in 'check_working_directory_clean'",
          "description": "Function 'check_working_directory_clean' on line 24 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/scripts/versionbump.py",
          "line": 32,
          "category": "LLM09: Overreliance",
          "title": "Direct execution of LLM output in 'check_main_branch_up_to_date'",
          "description": "Function 'check_main_branch_up_to_date' on line 32 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/scripts/versionbump.py",
          "line": 67,
          "category": "LLM09: Overreliance",
          "title": "Direct execution of LLM output in 'check_ok_to_push'",
          "description": "Function 'check_ok_to_push' on line 67 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/scripts/versionbump.py",
          "line": 78,
          "category": "LLM09: Overreliance",
          "title": "Direct execution of LLM output in 'main'",
          "description": "Function 'main' on line 78 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/scripts/update-history.py",
          "line": 127,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output flows to command_injection sink",
          "description": "LLM output variable 'cmd' flows to 'subprocess.run' on line 127 via direct flow. This creates a command_injection vulnerability.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/scripts/update-history.py",
          "line": 127,
          "category": "LLM08: Excessive Agency",
          "title": "LLM output flows to code execution in 'main'",
          "description": "In function 'main', LLM output variable 'aider_line' flows to 'subprocess.run' on line 127 via two_hop flow. This grants excessive agency to the LLM, allowing it to execute arbitrary code without huma",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/scripts/update-history.py",
          "line": 23,
          "category": "LLM08: Excessive Agency",
          "title": "Direct execution of LLM-generated code in 'run_git_log'",
          "description": "Function 'run_git_log' on line 23 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM outputs",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/scripts/update-history.py",
          "line": 40,
          "category": "LLM08: Excessive Agency",
          "title": "Direct execution of LLM-generated code in 'run_git_diff'",
          "description": "Function 'run_git_diff' on line 40 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM output",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/scripts/update-history.py",
          "line": 23,
          "category": "LLM09: Overreliance",
          "title": "Direct execution of LLM output in 'run_git_log'",
          "description": "Function 'run_git_log' on line 23 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/scripts/update-history.py",
          "line": 40,
          "category": "LLM09: Overreliance",
          "title": "Direct execution of LLM output in 'run_git_diff'",
          "description": "Function 'run_git_diff' on line 40 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/scripts/update-history.py",
          "line": 56,
          "category": "LLM09: Overreliance",
          "title": "Critical decision without oversight in 'main'",
          "description": "Function 'main' on line 56 makes critical data_modification decisions based on LLM output without human oversight or verification. Action edges detected (HTTP/file/DB/subprocess) - risk of automated e",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/scripts/tsl_pack_langs.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/scripts/my_models.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/scripts/yank-old-versions.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/aider/coders/single_wholefile_func_coder.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/aider/coders/ask_prompts.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/aider/coders/wholefile_func_prompts.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/aider/coders/single_wholefile_func_prompts.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/aider/coders/wholefile_func_coder.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/aider/coders/architect_prompts.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/aider/coders/search_replace.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/aider/coders/editor_diff_fenced_prompts.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/aider/coders/editblock_func_prompts.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/aider/coders/wholefile_coder.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/aider/coders/base_coder.py",
          "line": 880,
          "category": "LLM01: Prompt Injection",
          "title": "User input 'with_message' embedded in LLM prompt",
          "description": "User input parameter 'with_message' is directly passed to LLM API call 'self.run_one'. This is a high-confidence prompt injection vector.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/aider/coders/base_coder.py",
          "line": 887,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output flows to command_injection sink",
          "description": "LLM output variable 'user_message' flows to 'self.run_one' on line 887 via direct flow. This creates a command_injection vulnerability.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/aider/coders/base_coder.py",
          "line": 299,
          "category": "LLM04: Model Denial of Service",
          "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
          "description": "Function '__init__' on line 299 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaus",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/aider/coders/base_coder.py",
          "line": 876,
          "category": "LLM04: Model Denial of Service",
          "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting, No input length validation, No timeout configuration, No token/context limits",
          "description": "Function 'run' on line 876 has 5 DoS risk(s): LLM calls in loops, No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable atta",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/aider/coders/base_coder.py",
          "line": 1783,
          "category": "LLM04: Model Denial of Service",
          "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
          "description": "Function 'send' on line 1783 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust m",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/aider/coders/base_coder.py",
          "line": 1994,
          "category": "LLM04: Model Denial of Service",
          "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
          "description": "Function 'calculate_and_show_tokens_and_cost' on line 1994 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/aider/coders/base_coder.py",
          "line": 2244,
          "category": "LLM04: Model Denial of Service",
          "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits",
          "description": "Function 'check_added_files' on line 2244 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exha",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/aider/coders/help_prompts.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/aider/coders/editor_editblock_prompts.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/aider/coders/context_coder.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/aider/coders/architect_coder.py",
          "line": 44,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'editor_coder.run' is used in 'run(' on line 44 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/aider/coders/architect_coder.py",
          "line": 44,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/aider/coders/context_prompts.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/aider/coders/udiff_prompts.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/aider/coders/wholefile_prompts.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/aider/coders/editblock_func_coder.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/aider/coders/udiff_coder.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/aider/coders/base_prompts.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential indirect prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Indirect prompt injection: Malicious instructions from external data sources\n\n**Location:** ML model detected",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/aider/coders/editblock_prompts.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/aider/coders/patch_coder.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/aider/coders/editblock_coder.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/benchmark/benchmark.py",
          "line": 679,
          "category": "LLM07: Insecure Plugin Design",
          "title": "Insecure tool function 'run_test_real' executes dangerous operations",
          "description": "Tool function 'run_test_real' on line 679 takes LLM output as a parameter and performs dangerous operations (file_access) without proper validation. Attackers can craft malicious LLM outputs to execut",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/benchmark/benchmark.py",
          "line": 679,
          "category": "LLM09: Overreliance",
          "title": "Critical decision without oversight in 'run_test_real'",
          "description": "Function 'run_test_real' on line 679 makes critical financial, security, data_modification decisions based on LLM output without human oversight or verification. Action edges detected (HTTP/file/DB/su",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/benchmark/problem_stats.py",
          "line": 62,
          "category": "LLM08: Excessive Agency",
          "title": "High-risk delete/write/execute/network operation without confirmation in 'analyze_exercise_solutions'",
          "description": "Function 'analyze_exercise_solutions' on line 62 performs high-risk delete/write/execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM ",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/aider/analytics.py",
          "line": 55,
          "category": "LLM06: Sensitive Information Disclosure",
          "title": "Hardcoded secret detected: Hex High Entropy String",
          "description": "detect-secrets found a potential Hex High Entropy String on line 55. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/aider/main.py",
          "line": 451,
          "category": "LLM08: Excessive Agency",
          "title": "Unrestricted API access from LLM in 'main'",
          "description": "Function 'main' on line 451 makes HTTP/API requests based on LLM outputs without URL validation or allowlisting. This allows the LLM to make requests to arbitrary endpoints, potentially exfiltrating d",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/aider/linter.py",
          "line": 179,
          "category": "LLM05: Supply Chain Vulnerabilities",
          "title": "Code execution on external content",
          "description": "compile() for execution on line 179. ",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/aider/linter.py",
          "line": 82,
          "category": "LLM08: Excessive Agency",
          "title": "High-risk execute operation without confirmation in 'lint'",
          "description": "Function 'lint' on line 82 performs high-risk execute operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destruc",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/aider/commands.py",
          "line": 1109,
          "category": "LLM08: Excessive Agency",
          "title": "High-risk write/execute operation without confirmation in 'cmd_help'",
          "description": "Function 'cmd_help' on line 1109 performs high-risk write/execute operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potenti",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/aider/commands.py",
          "line": 1188,
          "category": "LLM08: Excessive Agency",
          "title": "High-risk write/execute operation without confirmation in '_generic_chat_command'",
          "description": "Function '_generic_chat_command' on line 1188 performs high-risk write/execute operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously ex",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5s08ivmp/aider/aider/commands.py",
          "line": 1360,
          "category": "LLM08: Excessive Agency",
          "title": "High-risk delete/network operation without confirmation in '_add_read_only_file'",
          "description": "Function '_add_read_only_file' on line 1360 performs high-risk delete/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously exe",
          "is_semantic_taint": true
        }
      ]
    },
    {
      "app": "opendevin",
      "description": "AI software engineer",
      "repo": "https://github.com/OpenDevin/OpenDevin",
      "python_files": 1216,
      "files_scanned": 0,
      "total_findings": 1138,
      "critical": 678,
      "high": 211,
      "potential_vulns": [
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/run_maintenance_tasks.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/version.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/scripts/update_openapi.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/utils/shared.py",
          "line": 193,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output flows to code_execution sink",
          "description": "LLM output variable 'eval_output_path' flows to 'EvalMetadata' on line 193 via direct flow. This creates a code_execution vulnerability.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/utils/shared.py",
          "line": 193,
          "category": "LLM08: Excessive Agency",
          "title": "LLM output flows to code execution in 'make_metadata'",
          "description": "In function 'make_metadata', LLM output variable 'model_path' flows to 'EvalMetadata' on line 193 via single_hop flow. This grants excessive agency to the LLM, allowing it to execute arbitrary code wi",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/gorilla/ast_eval_hf.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/gorilla/run_infer.py",
          "line": 45,
          "category": "LLM05: Supply Chain Vulnerabilities",
          "title": "Code execution on external content",
          "description": "eval() on non-literal content on line 45. File fetches external content - HIGH RISK.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/gorilla/run_infer.py",
          "line": 47,
          "category": "LLM05: Supply Chain Vulnerabilities",
          "title": "Code execution on external content",
          "description": "eval() on non-literal content on line 47. File fetches external content - HIGH RISK.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/gorilla/run_infer.py",
          "line": 42,
          "category": "LLM07: Insecure Plugin Design",
          "title": "Unsafe dynamic plugin loading in 'get_config'",
          "description": "Function 'get_config' on line 42 uses dynamic loading (__import__, eval, exec) to load plugins without validation. This creates a critical security risk where attackers can inject malicious code throu",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/gorilla/ast_eval_th.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/gorilla/utils.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/gorilla/ast_eval_tf.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/visual_swe_bench/run_infer.py",
          "line": 320,
          "category": "LLM04: Model Denial of Service",
          "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits",
          "description": "Function 'complete_runtime' on line 320 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaus",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/visual_swe_bench/run_infer.py",
          "line": 136,
          "category": "LLM07: Insecure Plugin Design",
          "title": "Unsafe dynamic plugin loading in 'get_config'",
          "description": "Function 'get_config' on line 136 uses dynamic loading (__import__, eval, exec) to load plugins without validation. This creates a critical security risk where attackers can inject malicious code thro",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/lca_ci_build_repair/run_infer.py",
          "line": 47,
          "category": "LLM07: Insecure Plugin Design",
          "title": "Unsafe dynamic plugin loading in 'get_config'",
          "description": "Function 'get_config' on line 47 uses dynamic loading (__import__, eval, exec) to load plugins without validation. This creates a critical security risk where attackers can inject malicious code throu",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/lca_ci_build_repair/setup.py",
          "line": 28,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'subprocess.run' is used in 'subprocess.' on line 28 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/lca_ci_build_repair/setup.py",
          "line": 36,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'subprocess.run' is used in 'subprocess.' on line 36 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/lca_ci_build_repair/setup.py",
          "line": 10,
          "category": "LLM08: Excessive Agency",
          "title": "Direct execution of LLM-generated code in 'setup'",
          "description": "Function 'setup' on line 10 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM outputs can e",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/lca_ci_build_repair/setup.py",
          "line": 10,
          "category": "LLM09: Overreliance",
          "title": "Direct execution of LLM output in 'setup'",
          "description": "Function 'setup' on line 10 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/lca_ci_build_repair/eval_infer.py",
          "line": 36,
          "category": "LLM07: Insecure Plugin Design",
          "title": "Unsafe dynamic plugin loading in 'get_config'",
          "description": "Function 'get_config' on line 36 uses dynamic loading (__import__, eval, exec) to load plugins without validation. This creates a critical security risk where attackers can inject malicious code throu",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/lca_ci_build_repair/eval_infer.py",
          "line": 68,
          "category": "LLM08: Excessive Agency",
          "title": "Direct execution of LLM-generated code in 'run_eval'",
          "description": "Function 'run_eval' on line 68 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM outputs ca",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/lca_ci_build_repair/eval_infer.py",
          "line": 68,
          "category": "LLM09: Overreliance",
          "title": "Direct execution of LLM output in 'run_eval'",
          "description": "Function 'run_eval' on line 68 directly executes LLM-generated code using eval(. This is extremely dangerous and allows arbitrary code execution.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/commit0/run_infer.py",
          "line": 334,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output flows to command_injection sink",
          "description": "LLM output variable 'tests' flows to 'runtimes.append' on line 334 via direct flow. This creates a command_injection vulnerability.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/commit0/run_infer.py",
          "line": 483,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output flows to code_execution sink",
          "description": "LLM output variable 'instruction' flows to 'EvalOutput' on line 483 via direct flow. This creates a code_execution vulnerability.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/commit0/run_infer.py",
          "line": 198,
          "category": "LLM04: Model Denial of Service",
          "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits",
          "description": "Function 'complete_runtime' on line 198 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaus",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/EDA/run_infer.py",
          "line": 61,
          "category": "LLM07: Insecure Plugin Design",
          "title": "Unsafe dynamic plugin loading in 'get_config'",
          "description": "Function 'get_config' on line 61 uses dynamic loading (__import__, eval, exec) to load plugins without validation. This creates a critical security risk where attackers can inject malicious code throu",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_perf/run_infer.py",
          "line": 422,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'runtime.run_action' is used in 'commands.' on line 422 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application secur",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_perf/run_infer.py",
          "line": 465,
          "category": "LLM04: Model Denial of Service",
          "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits",
          "description": "Function 'complete_runtime' on line 465 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaus",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_perf/run_infer.py",
          "line": 248,
          "category": "LLM07: Insecure Plugin Design",
          "title": "Unsafe dynamic plugin loading in 'get_config'",
          "description": "Function 'get_config' on line 248 uses dynamic loading (__import__, eval, exec) to load plugins without validation. This creates a critical security risk where attackers can inject malicious code thro",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_perf/format_conversion.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_perf/binary_patch_utils.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/humanevalfix/run_infer.py",
          "line": 84,
          "category": "LLM07: Insecure Plugin Design",
          "title": "Unsafe dynamic plugin loading in 'get_config'",
          "description": "Function 'get_config' on line 84 uses dynamic loading (__import__, eval, exec) to load plugins without validation. This creates a critical security risk where attackers can inject malicious code throu",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/mint/env.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/mint/run_infer.py",
          "line": 105,
          "category": "LLM07: Insecure Plugin Design",
          "title": "Unsafe dynamic plugin loading in 'get_config'",
          "description": "Function 'get_config' on line 105 uses dynamic loading (__import__, eval, exec) to load plugins without validation. This creates a critical security risk where attackers can inject malicious code thro",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/mint/datatypes.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/mint/utils.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/scienceagentbench/run_infer.py",
          "line": 83,
          "category": "LLM04: Model Denial of Service",
          "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits",
          "description": "Function 'initialize_runtime' on line 83 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhau",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/scienceagentbench/post_proc.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/the_agent_company/run_infer.py",
          "line": 36,
          "category": "LLM07: Insecure Plugin Design",
          "title": "Unsafe dynamic plugin loading in 'get_config'",
          "description": "Function 'get_config' on line 36 uses dynamic loading (__import__, eval, exec) to load plugins without validation. This creates a critical security risk where attackers can inject malicious code throu",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/the_agent_company/browsing.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/tau_bench/run_infer.py",
          "line": 49,
          "category": "LLM07: Insecure Plugin Design",
          "title": "Unsafe dynamic plugin loading in 'get_config'",
          "description": "Function 'get_config' on line 49 uses dynamic loading (__import__, eval, exec) to load plugins without validation. This creates a critical security risk where attackers can inject malicious code throu",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/aider_bench/run_infer.py",
          "line": 49,
          "category": "LLM07: Insecure Plugin Design",
          "title": "Unsafe dynamic plugin loading in 'get_config'",
          "description": "Function 'get_config' on line 49 uses dynamic loading (__import__, eval, exec) to load plugins without validation. This creates a critical security risk where attackers can inject malicious code throu",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/aider_bench/create_dataset.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/agent_bench/run_infer.py",
          "line": 41,
          "category": "LLM07: Insecure Plugin Design",
          "title": "Unsafe dynamic plugin loading in 'get_config'",
          "description": "Function 'get_config' on line 41 uses dynamic loading (__import__, eval, exec) to load plugins without validation. This creates a critical security risk where attackers can inject malicious code throu",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/agent_bench/helper.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/bfcl/run_infer.py",
          "line": 41,
          "category": "LLM07: Insecure Plugin Design",
          "title": "Unsafe dynamic plugin loading in 'get_config'",
          "description": "Function 'get_config' on line 41 uses dynamic loading (__import__, eval, exec) to load plugins without validation. This creates a critical security risk where attackers can inject malicious code throu",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/live_utils.py",
          "line": 16,
          "category": "LLM04: Model Denial of Service",
          "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits",
          "description": "Function 'complete_runtime' on line 16 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/run_infer.py",
          "line": 394,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'runtime.run_action' is used in 'commands.' on line 394 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application secur",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/run_infer.py",
          "line": 271,
          "category": "LLM04: Model Denial of Service",
          "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits",
          "description": "Function 'initialize_runtime' on line 271 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exha",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/run_infer.py",
          "line": 440,
          "category": "LLM04: Model Denial of Service",
          "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits",
          "description": "Function 'complete_runtime' on line 440 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaus",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/run_infer.py",
          "line": 206,
          "category": "LLM07: Insecure Plugin Design",
          "title": "Unsafe dynamic plugin loading in 'get_config'",
          "description": "Function 'get_config' on line 206 uses dynamic loading (__import__, eval, exec) to load plugins without validation. This creates a critical security risk where attackers can inject malicious code thro",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/run_infer_interact.py",
          "line": 77,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/run_infer_interact.py",
          "line": 88,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/run_infer_interact.py",
          "line": 103,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/binary_patch_utils.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/run_localize.py",
          "line": 226,
          "category": "LLM04: Model Denial of Service",
          "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits",
          "description": "Function 'initialize_runtime' on line 226 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exha",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/run_localize.py",
          "line": 399,
          "category": "LLM04: Model Denial of Service",
          "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits",
          "description": "Function 'complete_runtime' on line 399 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaus",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/run_localize.py",
          "line": 169,
          "category": "LLM07: Insecure Plugin Design",
          "title": "Unsafe dynamic plugin loading in 'get_config'",
          "description": "Function 'get_config' on line 169 uses dynamic loading (__import__, eval, exec) to load plugins without validation. This creates a critical security risk where attackers can inject malicious code thro",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/eval_infer.py",
          "line": 342,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output flows to command_injection sink",
          "description": "LLM output variable 'apply_patch_output' flows to 'RuntimeError' on line 342 via direct flow. This creates a command_injection vulnerability.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/eval_infer.py",
          "line": 260,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output flows to command_injection sink",
          "description": "LLM output variable 'pid' flows to 'CmdRunAction' on line 260 via direct flow. This creates a command_injection vulnerability.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/eval_infer.py",
          "line": 264,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output flows to command_injection sink",
          "description": "LLM output variable 'check_action' flows to 'runtime.run_action' on line 264 via direct flow. This creates a command_injection vulnerability.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/nocode_bench/consistants.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/nocode_bench/run_infer_nc.py",
          "line": 372,
          "category": "LLM04: Model Denial of Service",
          "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits",
          "description": "Function 'complete_runtime' on line 372 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaus",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/nocode_bench/run_infer_nc.py",
          "line": 150,
          "category": "LLM07: Insecure Plugin Design",
          "title": "Unsafe dynamic plugin loading in 'get_config'",
          "description": "Function 'get_config' on line 150 uses dynamic loading (__import__, eval, exec) to load plugins without validation. This creates a critical security risk where attackers can inject malicious code thro",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/nocode_bench/binary_patch_utils.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/multi_swe_bench/compute_skip_ids.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/multi_swe_bench/run_infer.py",
          "line": 500,
          "category": "LLM04: Model Denial of Service",
          "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits",
          "description": "Function 'complete_runtime' on line 500 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaus",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/multi_swe_bench/run_infer.py",
          "line": 311,
          "category": "LLM07: Insecure Plugin Design",
          "title": "Unsafe dynamic plugin loading in 'get_config'",
          "description": "Function 'get_config' on line 311 uses dynamic loading (__import__, eval, exec) to load plugins without validation. This creates a critical security risk where attackers can inject malicious code thro",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/multi_swe_bench/eval_infer.py",
          "line": 312,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output flows to command_injection sink",
          "description": "LLM output variable 'apply_patch_output' flows to 'RuntimeError' on line 312 via direct flow. This creates a command_injection vulnerability.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/multi_swe_bench/eval_infer.py",
          "line": 236,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output flows to command_injection sink",
          "description": "LLM output variable 'pid' flows to 'CmdRunAction' on line 236 via direct flow. This creates a command_injection vulnerability.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/multi_swe_bench/eval_infer.py",
          "line": 240,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output flows to command_injection sink",
          "description": "LLM output variable 'check_action' flows to 'runtime.run_action' on line 240 via direct flow. This creates a command_injection vulnerability.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/discoverybench/run_infer.py",
          "line": 137,
          "category": "LLM04: Model Denial of Service",
          "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits",
          "description": "Function 'initialize_runtime' on line 137 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exha",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/discoverybench/run_infer.py",
          "line": 64,
          "category": "LLM07: Insecure Plugin Design",
          "title": "Unsafe dynamic plugin loading in 'get_config'",
          "description": "Function 'get_config' on line 64 uses dynamic loading (__import__, eval, exec) to load plugins without validation. This creates a critical security risk where attackers can inject malicious code throu",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/gaia/run_infer.py",
          "line": 150,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous sql_injection sink",
          "description": "LLM output from 'runtime.run_action' is used in 'UPDATE' on line 150 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application security.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/gaia/run_infer.py",
          "line": 156,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous sql_injection sink",
          "description": "LLM output from 'runtime.run_action' is used in 'UPDATE' on line 156 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application security.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/gaia/run_infer.py",
          "line": 145,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous sql_injection sink",
          "description": "LLM output from 'runtime.run_action' is used in 'UPDATE' on line 145 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application security.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/gaia/run_infer.py",
          "line": 64,
          "category": "LLM07: Insecure Plugin Design",
          "title": "Unsafe dynamic plugin loading in 'get_config'",
          "description": "Function 'get_config' on line 64 uses dynamic loading (__import__, eval, exec) to load plugins without validation. This creates a critical security risk where attackers can inject malicious code throu",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/gaia/utils.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/gaia/get_score.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/gaia/scorer.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/bird/run_infer.py",
          "line": 123,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'subprocess.run' is used in 'subprocess.' on line 123 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application securit",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/bird/run_infer.py",
          "line": 73,
          "category": "LLM07: Insecure Plugin Design",
          "title": "Unsafe dynamic plugin loading in 'get_config'",
          "description": "Function 'get_config' on line 73 uses dynamic loading (__import__, eval, exec) to load plugins without validation. This creates a critical security risk where attackers can inject malicious code throu",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/bird/run_infer.py",
          "line": 107,
          "category": "LLM08: Excessive Agency",
          "title": "Direct execution of LLM-generated code in 'load_bird'",
          "description": "Function 'load_bird' on line 107 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM outputs ",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/bird/run_infer.py",
          "line": 110,
          "category": "LLM08: Excessive Agency",
          "title": "Direct execution of LLM-generated code in '_download_bird'",
          "description": "Function '_download_bird' on line 110 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM out",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/bird/run_infer.py",
          "line": 107,
          "category": "LLM09: Overreliance",
          "title": "Direct execution of LLM output in 'load_bird'",
          "description": "Function 'load_bird' on line 107 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/bird/run_infer.py",
          "line": 347,
          "category": "LLM09: Overreliance",
          "title": "Critical decision without oversight in 'process_instance'",
          "description": "Function 'process_instance' on line 347 makes critical data_modification decisions based on LLM output without human oversight or verification. Action edges detected (HTTP/file/DB/subprocess) - risk o",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/bird/run_infer.py",
          "line": 110,
          "category": "LLM09: Overreliance",
          "title": "Direct execution of LLM output in '_download_bird'",
          "description": "Function '_download_bird' on line 110 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/ml_bench/run_infer.py",
          "line": 79,
          "category": "LLM07: Insecure Plugin Design",
          "title": "Unsafe dynamic plugin loading in 'get_config'",
          "description": "Function 'get_config' on line 79 uses dynamic loading (__import__, eval, exec) to load plugins without validation. This creates a critical security risk where attackers can inject malicious code throu",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/ml_bench/run_infer.py",
          "line": 203,
          "category": "LLM08: Excessive Agency",
          "title": "Direct execution of LLM-generated code in 'process_instance'",
          "description": "Function 'process_instance' on line 203 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM o",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/miniwob/get_avg_reward.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/algotune/run_infer.py",
          "line": 288,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output flows to code_execution sink",
          "description": "LLM output variable 'full_output' flows to '_parse_evaluation_output' on line 288 via direct flow. This creates a code_execution vulnerability.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/algotune/run_infer.py",
          "line": 71,
          "category": "LLM07: Insecure Plugin Design",
          "title": "Unsafe dynamic plugin loading in 'get_config'",
          "description": "Function 'get_config' on line 71 uses dynamic loading (__import__, eval, exec) to load plugins without validation. This creates a critical security risk where attackers can inject malicious code throu",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/algotune/run_infer.py",
          "line": 71,
          "category": "LLM08: Excessive Agency",
          "title": "Direct execution of LLM-generated code in 'get_config'",
          "description": "Function 'get_config' on line 71 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM outputs ",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/algotune/run_infer.py",
          "line": 71,
          "category": "LLM09: Overreliance",
          "title": "Critical decision without oversight in 'get_config'",
          "description": "Function 'get_config' on line 71 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/webarena/run_infer.py",
          "line": 46,
          "category": "LLM07: Insecure Plugin Design",
          "title": "Unsafe dynamic plugin loading in 'get_config'",
          "description": "Function 'get_config' on line 46 uses dynamic loading (__import__, eval, exec) to load plugins without validation. This creates a critical security risk where attackers can inject malicious code throu",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/webarena/get_success_rate.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/browsing_delegation/run_infer.py",
          "line": 36,
          "category": "LLM07: Insecure Plugin Design",
          "title": "Unsafe dynamic plugin loading in 'get_config'",
          "description": "Function 'get_config' on line 36 uses dynamic loading (__import__, eval, exec) to load plugins without validation. This creates a critical security risk where attackers can inject malicious code throu",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swefficiency/run_infer.py",
          "line": 146,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous sql_injection sink",
          "description": "LLM output from 'sandbox_config.runtime_startup_env_vars.update' is used in 'UPDATE' on line 146 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can comprom",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swefficiency/run_infer.py",
          "line": 326,
          "category": "LLM04: Model Denial of Service",
          "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits",
          "description": "Function 'complete_runtime' on line 326 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaus",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swefficiency/run_infer.py",
          "line": 119,
          "category": "LLM07: Insecure Plugin Design",
          "title": "Unsafe dynamic plugin loading in 'get_config'",
          "description": "Function 'get_config' on line 119 uses dynamic loading (__import__, eval, exec) to load plugins without validation. This creates a critical security risk where attackers can inject malicious code thro",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swefficiency/run_infer.py",
          "line": 119,
          "category": "LLM08: Excessive Agency",
          "title": "Direct execution of LLM-generated code in 'get_config'",
          "description": "Function 'get_config' on line 119 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM outputs",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swefficiency/run_infer.py",
          "line": 119,
          "category": "LLM09: Overreliance",
          "title": "Critical decision without oversight in 'get_config'",
          "description": "Function 'get_config' on line 119 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swefficiency/binary_patch_utils.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/testgeneval/metrics.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/testgeneval/report_utils.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/testgeneval/compute_readability.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/testgeneval/constants.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/testgeneval/run_infer.py",
          "line": 118,
          "category": "LLM07: Insecure Plugin Design",
          "title": "Unsafe dynamic plugin loading in 'get_config'",
          "description": "Function 'get_config' on line 118 uses dynamic loading (__import__, eval, exec) to load plugins without validation. This creates a critical security risk where attackers can inject malicious code thro",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/testgeneval/pygments_utils.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/testgeneval/utils.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/testgeneval/eval_infer.py",
          "line": 135,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output flows to command_injection sink",
          "description": "LLM output variable 'pid' flows to 'CmdRunAction' on line 135 via direct flow. This creates a command_injection vulnerability.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/testgeneval/eval_infer.py",
          "line": 136,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output flows to command_injection sink",
          "description": "LLM output variable 'check_action' flows to 'runtime.run_action' on line 136 via direct flow. This creates a command_injection vulnerability.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/testgeneval/eval_infer.py",
          "line": 173,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output flows to command_injection sink",
          "description": "LLM output variable 'pid' flows to 'CmdRunAction' on line 173 via direct flow. This creates a command_injection vulnerability.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/testgeneval/eval_infer.py",
          "line": 174,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output flows to command_injection sink",
          "description": "LLM output variable 'check_action' flows to 'runtime.run_action' on line 174 via direct flow. This creates a command_injection vulnerability.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/gpqa/run_infer.py",
          "line": 63,
          "category": "LLM07: Insecure Plugin Design",
          "title": "Unsafe dynamic plugin loading in 'get_config'",
          "description": "Function 'get_config' on line 63 uses dynamic loading (__import__, eval, exec) to load plugins without validation. This creates a critical security risk where attackers can inject malicious code throu",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/logic_reasoning/logic_inference.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/logic_reasoning/run_infer.py",
          "line": 47,
          "category": "LLM07: Insecure Plugin Design",
          "title": "Unsafe dynamic plugin loading in 'get_config'",
          "description": "Function 'get_config' on line 47 uses dynamic loading (__import__, eval, exec) to load plugins without validation. This creates a critical security risk where attackers can inject malicious code throu",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/toolqa/run_infer.py",
          "line": 43,
          "category": "LLM07: Insecure Plugin Design",
          "title": "Unsafe dynamic plugin loading in 'get_config'",
          "description": "Function 'get_config' on line 43 uses dynamic loading (__import__, eval, exec) to load plugins without validation. This creates a critical security risk where attackers can inject malicious code throu",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/toolqa/utils.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/biocoder/run_infer.py",
          "line": 58,
          "category": "LLM07: Insecure Plugin Design",
          "title": "Unsafe dynamic plugin loading in 'get_config'",
          "description": "Function 'get_config' on line 58 uses dynamic loading (__import__, eval, exec) to load plugins without validation. This creates a critical security risk where attackers can inject malicious code throu",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/biocoder/run_infer.py",
          "line": 76,
          "category": "LLM09: Overreliance",
          "title": "Critical decision without oversight in 'initialize_runtime'",
          "description": "Function 'initialize_runtime' on line 76 makes critical financial, data_modification decisions based on LLM output without human oversight or verification. Action edges detected (HTTP/file/DB/subproce",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/visualwebarena/get_success_rate.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/biocoder/scripts/setup/remove_code.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/biocoder/scripts/setup/copy_changed_code.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/versicode/inference_utils/api_code_migration.py",
          "line": 32,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/versicode/inference_utils/api_code_migration.py",
          "line": 31,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** Function with user ",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/versicode/inference_utils/api_test_block_completion.py",
          "line": 32,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/versicode/inference_utils/api_test_block_completion.py",
          "line": 31,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** Function with user ",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/versicode/output_processing/clear_ans.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/versicode/output_processing/choose_core_line_from_block_versicode.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/versicode/output_processing/choose_core_line_from_migration_versicode.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/versicode/metric/compute_versicode_em_score.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/versicode/metric/compute_ism_pm_score.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/versicode/metric/compute_migration_cdc_score.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/versicode/metric/compute_versicode_cdc_score.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/testgeneval/scripts/docker/compare_txt_files.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/testgeneval/scripts/docker/add_testing_dependencies.py",
          "line": 11,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'subprocess.run' is used in 'subprocess.' on line 11 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/testgeneval/scripts/docker/add_testing_dependencies.py",
          "line": 9,
          "category": "LLM08: Excessive Agency",
          "title": "Direct execution of LLM-generated code in 'run_command'",
          "description": "Function 'run_command' on line 9 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM outputs ",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/testgeneval/scripts/docker/add_testing_dependencies.py",
          "line": 9,
          "category": "LLM09: Overreliance",
          "title": "Direct execution of LLM output in 'run_command'",
          "description": "Function 'run_command' on line 9 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/testgeneval/scripts/eval/convert_oh_output_to_md.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/testgeneval/scripts/eval/build_outputs_ablation.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/testgeneval/scripts/eval/compare_outputs.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/testgeneval/scripts/eval/summarize_outputs.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/testgeneval/scripts/eval/convert_oh_output_to_swe_json.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/testgeneval/scripts/eval/download_gold_test_suites.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/algotune/adapter/adapter.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/algotune/adapter/run_adapter.py",
          "line": 27,
          "category": "LLM01: Prompt Injection",
          "title": "User input 'repo_url' embedded in LLM prompt",
          "description": "User input parameter 'repo_url' is directly passed to LLM API call 'subprocess.run'. This is a high-confidence prompt injection vector.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/algotune/adapter/run_adapter.py",
          "line": 27,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'subprocess.run' is used in 'subprocess.' on line 27 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/algotune/adapter/run_adapter.py",
          "line": 92,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'adapter.generate_task' is used in 'subprocess.' on line 92 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application s",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/algotune/adapter/run_adapter.py",
          "line": 40,
          "category": "LLM04: Model Denial of Service",
          "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits",
          "description": "Function 'main' on line 40 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resou",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/algotune/adapter/run_adapter.py",
          "line": 20,
          "category": "LLM08: Excessive Agency",
          "title": "Direct execution of LLM-generated code in 'clone_repo'",
          "description": "Function 'clone_repo' on line 20 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM outputs ",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/algotune/adapter/run_adapter.py",
          "line": 40,
          "category": "LLM08: Excessive Agency",
          "title": "Direct execution of LLM-generated code in 'main'",
          "description": "Function 'main' on line 40 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM outputs can ex",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/algotune/adapter/run_adapter.py",
          "line": 20,
          "category": "LLM09: Overreliance",
          "title": "Direct execution of LLM output in 'clone_repo'",
          "description": "Function 'clone_repo' on line 20 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/algotune/adapter/utils.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/ml_bench/scripts/summarise_results.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/discoverybench/eval_utils/eval_w_subhypo_gen.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/discoverybench/eval_utils/openai_helpers.py",
          "line": 93,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/discoverybench/eval_utils/openai_helpers.py",
          "line": 90,
          "category": "LLM04: Model Denial of Service",
          "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits",
          "description": "Function 'get_response' on line 90 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust mod",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/discoverybench/eval_utils/openai_helpers.py",
          "line": 90,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** Function with user ",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/discoverybench/eval_utils/response_parser.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/discoverybench/eval_utils/lm_utils.py",
          "line": 47,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/discoverybench/eval_utils/lm_utils.py",
          "line": 54,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/discoverybench/eval_utils/lm_utils.py",
          "line": 31,
          "category": "LLM04: Model Denial of Service",
          "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits",
          "description": "Function 'run_chatgpt_query_multi_turn' on line 31 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attacker",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/discoverybench/eval_utils/openai_semantic_gen_prompts.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/multi_swe_bench/resource/mapping.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/multi_swe_bench/scripts/setup/compare_patch_filename.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/multi_swe_bench/scripts/eval/update_output_with_eval.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/multi_swe_bench/scripts/eval/combine_final_completions.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/multi_swe_bench/scripts/eval/convert.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/multi_swe_bench/scripts/eval/update_multi_swe_bench_config.py",
          "line": 12,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'subprocess.run' is used in 'subprocess.' on line 12 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/multi_swe_bench/scripts/eval/update_multi_swe_bench_config.py",
          "line": 7,
          "category": "LLM08: Excessive Agency",
          "title": "Direct execution of LLM-generated code in 'update_multi_swe_config'",
          "description": "Function 'update_multi_swe_config' on line 7 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy ",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/multi_swe_bench/scripts/eval/update_multi_swe_bench_config.py",
          "line": 7,
          "category": "LLM09: Overreliance",
          "title": "Direct execution of LLM output in 'update_multi_swe_config'",
          "description": "Function 'update_multi_swe_config' on line 7 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/multi_swe_bench/scripts/eval/update_multi_swe_bench_config.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/multi_swe_bench/scripts/data/data_change.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/nocode_bench/resource/mapping.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/nocode_bench/scripts/setup/compare_patch_filename.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/nocode_bench/scripts/utils/evaluation_utils.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/nocode_bench/scripts/eval/verify_costs.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/nocode_bench/scripts/eval/convert.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/resource/mapping.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/loc_eval/loc_evaluator.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/scripts/docker/push_docker_instance_images.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/scripts/docker/get_docker_image_names.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/scripts/setup/compare_patch_filename.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/scripts/swtbench/convert.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/scripts/live/convert.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/scripts/eval/update_output_with_eval.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/scripts/eval/convert_oh_output_to_md.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/scripts/eval/verify_costs.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/scripts/eval/combine_final_completions.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/scripts/eval/compare_outputs.py",
          "line": 47,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'subprocess.run' is used in 'subprocess.' on line 47 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/scripts/eval/compare_outputs.py",
          "line": 41,
          "category": "LLM08: Excessive Agency",
          "title": "Direct execution of LLM-generated code in 'summarize_file'",
          "description": "Function 'summarize_file' on line 41 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM outp",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/scripts/eval/compare_outputs.py",
          "line": 41,
          "category": "LLM09: Overreliance",
          "title": "Direct execution of LLM output in 'summarize_file'",
          "description": "Function 'summarize_file' on line 41 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/scripts/eval/summarize_outputs.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/scripts/eval/convert_oh_output_to_swe_json.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/scripts/eval/download_gold_patch.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/agent_bench/scripts/summarise_results.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/aider_bench/scripts/summarize_results.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/mint/tasks/reasoning.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/mint/tasks/codegen.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/mint/tasks/base.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_perf/resource/mapping.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_perf/scripts/setup/compare_patch_filename.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/utils/scripts/aggregate_token_usage.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/third_party/runtime/impl/runloop/runloop_runtime.py",
          "line": 85,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output flows to command_injection sink",
          "description": "LLM output variable 'devbox' flows to 'self.runloop_api_client.devboxes.retrieve' on line 85 via direct flow. This creates a command_injection vulnerability.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/third_party/runtime/impl/daytona/daytona_runtime.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/third_party/runtime/impl/modal/modal_runtime.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/third_party/runtime/impl/e2b/e2b_runtime.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/third_party/runtime/impl/e2b/sandbox.py",
          "line": 97,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'self.sandbox.commands.run' is used in 'run(' on line 97 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application secu",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/third_party/runtime/impl/e2b/sandbox.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/llm/metrics.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/llm/async_llm.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/llm/streaming_llm.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/llm/debug_mixin.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/llm/retry_mixin.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/llm/tool_names.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/llm/fn_call_converter.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/llm/llm_registry.py",
          "line": 75,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous sql_injection sink",
          "description": "LLM output from 'llm.completion' is used in 'UPDATE' on line 75 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application security.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/llm/llm_registry.py",
          "line": 65,
          "category": "LLM04: Model Denial of Service",
          "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
          "description": "Function 'request_extraneous_completion' on line 65 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/llm/llm_registry.py",
          "line": 75,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/llm/model_features.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/core/message_utils.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/core/message.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/core/logger.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/core/setup.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/core/exceptions.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/memory/memory.py",
          "line": 390,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential prompt injection vulnerability (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  ",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/memory/memory.py",
          "line": 384,
          "category": "LLM04: Model Denial of Service",
          "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
          "description": "Function 'set_runtime_status' on line 384 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/memory/view.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/memory/conversation_memory.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/config.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/experiments/experiment_manager.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/io/io.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/io/json.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/file_viewer_server.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/__init__.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/runtime_status.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/base.py",
          "line": 287,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output flows to command_injection sink",
          "description": "LLM output variable 'obs' flows to 'RuntimeError' on line 287 via direct flow. This creates a command_injection vulnerability.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/base.py",
          "line": 313,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output flows to command_injection sink",
          "description": "LLM output variable 'obs' flows to 'RuntimeError' on line 313 via direct flow. This creates a command_injection vulnerability.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/base.py",
          "line": 322,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output flows to command_injection sink",
          "description": "LLM output variable 'obs' flows to 'RuntimeError' on line 322 via direct flow. This creates a command_injection vulnerability.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/server/shared.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/server/utils.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/server/app.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/server/listen_socket.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/server/file_config.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/server/middleware.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/utils/shutdown_listener.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/utils/llm.py",
          "line": 14,
          "category": "LLM04: Model Denial of Service",
          "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits",
          "description": "Function 'get_supported_llm_models' on line 14 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/utils/utils.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/utils/conversation_summary.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/utils/term_color.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/utils/environment.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/utils/chunk_localizer.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/utils/prompt.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential indirect prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Indirect prompt injection: Malicious instructions from external data sources\n\n**Location:** ML model detected",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/utils/http_session.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/utils/import_utils.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/utils/search_utils.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/mcp/client.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/mcp/utils.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/storage/google_cloud.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/storage/batched_web_hook.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/storage/memory.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/storage/local.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/storage/web_hook.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/storage/__init__.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/storage/s3.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/controller/replay.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/controller/stuck.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/controller/agent.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential indirect prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Indirect prompt injection: Malicious instructions from external data sources\n\n**Location:** ML model detected",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/integrations/provider.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/integrations/utils.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/integrations/service_types.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/events/event.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/events/event_store.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/events/nested_event_store.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/microagent/microagent.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/resolver/visualize_resolver_output.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/resolver/send_pull_request.py",
          "line": 237,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output flows to command_injection sink",
          "description": "LLM output variable 'result' flows to 'RuntimeError' on line 237 via direct flow. This creates a command_injection vulnerability.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/resolver/send_pull_request.py",
          "line": 169,
          "category": "LLM04: Model Denial of Service",
          "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
          "description": "Function 'make_commit' on line 169 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exh",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/resolver/send_pull_request.py",
          "line": 240,
          "category": "LLM04: Model Denial of Service",
          "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
          "description": "Function 'send_pull_request' on line 240 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers ",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/resolver/send_pull_request.py",
          "line": 438,
          "category": "LLM04: Model Denial of Service",
          "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
          "description": "Function 'update_existing_pull_request' on line 438 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/resolver/send_pull_request.py",
          "line": 131,
          "category": "LLM08: Excessive Agency",
          "title": "Direct execution of LLM-generated code in 'initialize_repo'",
          "description": "Function 'initialize_repo' on line 131 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM ou",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/resolver/send_pull_request.py",
          "line": 169,
          "category": "LLM08: Excessive Agency",
          "title": "Direct execution of LLM-generated code in 'make_commit'",
          "description": "Function 'make_commit' on line 169 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM output",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/resolver/send_pull_request.py",
          "line": 240,
          "category": "LLM08: Excessive Agency",
          "title": "Direct execution of LLM-generated code in 'send_pull_request'",
          "description": "Function 'send_pull_request' on line 240 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM ",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/resolver/send_pull_request.py",
          "line": 438,
          "category": "LLM08: Excessive Agency",
          "title": "Direct execution of LLM-generated code in 'update_existing_pull_request'",
          "description": "Function 'update_existing_pull_request' on line 438 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/resolver/send_pull_request.py",
          "line": 131,
          "category": "LLM09: Overreliance",
          "title": "Direct execution of LLM output in 'initialize_repo'",
          "description": "Function 'initialize_repo' on line 131 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/resolver/send_pull_request.py",
          "line": 169,
          "category": "LLM09: Overreliance",
          "title": "Direct execution of LLM output in 'make_commit'",
          "description": "Function 'make_commit' on line 169 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/resolver/send_pull_request.py",
          "line": 240,
          "category": "LLM09: Overreliance",
          "title": "Direct execution of LLM output in 'send_pull_request'",
          "description": "Function 'send_pull_request' on line 240 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/resolver/send_pull_request.py",
          "line": 438,
          "category": "LLM09: Overreliance",
          "title": "Critical decision without oversight in 'update_existing_pull_request'",
          "description": "Function 'update_existing_pull_request' on line 438 makes critical security, data_modification decisions based on LLM output without human oversight or verification. Action edges detected (HTTP/file/D",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/resolver/issue_resolver.py",
          "line": 291,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output flows to command_injection sink",
          "description": "LLM output variable 'obs' flows to 'RuntimeError' on line 291 via direct flow. This creates a command_injection vulnerability.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/resolver/issue_resolver.py",
          "line": 304,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output flows to command_injection sink",
          "description": "LLM output variable 'obs' flows to 'RuntimeError' on line 304 via direct flow. This creates a command_injection vulnerability.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/resolver/utils.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/resolver/issue_handler_factory.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/resolver/interfaces/bitbucket.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/resolver/interfaces/forgejo.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/resolver/interfaces/gitlab.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/resolver/interfaces/issue_definitions.py",
          "line": 175,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/resolver/interfaces/issue_definitions.py",
          "line": 402,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/resolver/interfaces/issue_definitions.py",
          "line": 173,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** Function with user ",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/resolver/interfaces/azure_devops.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/resolver/interfaces/github.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/resolver/patching/patch.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/resolver/patching/snippets.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/resolver/patching/apply.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/events/serialization/observation.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/events/serialization/event.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/events/serialization/action.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/events/action/action.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/events/action/agent.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/events/observation/files.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/events/observation/commands.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/integrations/gitlab/gitlab_service.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/integrations/github/github_service.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/integrations/github/queries.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/integrations/azure_devops/azure_devops_service.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/integrations/forgejo/service/branches.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/integrations/forgejo/service/features.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/integrations/forgejo/service/repos.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/integrations/forgejo/service/resolver.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/integrations/forgejo/service/prs.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/integrations/forgejo/service/base.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/integrations/azure_devops/service/work_items.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/integrations/azure_devops/service/branches.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/integrations/azure_devops/service/features.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/integrations/azure_devops/service/repos.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/integrations/azure_devops/service/resolver.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/integrations/azure_devops/service/prs.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/integrations/bitbucket/service/branches.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/integrations/bitbucket/service/repos.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/integrations/bitbucket/service/prs.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/integrations/bitbucket/service/base.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/integrations/github/service/branches_prs.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/integrations/github/service/features.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/integrations/github/service/repos.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/integrations/github/service/resolver.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/integrations/github/service/prs.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/integrations/github/service/base.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/integrations/gitlab/service/branches.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/integrations/gitlab/service/features.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/integrations/gitlab/service/repos.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/integrations/gitlab/service/resolver.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/integrations/gitlab/service/prs.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/integrations/gitlab/service/base.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/controller/state/state_tracker.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/controller/state/state.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/storage/data_models/conversation_metadata.py",
          "line": 12,
          "category": "LLM06: Sensitive Information Disclosure",
          "title": "Hardcoded secret detected: Secret Keyword",
          "description": "detect-secrets found a potential Secret Keyword on line 12. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/storage/data_models/conversation_metadata.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/storage/data_models/conversation_status.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/storage/data_models/secrets.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/storage/data_models/settings.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/storage/settings/file_settings_store.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/storage/secrets/file_secrets_store.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/storage/conversation/conversation_validator.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/storage/conversation/file_conversation_store.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/server/conversation_manager/standalone_conversation_manager.py",
          "line": 97,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/server/conversation_manager/docker_nested_conversation_manager.py",
          "line": 554,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/server/user_auth/__init__.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/server/user_auth/user_auth.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/server/user_auth/default_user_auth.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/server/config/server_config.py",
          "line": 19,
          "category": "LLM06: Sensitive Information Disclosure",
          "title": "Hardcoded secret detected: Base64 High Entropy String",
          "description": "detect-secrets found a potential Base64 High Entropy String on line 19. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/server/config/server_config.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/server/routes/files.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/server/routes/git.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/server/routes/conversation.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/server/routes/feedback.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/server/routes/manage_conversations.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential indirect prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Indirect prompt injection: Malicious instructions from external data sources\n\n**Location:** ML model detected",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/server/routes/mcp.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/server/routes/secrets.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/server/routes/settings.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/server/services/conversation_stats.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/server/session/session.py",
          "line": 470,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential prompt injection vulnerability (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  ",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/server/session/session.py",
          "line": 82,
          "category": "LLM04: Model Denial of Service",
          "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
          "description": "Function '__init__' on line 82 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/server/session/session.py",
          "line": 466,
          "category": "LLM04: Model Denial of Service",
          "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
          "description": "Function 'queue_status_message' on line 466 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attacke",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/utils/log_capture.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/utils/files.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/utils/system.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/utils/command.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/utils/git_diff.py",
          "line": 32,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output flows to command_injection sink",
          "description": "LLM output variable 'result' flows to 'RuntimeError' on line 32 via direct flow. This creates a command_injection vulnerability.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/utils/git_diff.py",
          "line": 25,
          "category": "LLM08: Excessive Agency",
          "title": "Direct execution of LLM-generated code in 'run'",
          "description": "Function 'run' on line 25 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM outputs can exe",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/utils/git_diff.py",
          "line": 25,
          "category": "LLM09: Overreliance",
          "title": "Direct execution of LLM output in 'run'",
          "description": "Function 'run' on line 25 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/utils/git_diff.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/utils/runtime_init.py",
          "line": 91,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output flows to command_injection sink",
          "description": "LLM output variable 'output' flows to 'RuntimeError' on line 91 via direct flow. This creates a command_injection vulnerability.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/utils/runtime_init.py",
          "line": 106,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output flows to command_injection sink",
          "description": "LLM output variable 'output' flows to 'RuntimeError' on line 106 via direct flow. This creates a command_injection vulnerability.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/utils/runtime_init.py",
          "line": 8,
          "category": "LLM04: Model Denial of Service",
          "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
          "description": "Function 'init_user_and_working_directory' on line 8 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enabl",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/utils/runtime_init.py",
          "line": 8,
          "category": "LLM08: Excessive Agency",
          "title": "Direct execution of LLM-generated code in 'init_user_and_working_directory'",
          "description": "Function 'init_user_and_working_directory' on line 8 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious o",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/utils/runtime_init.py",
          "line": 8,
          "category": "LLM09: Overreliance",
          "title": "Direct execution of LLM output in 'init_user_and_working_directory'",
          "description": "Function 'init_user_and_working_directory' on line 8 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/utils/bash.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/utils/request.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/utils/edit.py",
          "line": 103,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output flows to sql_injection sink",
          "description": "LLM output variable 'resp' flows to '_extract_code' on line 103 via direct flow. This creates a sql_injection vulnerability.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/utils/edit.py",
          "line": 430,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output flows to command_injection sink",
          "description": "LLM output variable 'response' flows to 'codeact_function_calling.response_to_actions' on line 430 via direct flow. This creates a command_injection vulnerability.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/utils/edit.py",
          "line": 90,
          "category": "LLM04: Model Denial of Service",
          "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits",
          "description": "Function 'get_new_file_contents' on line 90 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to ex",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/utils/edit.py",
          "line": 102,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/utils/edit.py",
          "line": 429,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/utils/port_lock.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/utils/runtime_build.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/utils/git_changes.py",
          "line": 20,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output flows to command_injection sink",
          "description": "LLM output variable 'result' flows to 'RuntimeError' on line 20 via direct flow. This creates a command_injection vulnerability.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/utils/git_changes.py",
          "line": 13,
          "category": "LLM08: Excessive Agency",
          "title": "Direct execution of LLM-generated code in 'run'",
          "description": "Function 'run' on line 13 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM outputs can exe",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/utils/git_changes.py",
          "line": 13,
          "category": "LLM09: Overreliance",
          "title": "Direct execution of LLM output in 'run'",
          "description": "Function 'run' on line 13 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/utils/git_changes.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/utils/windows_bash.py",
          "line": 77,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'subprocess.run' is used in 'subprocess.' on line 77 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/utils/windows_bash.py",
          "line": 102,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'subprocess.run' is used in 'subprocess.' on line 102 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application securit",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/utils/windows_bash.py",
          "line": 51,
          "category": "LLM08: Excessive Agency",
          "title": "Direct execution of LLM-generated code in 'find_latest_pwsh_sdk_path'",
          "description": "Function 'find_latest_pwsh_sdk_path' on line 51 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or bug",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/utils/windows_bash.py",
          "line": 51,
          "category": "LLM09: Overreliance",
          "title": "Direct execution of LLM output in 'find_latest_pwsh_sdk_path'",
          "description": "Function 'find_latest_pwsh_sdk_path' on line 51 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/utils/git_handler.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/utils/system_stats.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/utils/file_viewer.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/browser/base64.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/browser/utils.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/browser/browser_env.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/builder/remote.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/builder/docker.py",
          "line": 45,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'subprocess.run' is used in 'subprocess.' on line 45 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/builder/docker.py",
          "line": 122,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'subprocess.run' is used in 'subprocess.' on line 122 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application securit",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/builder/docker.py",
          "line": 54,
          "category": "LLM04: Model Denial of Service",
          "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits",
          "description": "Function 'build' on line 54 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model reso",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/builder/docker.py",
          "line": 42,
          "category": "LLM08: Excessive Agency",
          "title": "Direct execution of LLM-generated code in 'check_buildx'",
          "description": "Function 'check_buildx' on line 42 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM output",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/builder/docker.py",
          "line": 54,
          "category": "LLM08: Excessive Agency",
          "title": "Direct execution of LLM-generated code in 'build'",
          "description": "Function 'build' on line 54 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM outputs can e",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/builder/docker.py",
          "line": 42,
          "category": "LLM09: Overreliance",
          "title": "Direct execution of LLM output in 'check_buildx'",
          "description": "Function 'check_buildx' on line 42 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/builder/docker.py",
          "line": 54,
          "category": "LLM09: Overreliance",
          "title": "Direct execution of LLM output in 'build'",
          "description": "Function 'build' on line 54 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/mcp/proxy/manager.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/plugins/vscode/__init__.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/plugins/agent_skills/agentskills.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/plugins/agent_skills/file_ops/file_ops.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/impl/action_execution/action_execution_client.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/impl/docker/docker_runtime.py",
          "line": 521,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'self.docker_client.containers.run' is used in 'run(' on line 521 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise applica",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/impl/docker/docker_runtime.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/impl/cli/cli_runtime.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/impl/local/local_runtime.py",
          "line": 1,
          "category": "LLM05: Supply Chain Vulnerabilities",
          "title": "Network fetch combined with code execution",
          "description": "This file downloads external content (lines [781]) and executes code (lines [1, 30, 31]). This pattern enables remote code execution attacks if the fetched content is not properly validated.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/impl/local/local_runtime.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/impl/kubernetes/kubernetes_runtime.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/impl/remote/remote_runtime.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/security/invariant/client.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/security/invariant/analyzer.py",
          "line": 57,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'self.docker_client.containers.run' is used in 'run(' on line 57 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise applicat",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/security/invariant/analyzer.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/security/invariant/parser.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/security/grayswan/analyzer.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/security/grayswan/utils.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/event_callback/webhook_router.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/event_callback/event_callback_models.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/event_callback/event_callback_result_models.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/event_callback/sql_event_callback_service.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/event_callback/set_title_callback_processor.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/utils/encryption_key.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/utils/sql_utils.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/utils/import_utils.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/user/auth_user_context.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/sandbox/sandbox_spec_service.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/sandbox/sandbox_service.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/sandbox/sandbox_models.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/sandbox/preset_sandbox_spec_service.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/sandbox/sandbox_router.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/sandbox/process_sandbox_service.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/sandbox/docker_sandbox_service.py",
          "line": 40,
          "category": "LLM06: Sensitive Information Disclosure",
          "title": "Hardcoded secret detected: Secret Keyword",
          "description": "detect-secrets found a potential Secret Keyword on line 40. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/sandbox/docker_sandbox_service.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/sandbox/docker_sandbox_spec_service.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/sandbox/remote_sandbox_service.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/app_lifespan/oss_app_lifespan_service.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/app_conversation/app_conversation_models.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/app_conversation/sql_app_conversation_info_service.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/app_conversation/sql_app_conversation_start_task_service.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/app_conversation/app_conversation_service_base.py",
          "line": 385,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous sql_injection sink",
          "description": "LLM output from 'llm.model_copy' is used in 'UPDATE' on line 385 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application security.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/app_conversation/app_conversation_service_base.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/app_conversation/app_conversation_router.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/app_conversation/skill_loader.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/app_conversation/live_status_app_conversation_service.py",
          "line": 934,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous sql_injection sink",
          "description": "LLM output from 'agent.llm.model_copy' is used in 'UPDATE' on line 934 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application security.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/app_conversation/live_status_app_conversation_service.py",
          "line": 949,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous sql_injection sink",
          "description": "LLM output from 'condenser_llm.model_copy' is used in 'UPDATE' on line 949 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application securi",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/app_conversation/live_status_app_conversation_service.py",
          "line": 624,
          "category": "LLM04: Model Denial of Service",
          "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
          "description": "Function '_configure_llm' on line 624 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to ",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/app_conversation/live_status_app_conversation_service.py",
          "line": 905,
          "category": "LLM04: Model Denial of Service",
          "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
          "description": "Function '_update_agent_with_llm_metadata' on line 905 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections ena",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/app_conversation/live_status_app_conversation_service.py",
          "line": 996,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/app_conversation/live_status_app_conversation_service.py",
          "line": 636,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/app_conversation/live_status_app_conversation_service.py",
          "line": 934,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/app_conversation/live_status_app_conversation_service.py",
          "line": 949,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/event/event_router.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/event/filesystem_event_service.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/event/google_cloud_event_service.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/services/httpx_client_injector.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/services/db_session_injector.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/services/jwt_service.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/app_lifespan/alembic/env.py",
          "line": 57,
          "category": "LLM09: Overreliance",
          "title": "Critical decision without oversight in 'run_migrations_offline'",
          "description": "Function 'run_migrations_offline' on line 57 makes critical financial decisions based on LLM output without human oversight or verification. Action edges detected (HTTP/file/DB/subprocess) - risk of a",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/app_lifespan/alembic/env.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/app_lifespan/alembic/versions/002.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/agenthub/codeact_agent/function_calling.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/agenthub/codeact_agent/codeact_agent.py",
          "line": 260,
          "category": "LLM04: Model Denial of Service",
          "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
          "description": "Function '_get_messages' on line 260 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to e",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/agenthub/browsing_agent/utils.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/agenthub/browsing_agent/response_parser.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/agenthub/loc_agent/function_calling.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/agenthub/dummy_agent/agent.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/agenthub/readonly_agent/function_calling.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/agenthub/loc_agent/tools/search_content.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/agenthub/loc_agent/tools/explore_structure.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/agenthub/codeact_agent/tools/bash.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/agenthub/codeact_agent/tools/browser.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/agenthub/codeact_agent/tools/ipython.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/agenthub/codeact_agent/tools/prompt.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/agenthub/codeact_agent/tools/str_replace_editor.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/agenthub/codeact_agent/tools/task_tracker.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/memory/condenser/condenser.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/memory/condenser/impl/conversation_window_condenser.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/memory/condenser/impl/amortized_forgetting_condenser.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/memory/condenser/impl/llm_attention_condenser.py",
          "line": 87,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output flows to sql_injection sink",
          "description": "LLM output variable 'response' flows to 'ImportantEventSelection.model_validate_json' on line 87 via direct flow. This creates a sql_injection vulnerability.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/memory/condenser/impl/llm_summarizing_condenser.py",
          "line": 52,
          "category": "LLM04: Model Denial of Service",
          "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits",
          "description": "Function 'get_condensation' on line 52 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/memory/condenser/impl/llm_summarizing_condenser.py",
          "line": 142,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/memory/condenser/impl/llm_summarizing_condenser.py",
          "line": 149,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/memory/condenser/impl/llm_summarizing_condenser.py",
          "line": 143,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/memory/condenser/impl/structured_summary_condenser.py",
          "line": 199,
          "category": "LLM04: Model Denial of Service",
          "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits",
          "description": "Function 'get_condensation' on line 199 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaus",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/core/config/condenser_config.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/core/config/agent_config.py",
          "line": 91,
          "category": "LLM04: Model Denial of Service",
          "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits",
          "description": "Function 'from_toml_section' on line 91 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaus",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/core/config/utils.py",
          "line": 465,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous sql_injection sink",
          "description": "LLM output from 'cfg.runtime.lower' is used in 'SELECT' on line 465 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application security.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/core/config/utils.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/core/config/arg_utils.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/core/config/mcp_config.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/core/config/llm_config.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/core/config/config_utils.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/core/schema/observation.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/core/schema/action.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/core/schema/agent.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/llm/router/base.py",
          "line": 140,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous sql_injection sink",
          "description": "LLM output from 'selected_llm.completion' is used in 'SELECT' on line 140 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application securit",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/migrations/env.py",
          "line": 69,
          "category": "LLM09: Overreliance",
          "title": "Critical decision without oversight in 'run_migrations_offline'",
          "description": "Function 'run_migrations_offline' on line 69 makes critical financial decisions based on LLM output without human oversight or verification. Action edges detected (HTTP/file/DB/subprocess) - risk of a",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/migrations/env.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/enterprise_local/convert_to_env.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/experiments/constants.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/config.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/constants.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/saas_nested_conversation_manager.py",
          "line": 409,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/logger.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/rate_limit.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/middleware.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/user_settings.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/subscription_access.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/saas_settings_store.py",
          "line": 291,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous sql_injection sink",
          "description": "LLM output from 'user_model.split' is used in 'UPDATE' on line 291 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application security.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/saas_settings_store.py",
          "line": 250,
          "category": "LLM04: Model Denial of Service",
          "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
          "description": "Function '_has_custom_settings' on line 250 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attacke",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/saas_settings_store.py",
          "line": 266,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/saas_settings_store.py",
          "line": 267,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/saas_settings_store.py",
          "line": 291,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/conversation_callback.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/auth_tokens.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/user_repo_map_store.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/saas_conversation_store.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/conversation_work.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/gitlab_webhook.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/stripe_customer.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/linear_workspace.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/jira_workspace.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/experiment_assignment.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/linear_user.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/slack_user.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/slack_conversation.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/auth_token_store.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/feedback.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/linear_conversation.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/stored_custom_secrets.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/maintenance_task.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/database.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/slack_team.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/jira_integration_store.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/jira_dc_integration_store.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/offline_token_store.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/linear_integration_store.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/jira_user.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/gitlab_webhook_store.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/telemetry_identity.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/api_key.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/device_code_store.py",
          "line": 28,
          "category": "LLM04: Model Denial of Service",
          "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits",
          "description": "Function 'create_device_code' on line 28 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhau",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/jira_dc_conversation.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/blocked_email_domain.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/redis.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/proactive_convos.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/jira_conversation.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/stored_offline_token.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/proactive_conversation_store.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/user_repo_map.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/saas_secrets_store.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/device_code.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/jira_dc_user.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/billing_session.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/openhands_pr.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/api_key_store.py",
          "line": 28,
          "category": "LLM04: Model Denial of Service",
          "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
          "description": "Function 'create_api_key' on line 28 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to e",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/api_key_store.py",
          "line": 20,
          "category": "LLM06: Sensitive Information Disclosure",
          "title": "Hardcoded secret detected: Secret Keyword",
          "description": "detect-secrets found a potential Secret Keyword on line 20. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/api_key_store.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/jira_dc_workspace.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/stored_repository.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/saas_conversation_validator.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/github_app_installation.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/telemetry_metrics.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/integrations/stripe_service.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/integrations/models.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/integrations/types.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/integrations/utils.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/integrations/resolver_context.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/sync/install_gitlab_webhooks.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/sync/common_room_sync.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/sync/resend_keycloak.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/sync/enrich_user_interaction_data.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/integrations/gitlab/gitlab_service.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/integrations/gitlab/gitlab_view.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/integrations/gitlab/gitlab_manager.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/integrations/gitlab/webhook_installation.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/integrations/linear/linear_view.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/integrations/linear/linear_manager.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/integrations/jira/jira_manager.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/integrations/jira/jira_view.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/integrations/github/github_service.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/integrations/github/data_collector.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/integrations/github/github_v1_callback_processor.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/integrations/github/github_types.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/integrations/github/github_solvability.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/integrations/github/github_manager.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/integrations/jira_dc/jira_dc_view.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/integrations/jira_dc/jira_dc_manager.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/integrations/slack/slack_v1_callback_processor.py",
          "line": 114,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/integrations/slack/slack_view.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/integrations/bitbucket/bitbucket_service.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/integrations/solvability/models/difficulty_level.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/integrations/solvability/models/featurizer.py",
          "line": 255,
          "category": "LLM04: Model Denial of Service",
          "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits",
          "description": "Function 'embed' on line 255 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model res",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/integrations/solvability/models/featurizer.py",
          "line": 288,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/integrations/solvability/models/classifier.py",
          "line": 258,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous sql_injection sink",
          "description": "LLM output from 'self.predict_proba' is used in 'WHERE' on line 258 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application security.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/integrations/solvability/models/classifier.py",
          "line": 205,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous sql_injection sink",
          "description": "LLM output from 'self.classifier.predict_proba' is used in 'WHERE' on line 205 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application se",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/integrations/solvability/models/classifier.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/integrations/solvability/models/summary.py",
          "line": 127,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/sharing/sql_shared_conversation_info_service.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/sharing/shared_conversation_router.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/sharing/shared_conversation_models.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/sharing/google_cloud_shared_event_service.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/sharing/shared_event_router.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/auth/domain_blocker.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/auth/token_manager.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/auth/email_validation.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/auth/constants.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/auth/auth_utils.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/auth/github_utils.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/auth/recaptcha_service.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/auth/saas_user_auth.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/auth/sheets_client.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/utils/rate_limit_utils.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/utils/conversation_callback_utils.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/conversation_callback_processor/github_callback_processor.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/conversation_callback_processor/jira_callback_processor.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/conversation_callback_processor/linear_callback_processor.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/conversation_callback_processor/jira_dc_callback_processor.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/conversation_callback_processor/gitlab_callback_processor.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/conversation_callback_processor/slack_callback_processor.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/routes/auth.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/routes/user.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/routes/oauth_device.py",
          "line": 23,
          "category": "LLM06: Sensitive Information Disclosure",
          "title": "Hardcoded secret detected: Secret Keyword",
          "description": "detect-secrets found a potential Secret Keyword on line 23. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/routes/oauth_device.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/routes/billing.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/routes/event_webhook.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/routes/feedback.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/routes/debugging.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/routes/email.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/routes/github_proxy.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/routes/mcp_patch.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/routes/api_keys.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/maintenance_task_processor/user_version_upgrade_processor.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/routes/integration/gitlab.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/routes/integration/linear.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/routes/integration/jira_dc.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/routes/integration/jira.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/routes/integration/github.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/routes/integration/slack.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/experiments/experiment_versions/_003_llm_claude4_vs_gpt5_experiment.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/experiments/experiment_versions/_004_condenser_max_step_experiment.py",
          "line": 227,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous sql_injection sink",
          "description": "LLM output from 'agent.llm.model_copy' is used in 'UPDATE' on line 227 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application security.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/experiments/experiment_versions/_004_condenser_max_step_experiment.py",
          "line": 201,
          "category": "LLM04: Model Denial of Service",
          "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
          "description": "Function 'handle_condenser_max_step_experiment__v1' on line 201 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protec",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/experiments/experiment_versions/_004_condenser_max_step_experiment.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/experiments/experiment_versions/_002_system_prompt_experiment.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/migrations/versions/022_create_api_keys_table.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/migrations/versions/047_create_conversation_feedback_table.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/migrations/versions/061_create_experiment_assignments_table.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/migrations/versions/060_create_user_version_upgrade_tasks.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/migrations/versions/019_remove_duplicates_from_stripe.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/migrations/versions/044_add_llm_model_to_conversation_metadata.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/migrations/versions/062_add_git_user_fields_to_user_settings.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/migrations/versions/051_update_conversation_callbacks_fk_to_cascade.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/migrations/versions/049_create_conversation_callbacks_table.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/migrations/versions/030_add_proactive_conversation_starters_column.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/migrations/versions/059_create_maintenance_tasks_table.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/migrations/versions/023_add_cost_and_token_metrics_columns.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/migrations/versions/084_create_device_codes_table.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/migrations/versions/080_add_status_and_updated_at_to_callback.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/migrations/versions/042_add_git_provider_to_conversation_metadata.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/scripts/update_openapi.py",
          "line": 128,
          "category": "LLM10: Model Theft",
          "title": "Model API without rate limiting in 'generate_openapi_spec'",
          "description": "API endpoint 'generate_openapi_spec' on line 128 provides model access without rate limiting. This allows attackers to make unlimited queries to extract model knowledge, potentially stealing intellect",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/utils/shared.py",
          "line": 699,
          "category": "LLM05: Supply Chain Vulnerabilities",
          "title": "Code execution on external content",
          "description": "eval() on non-literal content on line 699. ",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/utils/shared.py",
          "line": 715,
          "category": "LLM05: Supply Chain Vulnerabilities",
          "title": "Code execution on external content",
          "description": "eval() on non-literal content on line 715. ",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/utils/shared.py",
          "line": 734,
          "category": "LLM05: Supply Chain Vulnerabilities",
          "title": "Code execution on external content",
          "description": "eval() on non-literal content on line 734. ",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/utils/shared.py",
          "line": 752,
          "category": "LLM05: Supply Chain Vulnerabilities",
          "title": "Code execution on external content",
          "description": "eval() on non-literal content on line 752. ",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/utils/shared.py",
          "line": 164,
          "category": "LLM08: Excessive Agency",
          "title": "High-risk write/execute/network operation without confirmation in 'make_metadata'",
          "description": "Function 'make_metadata' on line 164 performs high-risk write/execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously exe",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/gorilla/run_infer.py",
          "line": 58,
          "category": "LLM08: Excessive Agency",
          "title": "High-risk delete/write/execute/network operation without confirmation in 'process_instance'",
          "description": "Function 'process_instance' on line 58 performs high-risk delete/write/execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonom",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/visual_swe_bench/run_infer.py",
          "line": 186,
          "category": "LLM08: Excessive Agency",
          "title": "High-risk delete/write/execute/network operation without confirmation in 'initialize_runtime'",
          "description": "Function 'initialize_runtime' on line 186 performs high-risk delete/write/execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to auto",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/visual_swe_bench/run_infer.py",
          "line": 320,
          "category": "LLM08: Excessive Agency",
          "title": "High-risk delete/write/execute/network operation without confirmation in 'complete_runtime'",
          "description": "Function 'complete_runtime' on line 320 performs high-risk delete/write/execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autono",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/visual_swe_bench/run_infer.py",
          "line": 440,
          "category": "LLM08: Excessive Agency",
          "title": "High-risk write/execute/network operation without confirmation in 'process_instance'",
          "description": "Function 'process_instance' on line 440 performs high-risk write/execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously ",
          "is_semantic_taint": true
        }
      ]
    },
    {
      "app": "quivr",
      "description": "Second brain with AI",
      "repo": "https://github.com/QuivrHQ/quivr",
      "python_files": 77,
      "files_scanned": 0,
      "total_findings": 61,
      "critical": 33,
      "high": 2,
      "potential_vulns": [
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/examples/simple_question_megaparse.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/examples/pdf_document_from_yaml.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/examples/pdf_parsing_tika.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/examples/quivr-whisper/app.py",
          "line": 135,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/examples/quivr-whisper/app.py",
          "line": 135,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'openai.audio.speech.create' is used in 'run(' on line 135 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application se",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/examples/quivr-whisper/app.py",
          "line": 21,
          "category": "LLM06: Sensitive Information Disclosure",
          "title": "Hardcoded secret detected: Secret Keyword",
          "description": "detect-secrets found a potential Secret Keyword on line 21. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/examples/quivr-whisper/app.py",
          "line": 134,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** Function with user ",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/examples/chatbot_voice/main.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/core/quivr_core/config.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/core/quivr_core/__init__.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/core/quivr_core/llm_tools/web_search_tools.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/core/quivr_core/processor/registry.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/core/quivr_core/processor/processor_base.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/core/quivr_core/language/models.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/core/quivr_core/rag/prompts.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/core/quivr_core/rag/quivr_rag_langgraph.py",
          "line": 1194,
          "category": "LLM01: Prompt Injection",
          "title": "User input 'prompt' embedded in LLM prompt",
          "description": "User input parameter 'prompt' is directly passed to LLM API call 'structured_llm.invoke'. This is a high-confidence prompt injection vector.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/core/quivr_core/rag/quivr_rag_langgraph.py",
          "line": 1197,
          "category": "LLM01: Prompt Injection",
          "title": "User input 'prompt' embedded in LLM prompt",
          "description": "User input parameter 'prompt' is directly passed to LLM API call 'structured_llm.invoke'. This is a high-confidence prompt injection vector.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/core/quivr_core/rag/quivr_rag_langgraph.py",
          "line": 429,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous sql_injection sink",
          "description": "LLM output from 'self.invoke_structured_output' is used in 'UPDATE' on line 429 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application s",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/core/quivr_core/rag/quivr_rag_langgraph.py",
          "line": 923,
          "category": "LLM04: Model Denial of Service",
          "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits",
          "description": "Function 'generate_zendesk_rag' on line 923 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to ex",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/core/quivr_core/rag/quivr_rag.py",
          "line": 175,
          "category": "LLM01: Prompt Injection",
          "title": "User input 'question' embedded in LLM prompt",
          "description": "User input parameter 'question' is directly passed to LLM API call 'conversational_qa_chain.invoke'. This is a high-confidence prompt injection vector.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/core/quivr_core/rag/quivr_rag.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/core/quivr_core/storage/file.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/core/quivr_core/files/file.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/core/quivr_core/brain/brain.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/core/quivr_core/brain/brain_defaults.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/core/quivr_core/brain/info.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/core/quivr_core/rag/entities/config.py",
          "line": 281,
          "category": "LLM04: Model Denial of Service",
          "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits",
          "description": "Function 'get_supplier_by_model_name' on line 281 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/core/quivr_core/rag/entities/config.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/core/quivr_core/rag/entities/models.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/core/quivr_core/processor/implementations/default.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/core/quivr_core/processor/implementations/tika_processor.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/core/quivr_core/processor/implementations/simple_txt_processor.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/core/quivr_core/processor/implementations/megaparse_processor.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/core/quivr_core/rag/quivr_rag_langgraph.py",
          "line": 916,
          "category": "LLM08: Excessive Agency",
          "title": "High-risk execute operation without confirmation in 'bind_tools_to_llm'",
          "description": "Function 'bind_tools_to_llm' on line 916 performs high-risk execute operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/core/quivr_core/brain/brain.py",
          "line": 375,
          "category": "LLM08: Excessive Agency",
          "title": "High-risk execute operation without confirmation in 'from_files'",
          "description": "Function 'from_files' on line 375 performs high-risk execute operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially ",
          "is_semantic_taint": true
        }
      ]
    },
    {
      "app": "privateGPT",
      "description": "Private document Q&A",
      "repo": "https://github.com/zylon-ai/private-gpt",
      "python_files": 88,
      "files_scanned": 0,
      "total_findings": 50,
      "critical": 35,
      "high": 9,
      "potential_vulns": [
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/scripts/ingest_folder.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/scripts/utils.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/scripts/extract_openapi.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/private_gpt/ui/ui.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/private_gpt/settings/settings_loader.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/private_gpt/settings/yaml.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/private_gpt/utils/eta.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/private_gpt/utils/ollama.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/private_gpt/components/llm/llm_component.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/private_gpt/components/llm/prompt_helper.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/private_gpt/components/ingest/ingest_component.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/private_gpt/components/ingest/ingest_helper.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/private_gpt/components/embedding/embedding_component.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/private_gpt/components/vector_store/batched_chroma.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/private_gpt/components/vector_store/vector_store_component.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/private_gpt/components/embedding/custom/sagemaker.py",
          "line": 50,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/private_gpt/components/llm/custom/sagemaker.py",
          "line": 266,
          "category": "LLM01: Prompt Injection",
          "title": "User input 'messages' embedded in LLM prompt",
          "description": "User input 'messages' flows to LLM call via call in variable 'prompt'. Function 'chat' may be vulnerable to prompt injection attacks.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/private_gpt/components/llm/custom/sagemaker.py",
          "line": 201,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous sql_injection sink",
          "description": "LLM output from 'self.generate_kwargs.update' is used in 'UPDATE' on line 201 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application sec",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/private_gpt/components/llm/custom/sagemaker.py",
          "line": 265,
          "category": "LLM04: Model Denial of Service",
          "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
          "description": "Function 'chat' on line 265 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust mo",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/private_gpt/components/llm/custom/sagemaker.py",
          "line": 201,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/private_gpt/components/llm/custom/sagemaker.py",
          "line": 213,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/private_gpt/components/llm/custom/sagemaker.py",
          "line": 267,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/private_gpt/components/llm/custom/sagemaker.py",
          "line": 237,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/private_gpt/components/llm/custom/sagemaker.py",
          "line": 200,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** Function with user ",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/private_gpt/components/llm/custom/sagemaker.py",
          "line": 228,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** Function with user ",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/private_gpt/server/embeddings/embeddings_router.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/private_gpt/server/chat/chat_service.py",
          "line": 185,
          "category": "LLM04: Model Denial of Service",
          "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
          "description": "Function 'chat' on line 185 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust mo",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/private_gpt/server/chat/chat_service.py",
          "line": 211,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/private_gpt/server/chat/chat_router.py",
          "line": 91,
          "category": "LLM01: Prompt Injection",
          "title": "User input 'body' embedded in LLM prompt",
          "description": "User input 'body' flows to LLM call via assignment in variable 'all_messages'. Function 'chat_completion' may be vulnerable to prompt injection attacks.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/private_gpt/server/chat/chat_router.py",
          "line": 108,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/private_gpt/server/completions/completions_router.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/private_gpt/server/ingest/ingest_router.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/private_gpt/server/ingest/ingest_service.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/private_gpt/server/chunks/chunks_service.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/private_gpt/server/recipes/summarize/summarize_service.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/scripts/utils.py",
          "line": 59,
          "category": "SQL Injection",
          "title": "SQL injection: f-string formatting",
          "description": "SQL query on line 59 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
          "is_semantic_taint": false
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/private_gpt/components/embedding/embedding_component.py",
          "line": 17,
          "category": "LLM08: Excessive Agency",
          "title": "High-risk write/execute/network operation without confirmation in '__init__'",
          "description": "Function '__init__' on line 17 performs high-risk write/execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute p",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/private_gpt/server/chat/chat_router.py",
          "line": 65,
          "category": "LLM08: Excessive Agency",
          "title": "High-risk delete/write/execute/network operation without confirmation in 'chat_completion'",
          "description": "Function 'chat_completion' on line 65 performs high-risk delete/write/execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomo",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/private_gpt/server/chat/chat_router.py",
          "line": 65,
          "category": "LLM10: Model Theft",
          "title": "Model API without rate limiting in 'chat_completion'",
          "description": "API endpoint 'chat_completion' on line 65 provides model access without rate limiting. This allows attackers to make unlimited queries to extract model knowledge, potentially stealing intellectual pro",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/private_gpt/server/completions/completions_router.py",
          "line": 53,
          "category": "LLM10: Model Theft",
          "title": "Model API without rate limiting in 'prompt_completion'",
          "description": "API endpoint 'prompt_completion' on line 53 provides model access without rate limiting. This allows attackers to make unlimited queries to extract model knowledge, potentially stealing intellectual p",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/private_gpt/server/ingest/ingest_router.py",
          "line": 41,
          "category": "LLM10: Model Theft",
          "title": "Model API without rate limiting in 'ingest_file'",
          "description": "API endpoint 'ingest_file' on line 41 provides model access without rate limiting. This allows attackers to make unlimited queries to extract model knowledge, potentially stealing intellectual propert",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/private_gpt/server/ingest/ingest_router.py",
          "line": 65,
          "category": "LLM10: Model Theft",
          "title": "Model API without rate limiting in 'ingest_text'",
          "description": "API endpoint 'ingest_text' on line 65 provides model access without rate limiting. This allows attackers to make unlimited queries to extract model knowledge, potentially stealing intellectual propert",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/private_gpt/server/chunks/chunks_router.py",
          "line": 27,
          "category": "LLM10: Model Theft",
          "title": "Model API without rate limiting in 'chunks_retrieval'",
          "description": "API endpoint 'chunks_retrieval' on line 27 provides model access without rate limiting. This allows attackers to make unlimited queries to extract model knowledge, potentially stealing intellectual pr",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/private_gpt/server/recipes/summarize/summarize_router.py",
          "line": 35,
          "category": "LLM10: Model Theft",
          "title": "Model API without rate limiting in 'summarize'",
          "description": "API endpoint 'summarize' on line 35 provides model access without rate limiting. This allows attackers to make unlimited queries to extract model knowledge, potentially stealing intellectual property ",
          "is_semantic_taint": true
        }
      ]
    },
    {
      "app": "danswer",
      "description": "Enterprise Q&A",
      "repo": "https://github.com/danswer-ai/danswer",
      "python_files": 1531,
      "files_scanned": 0,
      "total_findings": 1647,
      "critical": 968,
      "high": 481,
      "potential_vulns": [
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/examples/assistants-api/topics_analyzer.py",
          "line": 33,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output flows to command_injection sink",
          "description": "LLM output variable 'run' flows to 'client.beta.threads.runs.retrieve' on line 33 via direct flow. This creates a command_injection vulnerability.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/examples/assistants-api/topics_analyzer.py",
          "line": 109,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output flows to command_injection sink",
          "description": "LLM output variable 'run' flows to 'wait_on_run' on line 109 via direct flow. This creates a command_injection vulnerability.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/examples/assistants-api/topics_analyzer.py",
          "line": 85,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/examples/assistants-api/topics_analyzer.py",
          "line": 91,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/shared_configs/enums.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/shared_configs/configs.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/model_server/constants.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/model_server/utils.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/setup.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/main.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/onyx_openapi_schema.py",
          "line": 103,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'subprocess.run' is used in 'run(' on line 103 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/onyx_openapi_schema.py",
          "line": 68,
          "category": "LLM08: Excessive Agency",
          "title": "Direct execution of LLM-generated code in 'generate_client'",
          "description": "Function 'generate_client' on line 68 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM out",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/onyx_openapi_schema.py",
          "line": 68,
          "category": "LLM09: Overreliance",
          "title": "Direct execution of LLM output in 'generate_client'",
          "description": "Function 'generate_client' on line 68 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/chat_feedback_dump.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/sources_selection_analysis.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/api_inference_sample.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/celery_purge_queue.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/decrypt.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/reset_indexes.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/test-openapi-key.py",
          "line": 45,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential indirect prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Indirect prompt injection: Malicious instructions from external data sources\n\n**Location:** LLM call with pot",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/force_delete_connector_by_id.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/save_load_state.py",
          "line": 48,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'subprocess.run' is used in 'shell=True' on line 48 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/save_load_state.py",
          "line": 56,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'subprocess.run' is used in 'shell=True' on line 56 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/save_load_state.py",
          "line": 27,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'subprocess.run' is used in 'shell=True' on line 27 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/save_load_state.py",
          "line": 25,
          "category": "LLM05: Supply Chain Vulnerabilities",
          "title": "Network fetch combined with code execution",
          "description": "This file downloads external content (lines [67, 88]) and executes code (lines [25, 27, 48]). This pattern enables remote code execution attacks if the fetched content is not properly validated.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/save_load_state.py",
          "line": 23,
          "category": "LLM08: Excessive Agency",
          "title": "Direct execution of LLM-generated code in 'save_postgres'",
          "description": "Function 'save_postgres' on line 23 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM outpu",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/save_load_state.py",
          "line": 37,
          "category": "LLM08: Excessive Agency",
          "title": "Direct execution of LLM-generated code in 'load_postgres'",
          "description": "Function 'load_postgres' on line 37 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM outpu",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/save_load_state.py",
          "line": 23,
          "category": "LLM09: Overreliance",
          "title": "Direct execution of LLM output in 'save_postgres'",
          "description": "Function 'save_postgres' on line 23 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/save_load_state.py",
          "line": 37,
          "category": "LLM09: Overreliance",
          "title": "Direct execution of LLM output in 'load_postgres'",
          "description": "Function 'load_postgres' on line 37 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/resume_paused_connectors.py",
          "line": 6,
          "category": "LLM06: Sensitive Information Disclosure",
          "title": "Hardcoded secret detected: Secret Keyword",
          "description": "detect-secrets found a potential Secret Keyword on line 6. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/resume_paused_connectors.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/reset_postgres.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/hard_delete_chats.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/add_connector_creation_script.py",
          "line": 8,
          "category": "LLM06: Sensitive Information Disclosure",
          "title": "Hardcoded secret detected: Secret Keyword",
          "description": "detect-secrets found a potential Secret Keyword on line 8. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/add_connector_creation_script.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/dev_run_background_jobs.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/get_wikidocs.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/transform_openapi_for_docs.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/env.py",
          "line": 500,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'context.run_migrations' is used in 'run(' on line 500 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application securi",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic_tenants/env.py",
          "line": 60,
          "category": "LLM09: Overreliance",
          "title": "Critical decision without oversight in 'run_migrations_offline'",
          "description": "Function 'run_migrations_offline' on line 60 makes critical financial decisions based on LLM output without human oversight or verification. Action edges detected (HTTP/file/DB/subprocess) - risk of a",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic_tenants/env.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic_tenants/versions/a4f6ee863c47_mapping_for_anonymous_user_path.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic_tenants/versions/ac842f85f932_new_column_user_tenant_mapping.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic_tenants/versions/3b9f09038764_add_read_only_kg_user.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic_tenants/versions/14a83a331951_create_usertenantmapping_table.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic_tenants/versions/34e3630c7f32_lowercase_multi_tenant_user_auth.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic_tenants/versions/3b45e0018bf1_add_new_available_tenant_table.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/a3c1a7904cd0_remove_userfile_related_deprecated_.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/473a1a7ca408_add_display_model_names_to_llm_provider.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/da4c21c69164_chosen_assistants_changed_to_jsonb.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/238b84885828_add_foreign_key_to_user__external_user_.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/2a391f840e85_add_last_refreshed_at_mcp_server.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/58c50ef19f08_add_stale_column_to_user__external_user_.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/cf90764725d8_larger_refresh_tokens.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/2f80c6a2550f_add_chat_session_specific_temperature_.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/ed9e44312505_add_icon_name_field.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/f11b408e39d3_force_lowercase_all_users.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/f7505c5b0284_updated_constraints_for_ccpairs.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/c0aab6edb6dd_delete_workspace.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/7b9b952abdf6_update_entities.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/3781a5eb12cb_add_chunk_stats_table.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/ca04500b9ee8_add_cascade_deletes_to_agent_tables.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/0ebb1d516877_add_ccpair_deletion_failure_message.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/5e1c073d48a3_add_personal_access_token_table.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/16c37a30adf2_user_file_relationship_migration.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/b7c2b63c4a03_add_background_reindex_enabled_field.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/c5eae4a75a1b_add_chat_message__standard_answer_table.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/e1392f05e840_added_input_prompts.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential indirect prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Indirect prompt injection: Malicious instructions from external data sources\n\n**Location:** ML model detected",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/2b90f3af54b8_usage_limits.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/df46c75b714e_add_default_vision_provider_to_llm_.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/4d58345da04a_lowercase_user_emails.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/7547d982db8f_chat_folders.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/4f8a2b3c1d9e_add_open_url_tool.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/a7688ab35c45_add_public_external_user_group_table.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/3c6531f32351_add_back_input_prompts.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential indirect prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Indirect prompt injection: Malicious instructions from external data sources\n\n**Location:** ML model detected",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/4ee1287bd26a_add_multiple_slack_bot_support.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/7206234e012a_add_image_generation_config_table.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/dfbe9e93d3c7_extended_role_for_non_web.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/9087b548dd69_seed_default_image_gen_config.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/c1d2e3f4a5b6_add_deep_research_tool.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/9aadf32dfeb4_add_user_files.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/c99d76fcd298_add_nullable_to_persona_id_in_chat_.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/54a74a0417fc_danswerbot_onyxbot.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/b7ec9b5b505f_adjust_prompt_length.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/b8c9d0e1f2a3_drop_milestone_table.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/efb35676026c_standard_answer_match_regex_flag.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/5ae8240accb3_add_research_agent_database_tables_and_.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/eaa3b5593925_add_default_slack_channel_config.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/3c9a65f1207f_seed_exa_provider_from_env.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/a852cbe15577_new_chat_history.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/949b4a92a401_remove_rt.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/64bd5677aeb6_add_image_input_support_to_model_config.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/3a78dba1080a_user_file_legacy_data_cleanup.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/f71470ba9274_add_prompt_length_limit.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/c7e9f4a3b2d1_add_python_tool.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/5d12a446f5c0_add_api_version_and_deployment_name_to_.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/a4f23d6b71c8_add_llm_provider_persona_restrictions.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/8818cf73fa1a_drop_include_citations.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/bd2921608c3a_non_nullable_default_persona.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/9a0296d7421e_add_is_auto_mode_to_llm_provider.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/18b5b2524446_add_is_clarification_to_chat_message.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/5c3dca366b35_backend_driven_notification_details.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/0cd424f32b1d_user_file_data_preparation_and_backfill.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/b558f51620b4_pause_finished_user_file_connectors.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/03d710ccf29c_add_permission_sync_attempt_tables.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/b156fa702355_chat_reworked.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/2daa494a0851_add_group_sync_time.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/40926a4dab77_reset_userfile_document_id_migrated_.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/7a70b7664e37_add_model_configuration_table.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/1f2a3b4c5d6e_add_internet_search_and_content_providers.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/d1b637d7050a_sync_exa_api_key_to_content_provider.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/d961aca62eb3_update_status_length.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/505c488f6662_merge_default_assistants_into_unified.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/a8c2065484e6_add_auto_scroll_to_user_model.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/7e490836d179_nullify_default_system_prompt.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/6f4f86aef280_add_queries_and_is_web_fetch_to_.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/96a5702df6aa_mcp_tool_enabled.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/0816326d83aa_add_federated_connector_tables.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/7cc3fcc116c1_user_file_uuid_primary_key_swap.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/98a5008d8711_agent_tracking.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/52a219fb5233_add_last_synced_and_last_modified_to_document_table.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/91a0a4d62b14_milestone.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/26b931506ecb_default_chosen_assistants_to_none.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/2f95e36923e6_add_indexing_coordination.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/3d1cca026fe8_add_oauth_config_and_user_tokens.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/65bc6e0f8500_remove_kg_subtype_from_db.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/f9b8c7d6e5a4_update_parent_question_id_foreign_key_to_research_agent_iteration.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/b329d00a9ea6_adding_assistant_specific_user_.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/35e518e0ddf4_properly_cascade.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/acaab4ef4507_remove_inactive_ccpair_status_on_.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/f39c5794c10a_add_background_errors_table.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/dbaa756c2ccf_embedding_models.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/f32615f71aeb_add_custom_headers_to_tools.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/8f43500ee275_add_index.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/be2ab2aa50ee_fix_capitalization.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/dab04867cd88_add_composite_index_to_document_by_.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/699221885109_nullify_default_task_prompt.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/b7a7eee5aa15_add_checkpointing_failure_handling.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/aeda5f2df4f6_add_pinned_assistants.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/8e1ac4f39a9f_enable_contextual_retrieval.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/3fc5d75723b3_add_doc_metadata_field_in_document_model.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/9c00a2bccb83_chat_message_agentic.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/2b75d0a8ffcb_user_file_schema_cleanup.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/6756efa39ada_id_uuid_for_chat_session.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/a1b2c3d4e5f6_add_license_table.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/47a07e1a38f1_fix_invalid_model_configurations_state.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/3934b1bc7b62_update_github_connector_repo_name_to_.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/325975216eb3_add_icon_color_and_icon_shape_to_persona.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/87c52ec39f84_update_default_system_prompt.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/1b8206b29c5d_add_user_delete_cascades.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential indirect prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Indirect prompt injection: Malicious instructions from external data sources\n\n**Location:** ML model detected",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/9f696734098f_combine_search_and_chat.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/33ea50e88f24_foreign_key_input_prompts.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential indirect prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Indirect prompt injection: Malicious instructions from external data sources\n\n**Location:** ML model detected",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/da42808081e3_migrate_jira_connectors_to_new_format.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/f5437cc136c5_delete_non_search_assistants.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/09995b8811eb_add_theme_preference_to_user.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/abbfec3a5ac5_merge_prompt_into_persona.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/61ff3651add4_add_permission_syncing.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/a3795dce87be_migration_confluence_to_be_explicit.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/d25168c2beee_tool_name_consistency.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/e4334d5b33ba_add_deployment_name_to_llmprovider.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/b4ef3ae0bf6e_add_user_oauth_token_to_slack_bot.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/b30353be4eec_add_mcp_auth_performer.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/27c6ecc08586_permission_framework.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/1b10e1fda030_add_additional_data_to_notifications.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/6a804aeb4830_duplicated_no_harm_user_file_migration.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/9b66d3156fc6_user_file_schema_additions.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/23957775e5f5_remove_feedback_foreignkey_constraint.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/027381bce97c_add_shortcut_option_for_users.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/2955778aa44c_add_chunk_count_to_document.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/62c3a055a141_add_file_names_to_file_connector_config.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/6436661d5b65_add_created_at_in_project_userfile.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/3bd4c84fe72f_improved_index.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/12635f6655b7_drive_canonical_ids.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/93560ba1b118_add_web_ui_option_to_slack_config.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/0a98909f2757_enable_encrypted_fields.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/c7bf5721733e_add_has_been_indexed_to_.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/bd7c3bf8beba_migrate_agent_responses_to_research_.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/c9e2cd766c29_add_s3_file_store_table.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/90e3b9af7da4_tag_fix.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/5b29123cd710_nullable_search_settings_for_historic_.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/abe7378b8217_add_indexing_trigger_to_cc_pair.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/b388730a2899_nullable_preferences.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/4794bc13e484_update_prompt_length.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/9drpiiw74ljy_add_config_to_federated_connector.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/ac5eaac849f9_add_last_pruned_to_connector_table.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/55546a7967ee_assistant_rework.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/9cf5c00f72fe_add_creator_to_cc_pair.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/c8a93a2af083_personalization_user_info.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/6fc7886d665d_make_categories_labels_and_many_to_many.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/fec3db967bf7_add_time_updated_to_usergroup_and_.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/6d562f86c78b_remove_default_bot.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/cec7ec36c505_kgentity_parent.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/97dbb53fa8c8_add_syncrecord.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/bf7a81109301_delete_input_prompts.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential indirect prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Indirect prompt injection: Malicious instructions from external data sources\n\n**Location:** ML model detected",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/495cb26ce93e_create_knowlege_graph_tables.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/94dc3d0236f8_make_document_set_description_optional.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/797089dfb4d2_persona_start_date.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/b72ed7a5db0e_remove_description_from_starter_messages.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/7ed603b64d5a_add_mcp_server_and_connection_config_.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/c0fd6e4da83a_add_recent_assistants.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/46b7a812670f_fix_user__external_user_group_id_fk.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/33cb72ea4d80_single_tool_call_per_message.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/5e6f7a8b9c0d_update_default_persona_prompt.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/177de57c21c9_display_custom_llm_models.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/a6df6b88ef81_remove_recent_assistants.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/8405ca81cc83_notifications_constraint.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/e209dc5a8156_added_prune_frequency.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/f7a894b06d02_non_nullbale_slack_bot_id_in_channel_.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/03bf8be6b53a_rework_kg_config.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/f8a9b2c3d4e5_add_research_answer_purpose_to_chat_message.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/d09fc20a3c66_seed_builtin_tools.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/2cdeff6d8c93_set_built_in_to_default.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/36e9220ab794_update_kg_trigger_functions.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/5c7fdadae813_match_any_keywords_flag_for_standard_.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/7bd55f264e1b_add_display_name_to_model_configuration.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/e8f0d2a38171_add_status_to_mcp_server_and_make_auth_.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/47e5bef3a1d7_add_persona_categories.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/a01bf2971c5d_update_default_tool_descriptions.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/35e6853a51d5_server_default_chosen_assistants.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/query_time_check/seed_dummy_docs.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/no_bastion_analyze_tenants.py",
          "line": 54,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'subprocess.run' is used in 'run(' on line 54 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/no_bastion_analyze_tenants.py",
          "line": 63,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'subprocess.run' is used in 'run(' on line 63 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/no_bastion_analyze_tenants.py",
          "line": 142,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'subprocess.run' is used in 'run(' on line 142 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/no_bastion_analyze_tenants.py",
          "line": 121,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous sql_injection sink",
          "description": "LLM output from 'subprocess.run' is used in 'execute(' on line 121 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application security.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/no_bastion_analyze_tenants.py",
          "line": 151,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'subprocess.run' is used in 'run(' on line 151 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/no_bastion_analyze_tenants.py",
          "line": 28,
          "category": "LLM08: Excessive Agency",
          "title": "Direct execution of LLM-generated code in 'collect_tenant_data'",
          "description": "Function 'collect_tenant_data' on line 28 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/no_bastion_analyze_tenants.py",
          "line": 80,
          "category": "LLM08: Excessive Agency",
          "title": "Direct execution of LLM-generated code in 'collect_control_plane_data_from_pod'",
          "description": "Function 'collect_control_plane_data_from_pod' on line 80 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malici",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/no_bastion_analyze_tenants.py",
          "line": 28,
          "category": "LLM09: Overreliance",
          "title": "Direct execution of LLM output in 'collect_tenant_data'",
          "description": "Function 'collect_tenant_data' on line 28 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/no_bastion_analyze_tenants.py",
          "line": 80,
          "category": "LLM09: Overreliance",
          "title": "Direct execution of LLM output in 'collect_control_plane_data_from_pod'",
          "description": "Function 'collect_control_plane_data_from_pod' on line 80 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/no_bastion_mark_connectors.py",
          "line": 94,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output flows to command_injection sink",
          "description": "LLM output variable 'result' flows to 'RuntimeError' on line 94 via direct flow. This creates a command_injection vulnerability.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/no_bastion_mark_connectors.py",
          "line": 42,
          "category": "LLM08: Excessive Agency",
          "title": "Direct execution of LLM-generated code in 'run_connector_deletion'",
          "description": "Function 'run_connector_deletion' on line 42 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy ",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/no_bastion_mark_connectors.py",
          "line": 42,
          "category": "LLM09: Overreliance",
          "title": "Direct execution of LLM output in 'run_connector_deletion'",
          "description": "Function 'run_connector_deletion' on line 42 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/cleanup_utils.py",
          "line": 121,
          "category": "LLM01: Prompt Injection",
          "title": "User input 'query' embedded in LLM prompt",
          "description": "User input 'query' flows to LLM call via f-string in variable 'full_cmd'. Function 'execute_control_plane_query' may be vulnerable to prompt injection attacks.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/cleanup_utils.py",
          "line": 27,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'subprocess.run' is used in 'run(' on line 27 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/cleanup_utils.py",
          "line": 126,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'subprocess.run' is used in 'shell=True' on line 126 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/cleanup_utils.py",
          "line": 107,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'subprocess.run' is used in 'subprocess.' on line 107 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application securit",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/cleanup_utils.py",
          "line": 97,
          "category": "LLM04: Model Denial of Service",
          "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
          "description": "Function 'execute_control_plane_query' on line 97 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable a",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/cleanup_utils.py",
          "line": 23,
          "category": "LLM08: Excessive Agency",
          "title": "Direct execution of LLM-generated code in 'find_worker_pod'",
          "description": "Function 'find_worker_pod' on line 23 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM out",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/cleanup_utils.py",
          "line": 97,
          "category": "LLM08: Excessive Agency",
          "title": "Direct execution of LLM-generated code in 'execute_control_plane_query'",
          "description": "Function 'execute_control_plane_query' on line 97 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or b",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/cleanup_utils.py",
          "line": 23,
          "category": "LLM09: Overreliance",
          "title": "Direct execution of LLM output in 'find_worker_pod'",
          "description": "Function 'find_worker_pod' on line 23 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/cleanup_utils.py",
          "line": 97,
          "category": "LLM09: Overreliance",
          "title": "Direct execution of LLM output in 'execute_control_plane_query'",
          "description": "Function 'execute_control_plane_query' on line 97 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/no_bastion_cleanup_tenants.py",
          "line": 149,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output flows to command_injection sink",
          "description": "LLM output variable 'message' flows to 'RuntimeError' on line 149 via direct flow. This creates a command_injection vulnerability.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/no_bastion_cleanup_tenants.py",
          "line": 274,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output flows to command_injection sink",
          "description": "LLM output variable 'error_details' flows to 'RuntimeError' on line 274 via direct flow. This creates a command_injection vulnerability.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/no_bastion_cleanup_tenants.py",
          "line": 335,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output flows to command_injection sink",
          "description": "LLM output variable 'message' flows to 'RuntimeError' on line 335 via direct flow. This creates a command_injection vulnerability.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/no_bastion_cleanup_tenants.py",
          "line": 47,
          "category": "LLM04: Model Denial of Service",
          "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits",
          "description": "Function 'setup_scripts_on_pod' on line 47 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exh",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/no_bastion_cleanup_tenants.py",
          "line": 47,
          "category": "LLM08: Excessive Agency",
          "title": "Direct execution of LLM-generated code in 'setup_scripts_on_pod'",
          "description": "Function 'setup_scripts_on_pod' on line 47 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LL",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/no_bastion_cleanup_tenants.py",
          "line": 80,
          "category": "LLM08: Excessive Agency",
          "title": "Direct execution of LLM-generated code in 'get_tenant_index_name'",
          "description": "Function 'get_tenant_index_name' on line 80 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy L",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/no_bastion_cleanup_tenants.py",
          "line": 165,
          "category": "LLM08: Excessive Agency",
          "title": "Direct execution of LLM-generated code in 'get_tenant_users'",
          "description": "Function 'get_tenant_users' on line 165 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM o",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/no_bastion_cleanup_tenants.py",
          "line": 225,
          "category": "LLM08: Excessive Agency",
          "title": "Direct execution of LLM-generated code in 'check_documents_deleted'",
          "description": "Function 'check_documents_deleted' on line 225 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or bugg",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/no_bastion_cleanup_tenants.py",
          "line": 292,
          "category": "LLM08: Excessive Agency",
          "title": "Direct execution of LLM-generated code in 'drop_data_plane_schema'",
          "description": "Function 'drop_data_plane_schema' on line 292 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/no_bastion_cleanup_tenants.py",
          "line": 47,
          "category": "LLM09: Overreliance",
          "title": "Direct execution of LLM output in 'setup_scripts_on_pod'",
          "description": "Function 'setup_scripts_on_pod' on line 47 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/no_bastion_cleanup_tenants.py",
          "line": 80,
          "category": "LLM09: Overreliance",
          "title": "Direct execution of LLM output in 'get_tenant_index_name'",
          "description": "Function 'get_tenant_index_name' on line 80 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/no_bastion_cleanup_tenants.py",
          "line": 165,
          "category": "LLM09: Overreliance",
          "title": "Direct execution of LLM output in 'get_tenant_users'",
          "description": "Function 'get_tenant_users' on line 165 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/no_bastion_cleanup_tenants.py",
          "line": 225,
          "category": "LLM09: Overreliance",
          "title": "Direct execution of LLM output in 'check_documents_deleted'",
          "description": "Function 'check_documents_deleted' on line 225 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/no_bastion_cleanup_tenants.py",
          "line": 292,
          "category": "LLM09: Overreliance",
          "title": "Direct execution of LLM output in 'drop_data_plane_schema'",
          "description": "Function 'drop_data_plane_schema' on line 292 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/no_bastion_cleanup_utils.py",
          "line": 27,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'subprocess.run' is used in 'run(' on line 27 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/no_bastion_cleanup_utils.py",
          "line": 56,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'subprocess.run' is used in 'run(' on line 56 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/no_bastion_cleanup_utils.py",
          "line": 188,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'subprocess.run' is used in 'run(' on line 188 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/no_bastion_cleanup_utils.py",
          "line": 198,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'subprocess.run' is used in 'run(' on line 198 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/no_bastion_cleanup_utils.py",
          "line": 160,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous sql_injection sink",
          "description": "LLM output from 'subprocess.run' is used in 'execute(' on line 160 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application security.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/no_bastion_cleanup_utils.py",
          "line": 105,
          "category": "LLM04: Model Denial of Service",
          "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
          "description": "Function 'execute_control_plane_query_from_pod' on line 105 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protection",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/no_bastion_cleanup_utils.py",
          "line": 17,
          "category": "LLM08: Excessive Agency",
          "title": "Direct execution of LLM-generated code in 'find_worker_pod'",
          "description": "Function 'find_worker_pod' on line 17 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM out",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/no_bastion_cleanup_utils.py",
          "line": 46,
          "category": "LLM08: Excessive Agency",
          "title": "Direct execution of LLM-generated code in 'find_background_pod'",
          "description": "Function 'find_background_pod' on line 46 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/no_bastion_cleanup_utils.py",
          "line": 105,
          "category": "LLM08: Excessive Agency",
          "title": "Direct execution of LLM-generated code in 'execute_control_plane_query_from_pod'",
          "description": "Function 'execute_control_plane_query_from_pod' on line 105 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where mali",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/no_bastion_cleanup_utils.py",
          "line": 17,
          "category": "LLM09: Overreliance",
          "title": "Direct execution of LLM output in 'find_worker_pod'",
          "description": "Function 'find_worker_pod' on line 17 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/no_bastion_cleanup_utils.py",
          "line": 46,
          "category": "LLM09: Overreliance",
          "title": "Direct execution of LLM output in 'find_background_pod'",
          "description": "Function 'find_background_pod' on line 46 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/no_bastion_cleanup_utils.py",
          "line": 105,
          "category": "LLM09: Overreliance",
          "title": "Direct execution of LLM output in 'execute_control_plane_query_from_pod'",
          "description": "Function 'execute_control_plane_query_from_pod' on line 105 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/cleanup_tenants.py",
          "line": 103,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output flows to command_injection sink",
          "description": "LLM output variable 'message' flows to 'RuntimeError' on line 103 via direct flow. This creates a command_injection vulnerability.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/cleanup_tenants.py",
          "line": 269,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output flows to command_injection sink",
          "description": "LLM output variable 'error_details' flows to 'RuntimeError' on line 269 via direct flow. This creates a command_injection vulnerability.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/cleanup_tenants.py",
          "line": 346,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output flows to command_injection sink",
          "description": "LLM output variable 'message' flows to 'RuntimeError' on line 346 via direct flow. This creates a command_injection vulnerability.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/cleanup_tenants.py",
          "line": 45,
          "category": "LLM08: Excessive Agency",
          "title": "Direct execution of LLM-generated code in 'get_tenant_index_name'",
          "description": "Function 'get_tenant_index_name' on line 45 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy L",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/cleanup_tenants.py",
          "line": 119,
          "category": "LLM08: Excessive Agency",
          "title": "Direct execution of LLM-generated code in 'get_tenant_users'",
          "description": "Function 'get_tenant_users' on line 119 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM o",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/cleanup_tenants.py",
          "line": 201,
          "category": "LLM08: Excessive Agency",
          "title": "Direct execution of LLM-generated code in 'check_documents_deleted'",
          "description": "Function 'check_documents_deleted' on line 201 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or bugg",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/cleanup_tenants.py",
          "line": 287,
          "category": "LLM08: Excessive Agency",
          "title": "Direct execution of LLM-generated code in 'drop_data_plane_schema'",
          "description": "Function 'drop_data_plane_schema' on line 287 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/cleanup_tenants.py",
          "line": 45,
          "category": "LLM09: Overreliance",
          "title": "Direct execution of LLM output in 'get_tenant_index_name'",
          "description": "Function 'get_tenant_index_name' on line 45 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/cleanup_tenants.py",
          "line": 119,
          "category": "LLM09: Overreliance",
          "title": "Direct execution of LLM output in 'get_tenant_users'",
          "description": "Function 'get_tenant_users' on line 119 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/cleanup_tenants.py",
          "line": 201,
          "category": "LLM09: Overreliance",
          "title": "Direct execution of LLM output in 'check_documents_deleted'",
          "description": "Function 'check_documents_deleted' on line 201 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/cleanup_tenants.py",
          "line": 287,
          "category": "LLM09: Overreliance",
          "title": "Direct execution of LLM output in 'drop_data_plane_schema'",
          "description": "Function 'drop_data_plane_schema' on line 287 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/analyze_current_tenants.py",
          "line": 39,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'subprocess.run' is used in 'run(' on line 39 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/analyze_current_tenants.py",
          "line": 52,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'subprocess.run' is used in 'run(' on line 52 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/analyze_current_tenants.py",
          "line": 102,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'subprocess.run' is used in 'shell=True' on line 102 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/analyze_current_tenants.py",
          "line": 113,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'subprocess.run' is used in 'run(' on line 113 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/analyze_current_tenants.py",
          "line": 24,
          "category": "LLM08: Excessive Agency",
          "title": "Direct execution of LLM-generated code in 'collect_tenant_data'",
          "description": "Function 'collect_tenant_data' on line 24 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/analyze_current_tenants.py",
          "line": 74,
          "category": "LLM08: Excessive Agency",
          "title": "Direct execution of LLM-generated code in 'collect_control_plane_data'",
          "description": "Function 'collect_control_plane_data' on line 74 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or bu",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/analyze_current_tenants.py",
          "line": 24,
          "category": "LLM09: Overreliance",
          "title": "Direct execution of LLM output in 'collect_tenant_data'",
          "description": "Function 'collect_tenant_data' on line 24 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/analyze_current_tenants.py",
          "line": 74,
          "category": "LLM09: Overreliance",
          "title": "Direct execution of LLM output in 'collect_control_plane_data'",
          "description": "Function 'collect_control_plane_data' on line 74 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/check_no_bastion_setup.py",
          "line": 25,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'subprocess.run' is used in 'run(' on line 25 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/check_no_bastion_setup.py",
          "line": 36,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'subprocess.run' is used in 'run(' on line 36 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/check_no_bastion_setup.py",
          "line": 70,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'subprocess.run' is used in 'run(' on line 70 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/check_no_bastion_setup.py",
          "line": 113,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'subprocess.run' is used in 'run(' on line 113 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/check_no_bastion_setup.py",
          "line": 147,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'subprocess.run' is used in 'run(' on line 147 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/check_no_bastion_setup.py",
          "line": 20,
          "category": "LLM08: Excessive Agency",
          "title": "Direct execution of LLM-generated code in 'check_kubectl_access'",
          "description": "Function 'check_kubectl_access' on line 20 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LL",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/check_no_bastion_setup.py",
          "line": 65,
          "category": "LLM08: Excessive Agency",
          "title": "Direct execution of LLM-generated code in 'check_worker_pods'",
          "description": "Function 'check_worker_pods' on line 65 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM o",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/check_no_bastion_setup.py",
          "line": 108,
          "category": "LLM08: Excessive Agency",
          "title": "Direct execution of LLM-generated code in 'check_pod_exec_permission'",
          "description": "Function 'check_pod_exec_permission' on line 108 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or bu",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/check_no_bastion_setup.py",
          "line": 136,
          "category": "LLM08: Excessive Agency",
          "title": "Direct execution of LLM-generated code in 'check_pod_db_access'",
          "description": "Function 'check_pod_db_access' on line 136 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LL",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/check_no_bastion_setup.py",
          "line": 20,
          "category": "LLM09: Overreliance",
          "title": "Direct execution of LLM output in 'check_kubectl_access'",
          "description": "Function 'check_kubectl_access' on line 20 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/check_no_bastion_setup.py",
          "line": 65,
          "category": "LLM09: Overreliance",
          "title": "Direct execution of LLM output in 'check_worker_pods'",
          "description": "Function 'check_worker_pods' on line 65 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/check_no_bastion_setup.py",
          "line": 108,
          "category": "LLM09: Overreliance",
          "title": "Direct execution of LLM output in 'check_pod_exec_permission'",
          "description": "Function 'check_pod_exec_permission' on line 108 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/check_no_bastion_setup.py",
          "line": 136,
          "category": "LLM09: Overreliance",
          "title": "Direct execution of LLM output in 'check_pod_db_access'",
          "description": "Function 'check_pod_db_access' on line 136 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/mark_connectors_for_deletion.py",
          "line": 99,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output flows to command_injection sink",
          "description": "LLM output variable 'result' flows to 'RuntimeError' on line 99 via direct flow. This creates a command_injection vulnerability.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/mark_connectors_for_deletion.py",
          "line": 51,
          "category": "LLM08: Excessive Agency",
          "title": "Direct execution of LLM-generated code in 'run_connector_deletion'",
          "description": "Function 'run_connector_deletion' on line 51 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy ",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/mark_connectors_for_deletion.py",
          "line": 51,
          "category": "LLM09: Overreliance",
          "title": "Direct execution of LLM output in 'run_connector_deletion'",
          "description": "Function 'run_connector_deletion' on line 51 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/debugging/onyx_vespa_schemas.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/debugging/onyx_db.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/debugging/onyx_redis.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/debugging/onyx_list_tenants.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/lib/logger.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/debugging/litellm/directly_hit_azure_api.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/on_pod_scripts/get_tenant_users.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/on_pod_scripts/get_tenant_index_name.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/on_pod_scripts/check_documents_deleted.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/on_pod_scripts/execute_connector_deletion.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/on_pod_scripts/cleanup_tenant_schema.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/on_pod_scripts/get_tenant_connectors.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/on_pod_scripts/understand_tenants.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/interfaces.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/models.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/factory.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/credentials_provider.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/connector_runner.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/tracing/langfuse_tracing.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/tracing/openinference_tracing_processor.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/tracing/braintrust_tracing_processor.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/tracing/braintrust_tracing.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/evals/eval_cli.py",
          "line": 115,
          "category": "LLM05: Supply Chain Vulnerabilities",
          "title": "Code execution on external content",
          "description": "eval() on non-literal content on line 115. File fetches external content - HIGH RISK.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/evals/eval_cli.py",
          "line": 126,
          "category": "LLM05: Supply Chain Vulnerabilities",
          "title": "Code execution on external content",
          "description": "eval() on non-literal content on line 126. File fetches external content - HIGH RISK.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/evals/eval_cli.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/evals/eval.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/document_index/interfaces_new.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/document_index/document_index_utils.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/document_index/vespa_constants.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/tools/models.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/tools/constants.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/tools/utils.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/tools/tool_runner.py",
          "line": 120,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'tool.run' is used in 'run(' on line 120 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/tools/tool_runner.py",
          "line": 120,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/tools/tool_constructor.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/llm/models.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/llm/model_name_parser.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/llm/constants.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/llm/multi_llm.py",
          "line": 269,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** Function with user ",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/llm/factory.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/llm/model_response.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/chat/citation_processor.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/chat/models.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/chat/process_message.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/chat/llm_loop.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/chat/chat_processing_checker.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/chat/save_chat.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/chat/stop_signal_checker.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/chat/llm_step.py",
          "line": 567,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/chat/chat_state.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/chat/prompt_utils.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/chat/chat_utils.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/chat/citation_utils.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/access/models.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/access/access.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/redis/redis_object_helper.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/redis/redis_connector.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/redis/redis_connector_prune.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/redis/redis_connector_doc_perm_sync.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/redis/redis_document_set.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/redis/redis_connector_delete.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/redis/redis_connector_ext_group_sync.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/redis/redis_pool.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/redis/redis_usergroup.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/redis/redis_connector_stop.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/auth/disposable_email_validator.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/auth/email_utils.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/auth/users.py",
          "line": 614,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/auth/oauth_token_manager.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/auth/constants.py",
          "line": 4,
          "category": "LLM06: Sensitive Information Disclosure",
          "title": "Hardcoded secret detected: Secret Keyword",
          "description": "detect-secrets found a potential Secret Keyword on line 4. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/auth/constants.py",
          "line": 5,
          "category": "LLM06: Sensitive Information Disclosure",
          "title": "Hardcoded secret detected: Secret Keyword",
          "description": "detect-secrets found a potential Secret Keyword on line 5. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/auth/constants.py",
          "line": 13,
          "category": "LLM06: Sensitive Information Disclosure",
          "title": "Hardcoded secret detected: Secret Keyword",
          "description": "detect-secrets found a potential Secret Keyword on line 13. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/auth/constants.py",
          "line": 14,
          "category": "LLM06: Sensitive Information Disclosure",
          "title": "Hardcoded secret detected: Secret Keyword",
          "description": "detect-secrets found a potential Secret Keyword on line 14. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/auth/constants.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/auth/captcha.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/auth/schemas.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/auth/utils.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/auth/noauth_user.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/auth/invited_users.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/auth/jwt.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/auth/oauth_refresher.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/file_processing/password_validation.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/file_processing/image_summarization.py",
          "line": 96,
          "category": "LLM04: Model Denial of Service",
          "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
          "description": "Function '_summarize_image' on line 96 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/file_processing/file_types.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/file_processing/html_utils.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/file_processing/extract_file_text.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/file_processing/unstructured.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/file_store/s3_key_utils.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/file_store/file_store.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/file_store/models.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/file_store/utils.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/file_store/document_batch_storage.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/seeding/load_yamls.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential indirect prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Indirect prompt injection: Malicious instructions from external data sources\n\n**Location:** ML model detected",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/api_key_usage.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/saml.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/usage_limits.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/tenant_usage_limits.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/auth_check.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/utils/variable_functionality.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/utils/b64.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/utils/subclasses.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/utils/threadpool_concurrency.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/utils/supervisord_watchdog.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/utils/text_processing.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/utils/sitemap.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/utils/object_size_check.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/utils/telemetry.py",
          "line": 140,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'current_context.run' is used in 'run(' on line 140 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/utils/telemetry.py",
          "line": 95,
          "category": "LLM04: Model Denial of Service",
          "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
          "description": "Function 'optional_telemetry' on line 95 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers ",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/utils/telemetry.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/utils/logger.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/utils/url.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/utils/file.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/utils/timing.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/utils/long_term_log.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/utils/web_content.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/utils/gpu_utils.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/utils/headers.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/utils/middleware.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/secondary_llm_flows/starter_message_creation.py",
          "line": 190,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output flows to command_injection sink",
          "description": "LLM output variable 'functions' flows to 'run_functions_in_parallel' on line 190 via direct flow. This creates a command_injection vulnerability.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/secondary_llm_flows/starter_message_creation.py",
          "line": 109,
          "category": "LLM04: Model Denial of Service",
          "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
          "description": "Function 'generate_starter_messages' on line 109 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable at",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/secondary_llm_flows/starter_message_creation.py",
          "line": 145,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/secondary_llm_flows/chunk_usefulness.py",
          "line": 49,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output flows to sql_injection sink",
          "description": "LLM output variable 'model_output' flows to '_extract_usefulness' on line 49 via direct flow. This creates a sql_injection vulnerability.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/secondary_llm_flows/chunk_usefulness.py",
          "line": 14,
          "category": "LLM04: Model Denial of Service",
          "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
          "description": "Function 'llm_eval_section' on line 14 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/secondary_llm_flows/chunk_usefulness.py",
          "line": 43,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/secondary_llm_flows/chunk_usefulness.py",
          "line": 14,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** Function with user ",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/secondary_llm_flows/query_expansion.py",
          "line": 229,
          "category": "LLM01: Prompt Injection",
          "title": "User input 'query' embedded in LLM prompt",
          "description": "User input 'query' flows to LLM call via format_call in variable 'prompt'. Function 'llm_multilingual_query_expansion' may be vulnerable to prompt injection attacks.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/secondary_llm_flows/query_expansion.py",
          "line": 226,
          "category": "LLM04: Model Denial of Service",
          "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
          "description": "Function 'llm_multilingual_query_expansion' on line 226 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections en",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/secondary_llm_flows/answer_validation.py",
          "line": 26,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output flows to sql_injection sink",
          "description": "LLM output variable 'model_output' flows to '_extract_validity' on line 26 via direct flow. This creates a sql_injection vulnerability.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/secondary_llm_flows/answer_validation.py",
          "line": 11,
          "category": "LLM04: Model Denial of Service",
          "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
          "description": "Function 'get_answer_validity' on line 11 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/secondary_llm_flows/time_filter.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/secondary_llm_flows/document_filter.py",
          "line": 111,
          "category": "LLM01: Prompt Injection",
          "title": "User input 'section_text' embedded in LLM prompt",
          "description": "User input 'section_text' flows to LLM call via format_call in variable 'prompt_text'. Function 'classify_section_relevance' may be vulnerable to prompt injection attacks.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/secondary_llm_flows/document_filter.py",
          "line": 124,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous sql_injection sink",
          "description": "LLM output from 'llm.invoke' is used in 'SELECT' on line 124 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application security.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/secondary_llm_flows/document_filter.py",
          "line": 258,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous sql_injection sink",
          "description": "LLM output from 'llm.invoke' is used in 'SELECT' on line 258 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application security.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/secondary_llm_flows/document_filter.py",
          "line": 90,
          "category": "LLM04: Model Denial of Service",
          "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
          "description": "Function 'classify_section_relevance' on line 90 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable at",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/secondary_llm_flows/document_filter.py",
          "line": 124,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/secondary_llm_flows/document_filter.py",
          "line": 258,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/federated_connectors/oauth_utils.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/federated_connectors/factory.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/federated_connectors/federated_retrieval.py",
          "line": 131,
          "category": "LLM06: Sensitive Information Disclosure",
          "title": "Hardcoded secret detected: Secret Keyword",
          "description": "detect-secrets found a potential Secret Keyword on line 131. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/federated_connectors/federated_retrieval.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/natural_language_processing/constants.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/natural_language_processing/utils.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/configs/chat_configs.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/configs/constants.py",
          "line": 21,
          "category": "LLM06: Sensitive Information Disclosure",
          "title": "Hardcoded secret detected: Secret Keyword",
          "description": "detect-secrets found a potential Secret Keyword on line 21. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/configs/constants.py",
          "line": 93,
          "category": "LLM06: Sensitive Information Disclosure",
          "title": "Hardcoded secret detected: Secret Keyword",
          "description": "detect-secrets found a potential Secret Keyword on line 93. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/configs/constants.py",
          "line": 94,
          "category": "LLM06: Sensitive Information Disclosure",
          "title": "Hardcoded secret detected: Secret Keyword",
          "description": "detect-secrets found a potential Secret Keyword on line 94. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/configs/constants.py",
          "line": 100,
          "category": "LLM06: Sensitive Information Disclosure",
          "title": "Hardcoded secret detected: Secret Keyword",
          "description": "detect-secrets found a potential Secret Keyword on line 100. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/configs/constants.py",
          "line": 107,
          "category": "LLM06: Sensitive Information Disclosure",
          "title": "Hardcoded secret detected: Secret Keyword",
          "description": "detect-secrets found a potential Secret Keyword on line 107. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/configs/constants.py",
          "line": 109,
          "category": "LLM06: Sensitive Information Disclosure",
          "title": "Hardcoded secret detected: Secret Keyword",
          "description": "detect-secrets found a potential Secret Keyword on line 109. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/configs/constants.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/configs/app_configs.py",
          "line": 282,
          "category": "LLM06: Sensitive Information Disclosure",
          "title": "Hardcoded secret detected: Secret Keyword",
          "description": "detect-secrets found a potential Secret Keyword on line 282. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/configs/app_configs.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/configs/agent_configs.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/configs/model_configs.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/configs/onyxbot_configs.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/configs/tool_configs.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/kg/models.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/prompts/chat_tools.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/prompts/filter_extration.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/prompts/query_validation.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential indirect prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Indirect prompt injection: Malicious instructions from external data sources\n\n**Location:** ML model detected",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/prompts/constants.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/prompts/chat_prompts.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/prompts/tool_prompts.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/prompts/prompt_utils.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/prompts/search_prompts.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/prompts/direct_qa_prompts.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/prompts/contextual_retrieval.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/prompts/kg_prompts.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential indirect prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Indirect prompt injection: Malicious instructions from external data sources\n\n**Location:** ML model detected",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/prompts/prompt_template.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/kg_config.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/image_generation.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/auth.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/deletion_attempt.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/user_preferences.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/pat.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/release_notes.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/enums.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/sync_record.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/tasks.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/connector_credential_pair.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/models.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential indirect prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Indirect prompt injection: Malicious instructions from external data sources\n\n**Location:** ML model detected",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/token_limit.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/users.py",
          "line": 229,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/users.py",
          "line": 275,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/credentials.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/oauth_config.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/swap_index.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/index_attempt.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/feedback.py",
          "line": 267,
          "category": "LLM04: Model Denial of Service",
          "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
          "description": "Function 'remove_chat_message_feedback' on line 267 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/feedback.py",
          "line": 281,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/notification.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/tools.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/saml.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/constants.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/pydantic_type.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/llm.py",
          "line": 304,
          "category": "LLM04: Model Denial of Service",
          "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits",
          "description": "Function 'sync_model_configurations' on line 304 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers ",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/llm.py",
          "line": 340,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/llm.py",
          "line": 341,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/llm.py",
          "line": 342,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/persona.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/chunk.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/api_key.py",
          "line": 67,
          "category": "LLM04: Model Denial of Service",
          "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
          "description": "Function 'insert_api_key' on line 67 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to e",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/api_key.py",
          "line": 83,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/entity_type.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/chat.py",
          "line": 148,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous sql_injection sink",
          "description": "LLM output from 'ChatMessage__SearchDoc.chat_message_id.is_' is used in 'DELETE' on line 148 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise ",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/chat.py",
          "line": 422,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous sql_injection sink",
          "description": "LLM output from 'ChatMessage.chat_session_id.in_' is used in 'execute(' on line 422 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise applicati",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/chat.py",
          "line": 409,
          "category": "LLM04: Model Denial of Service",
          "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting, No input length validation, No timeout configuration, No token/context limits",
          "description": "Function 'get_chat_messages_by_sessions' on line 409 has 5 DoS risk(s): LLM calls in loops, No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missi",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/chat.py",
          "line": 498,
          "category": "LLM04: Model Denial of Service",
          "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
          "description": "Function 'get_chat_messages_by_session' on line 498 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/chat.py",
          "line": 952,
          "category": "LLM04: Model Denial of Service",
          "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
          "description": "Function 'update_db_session_with_messages' on line 952 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections ena",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/chat.py",
          "line": 146,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/chat.py",
          "line": 421,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/chat.py",
          "line": 512,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/chat.py",
          "line": 966,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/chat.py",
          "line": 126,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/chat.py",
          "line": 539,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/chat.py",
          "line": 148,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/chat.py",
          "line": 952,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** Function with user ",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/indexing_coordination.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/chat_search.py",
          "line": 85,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/chat_search.py",
          "line": 18,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** Function with user ",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/mcp.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/search_settings.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/utils.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/document_set.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/federated.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/document.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/connector.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/slack_channel_config.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/permission_sync_attempt.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/entities.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/input_prompt.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/projects.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/web_search.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/usage.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/tag.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/relationships.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/user_file.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/slack_bot.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/key_value_store/store.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/indexing/models.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/indexing/chunker.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/indexing/embedder.py",
          "line": 173,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output flows to sql_injection sink",
          "description": "LLM output variable 'title_embeddings' flows to 'title_embed_dict.update' on line 173 via direct flow. This creates a sql_injection vulnerability.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/indexing/embedder.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/deep_research/dr_loop.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/deep_research/utils.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/deep_research/dr_mock_tools.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/mcp_server/api.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/mcp_server/utils.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/mcp_server/tools/search.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/indexing/adapters/user_file_indexing_adapter.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/indexing/adapters/document_indexing_adapter.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/seeding/chat_history_seeding.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/engine/connection_warmup.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/engine/async_sql_engine.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/engine/iam_auth.py",
          "line": 14,
          "category": "LLM04: Model Denial of Service",
          "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
          "description": "Function 'get_iam_auth_token' on line 14 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers ",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/engine/iam_auth.py",
          "line": 21,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/engine/sql_engine.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/_deprecated/pg_file_store.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/prompts/deep_research/dr_tool_prompts.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/prompts/deep_research/research_agent.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/prompts/deep_research/orchestration_layer.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/kg/vespa/vespa_interactions.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/kg/setup/kg_default_entity_definitions.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/kg/clustering/clustering.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/kg/clustering/normalizations.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/kg/utils/extraction_utils.py",
          "line": 371,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output flows to sql_injection sink",
          "description": "LLM output variable 'chunk_batch_results' flows to 'result.deep_extracted_entities.update' on line 371 via direct flow. This creates a sql_injection vulnerability.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/kg/utils/extraction_utils.py",
          "line": 374,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output flows to sql_injection sink",
          "description": "LLM output variable 'chunk_batch_results' flows to 'result.deep_extracted_relationships.update' on line 374 via direct flow. This creates a sql_injection vulnerability.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/kg/utils/extraction_utils.py",
          "line": 496,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output flows to sql_injection sink",
          "description": "LLM output variable 'parsed_result' flows to 'KGDocumentDeepExtractionResults' on line 496 via direct flow. This creates a sql_injection vulnerability.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/kg/utils/extraction_utils.py",
          "line": 420,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/kg/utils/extraction_utils.py",
          "line": 484,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/kg/utils/formatting_utils.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/kg/resets/reset_vespa.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/kg/extractions/extraction_processing.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/federated_connectors/slack/federated_connector.py",
          "line": 310,
          "category": "LLM05: Supply Chain Vulnerabilities",
          "title": "Code execution on external content",
          "description": "eval() on non-literal content on line 310. File fetches external content - HIGH RISK.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/federated_connectors/slack/federated_connector.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/onyxbot/slack/config.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/onyxbot/slack/models.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/onyxbot/slack/constants.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/onyxbot/slack/formatting.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/onyxbot/slack/utils.py",
          "line": 250,
          "category": "LLM01: Prompt Injection",
          "title": "User input 'text' embedded in LLM prompt",
          "description": "User input parameter 'text' is directly passed to LLM API call 'client.chat_postMessage'. This is a high-confidence prompt injection vector.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/onyxbot/slack/utils.py",
          "line": 273,
          "category": "LLM01: Prompt Injection",
          "title": "User input 'text' embedded in LLM prompt",
          "description": "User input parameter 'text' is directly passed to LLM API call 'client.chat_postMessage'. This is a high-confidence prompt injection vector.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/onyxbot/slack/utils.py",
          "line": 287,
          "category": "LLM01: Prompt Injection",
          "title": "User input 'text' embedded in LLM prompt",
          "description": "User input parameter 'text' is directly passed to LLM API call 'client.chat_postEphemeral'. This is a high-confidence prompt injection vector.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/onyxbot/slack/utils.py",
          "line": 311,
          "category": "LLM01: Prompt Injection",
          "title": "User input 'text' embedded in LLM prompt",
          "description": "User input parameter 'text' is directly passed to LLM API call 'client.chat_postEphemeral'. This is a high-confidence prompt injection vector.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/onyxbot/slack/utils.py",
          "line": 759,
          "category": "LLM01: Prompt Injection",
          "title": "User input 'message' embedded in LLM prompt",
          "description": "User input parameter 'message' is directly passed to LLM API call 'super().run_message_listeners'. This is a high-confidence prompt injection vector.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/onyxbot/slack/blocks.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/onyxbot/slack/listener.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/onyxbot/slack/handlers/handle_buttons.py",
          "line": 556,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous sql_injection sink",
          "description": "LLM output from 'client.web_client.chat_delete' is used in 'DELETE' on line 556 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application s",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/onyxbot/slack/handlers/handle_buttons.py",
          "line": 362,
          "category": "LLM04: Model Denial of Service",
          "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
          "description": "Function 'handle_slack_feedback' on line 362 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attack",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/onyxbot/slack/handlers/handle_buttons.py",
          "line": 424,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/onyxbot/slack/handlers/handle_buttons.py",
          "line": 556,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/onyxbot/slack/handlers/handle_message.py",
          "line": 96,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous sql_injection sink",
          "description": "LLM output from 'client.chat_deleteScheduledMessage' is used in 'DELETE' on line 96 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise applicati",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/onyxbot/slack/handlers/handle_regular_answer.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/settings/store.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/settings/models.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/settings/api.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/onyx_api/ingestion.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/runtime/onyx_runtime.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/long_term_logs/long_term_logs_api.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/kg/api.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/manage/models.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/manage/users.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/manage/search_settings.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/manage/validate_tokens.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/manage/administrative.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/manage/slack_bot.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/documents/models.py",
          "line": 316,
          "category": "LLM04: Model Denial of Service",
          "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
          "description": "Function 'from_models' on line 316 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exh",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/documents/models.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/documents/standard_oauth.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/documents/document.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/documents/cc_pair.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/documents/connector.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/documents/credential.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/query_and_chat/chat_backend.py",
          "line": 220,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous sql_injection sink",
          "description": "LLM output from 'chat_session.current_alternate_model.lower' is used in 'UPDATE' on line 220 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise ",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/query_and_chat/chat_backend.py",
          "line": 195,
          "category": "LLM04: Model Denial of Service",
          "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
          "description": "Function 'update_chat_session_temperature' on line 195 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections ena",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/query_and_chat/chat_backend.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/query_and_chat/models.py",
          "line": 274,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous sql_injection sink",
          "description": "LLM output from 'model.time_created.isoformat' is used in 'UPDATE' on line 274 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application se",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/query_and_chat/models.py",
          "line": 275,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous sql_injection sink",
          "description": "LLM output from 'model.time_updated.isoformat' is used in 'UPDATE' on line 275 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application se",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/query_and_chat/models.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/query_and_chat/streaming_utils.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/query_and_chat/query_backend.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/query_and_chat/streaming_models.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/query_and_chat/session_loading.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/federated/api.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/pat/api.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/manage/web_search/api.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/manage/llm/models.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/manage/llm/api.py",
          "line": 575,
          "category": "LLM04: Model Denial of Service",
          "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits",
          "description": "Function 'get_bedrock_available_models' on line 575 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attacke",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/manage/llm/utils.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/manage/image_generation/api.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/features/web_search/api.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/features/release_notes/constants.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/features/release_notes/utils.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/features/projects/api.py",
          "line": 510,
          "category": "LLM04: Model Denial of Service",
          "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
          "description": "Function 'move_chat_session' on line 510 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers ",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/features/projects/api.py",
          "line": 530,
          "category": "LLM04: Model Denial of Service",
          "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
          "description": "Function 'remove_chat_session' on line 530 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attacker",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/features/projects/api.py",
          "line": 518,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/features/projects/api.py",
          "line": 537,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/features/projects/projects_file_utils.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/features/document_set/api.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/features/mcp/api.py",
          "line": 246,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/features/mcp/api.py",
          "line": 503,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/features/input_prompt/api.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/features/default_assistant/api.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/features/persona/api.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/features/oauth_config/api.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/features/notifications/api.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/features/tool/tool_visibility.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/features/tool/api.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/context/search/enums.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/context/search/models.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/context/search/utils.py",
          "line": 77,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/context/search/pipeline.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/context/search/retrieval/search_runner.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/context/search/federated/slack_search.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/context/search/federated/slack_search_utils.py",
          "line": 136,
          "category": "LLM04: Model Denial of Service",
          "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
          "description": "Function 'extract_date_range_from_query' on line 136 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enabl",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/llm/well_known_providers/auto_update_service.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/llm/well_known_providers/constants.py",
          "line": 36,
          "category": "LLM06: Sensitive Information Disclosure",
          "title": "Hardcoded secret detected: Secret Keyword",
          "description": "detect-secrets found a potential Secret Keyword on line 36. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/llm/well_known_providers/constants.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/llm/well_known_providers/llm_provider_options.py",
          "line": 187,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous sql_injection sink",
          "description": "LLM output from 'litellm.model_cost.keys' is used in 'UPDATE' on line 187 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application securit",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/llm/well_known_providers/llm_provider_options.py",
          "line": 130,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous code_execution sink",
          "description": "LLM output from 'model.lower' is used in 'compile(' on line 130 without sanitization. This creates a code_execution vulnerability where malicious LLM output can compromise application security.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/llm/well_known_providers/llm_provider_options.py",
          "line": 167,
          "category": "LLM04: Model Denial of Service",
          "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits",
          "description": "Function 'get_vertexai_model_names' on line 167 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers t",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/llm/well_known_providers/llm_provider_options.py",
          "line": 102,
          "category": "LLM08: Excessive Agency",
          "title": "Direct execution of LLM-generated code in 'get_openai_model_names'",
          "description": "Function 'get_openai_model_names' on line 102 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/llm/well_known_providers/llm_provider_options.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/llm/prompt_cache/cache_manager.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/llm/prompt_cache/__init__.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential indirect prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Indirect prompt injection: Malicious instructions from external data sources\n\n**Location:** ML model detected",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/llm/prompt_cache/processor.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/llm/prompt_cache/utils.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/llm/litellm_singleton/config.py",
          "line": 137,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous sql_injection sink",
          "description": "LLM output from 'litellm.model_cost[model_key].update' is used in 'UPDATE' on line 137 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise applic",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/llm/litellm_singleton/config.py",
          "line": 112,
          "category": "LLM04: Model Denial of Service",
          "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits",
          "description": "Function 'load_model_metadata_enrichments' on line 112 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable atta",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/llm/litellm_singleton/monkey_patches.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/llm/prompt_cache/providers/__init__.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential indirect prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Indirect prompt injection: Malicious instructions from external data sources\n\n**Location:** ML model detected",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/llm/prompt_cache/providers/vertex.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/llm/prompt_cache/providers/anthropic.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/tools/fake_tools/research_agent.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/tools/tool_implementations/search_like_tool_utils.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/tools/tool_implementations/utils.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/tools/tool_implementations/web_search/models.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/tools/tool_implementations/web_search/providers.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/tools/tool_implementations/web_search/utils.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/tools/tool_implementations/web_search/web_search_tool.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/tools/tool_implementations/knowledge_graph/knowledge_graph_tool.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/tools/tool_implementations/open_url/onyx_web_crawler.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/tools/tool_implementations/open_url/open_url_tool.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/tools/tool_implementations/open_url/url_normalization.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/tools/tool_implementations/python/python_tool.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/tools/tool_implementations/python/code_interpreter_client.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/tools/tool_implementations/images/image_generation_tool.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/tools/tool_implementations/mcp/mcp_client.py",
          "line": 119,
          "category": "LLM05: Supply Chain Vulnerabilities",
          "title": "Network fetch combined with code execution",
          "description": "This file downloads external content (lines [107]) and executes code (lines [119]). This pattern enables remote code execution attacks if the fetched content is not properly validated.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/tools/tool_implementations/mcp/mcp_client.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/tools/tool_implementations/mcp/mcp_tool.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/tools/tool_implementations/search/search_tool.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/tools/tool_implementations/search/constants.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/tools/tool_implementations/search/search_utils.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/tools/tool_implementations/custom/custom_tool_prompts.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/tools/tool_implementations/custom/openapi_parsing.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/tools/tool_implementations/custom/custom_tool.py",
          "line": 336,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/tools/tool_implementations/custom/custom_tool.py",
          "line": 337,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/tools/tool_implementations/web_search/clients/google_pse_client.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/tools/tool_implementations/web_search/clients/exa_client.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/tools/tool_implementations/web_search/clients/searxng_client.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/document_index/vespa/index.py",
          "line": 709,
          "category": "LLM05: Supply Chain Vulnerabilities",
          "title": "Code execution on external content",
          "description": "eval() on non-literal content on line 709. File fetches external content - HIGH RISK.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/document_index/vespa/index.py",
          "line": 735,
          "category": "LLM05: Supply Chain Vulnerabilities",
          "title": "Code execution on external content",
          "description": "eval() on non-literal content on line 735. File fetches external content - HIGH RISK.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/document_index/vespa/index.py",
          "line": 742,
          "category": "LLM05: Supply Chain Vulnerabilities",
          "title": "Code execution on external content",
          "description": "eval() on non-literal content on line 742. File fetches external content - HIGH RISK.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/document_index/vespa/index.py",
          "line": 777,
          "category": "LLM05: Supply Chain Vulnerabilities",
          "title": "Code execution on external content",
          "description": "eval() on non-literal content on line 777. File fetches external content - HIGH RISK.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/document_index/vespa/index.py",
          "line": 787,
          "category": "LLM05: Supply Chain Vulnerabilities",
          "title": "Code execution on external content",
          "description": "eval() on non-literal content on line 787. File fetches external content - HIGH RISK.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/document_index/vespa/index.py",
          "line": 1018,
          "category": "LLM05: Supply Chain Vulnerabilities",
          "title": "Code execution on external content",
          "description": "eval() on non-literal content on line 1018. File fetches external content - HIGH RISK.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/document_index/vespa/index.py",
          "line": 1038,
          "category": "LLM05: Supply Chain Vulnerabilities",
          "title": "Code execution on external content",
          "description": "eval() on non-literal content on line 1038. File fetches external content - HIGH RISK.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/document_index/vespa/vespa_document_index.py",
          "line": 500,
          "category": "LLM05: Supply Chain Vulnerabilities",
          "title": "Code execution on external content",
          "description": "eval() on non-literal content on line 500. File fetches external content - HIGH RISK.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/document_index/vespa/vespa_document_index.py",
          "line": 519,
          "category": "LLM05: Supply Chain Vulnerabilities",
          "title": "Code execution on external content",
          "description": "eval() on non-literal content on line 519. File fetches external content - HIGH RISK.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/document_index/vespa/vespa_document_index.py",
          "line": 529,
          "category": "LLM05: Supply Chain Vulnerabilities",
          "title": "Code execution on external content",
          "description": "eval() on non-literal content on line 529. File fetches external content - HIGH RISK.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/document_index/vespa/vespa_document_index.py",
          "line": 539,
          "category": "LLM05: Supply Chain Vulnerabilities",
          "title": "Code execution on external content",
          "description": "eval() on non-literal content on line 539. File fetches external content - HIGH RISK.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/document_index/vespa/vespa_document_index.py",
          "line": 600,
          "category": "LLM05: Supply Chain Vulnerabilities",
          "title": "Code execution on external content",
          "description": "eval() on non-literal content on line 600. File fetches external content - HIGH RISK.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/document_index/vespa/chunk_retrieval.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/document_index/vespa/kg_interactions.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/document_index/opensearch/client.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/document_index/opensearch/constants.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/document_index/opensearch/opensearch_document_index.py",
          "line": 295,
          "category": "LLM05: Supply Chain Vulnerabilities",
          "title": "Code execution on external content",
          "description": "eval() on non-literal content on line 295. File fetches external content - HIGH RISK.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/document_index/opensearch/opensearch_document_index.py",
          "line": 311,
          "category": "LLM05: Supply Chain Vulnerabilities",
          "title": "Code execution on external content",
          "description": "eval() on non-literal content on line 311. File fetches external content - HIGH RISK.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/document_index/opensearch/opensearch_document_index.py",
          "line": 315,
          "category": "LLM05: Supply Chain Vulnerabilities",
          "title": "Code execution on external content",
          "description": "eval() on non-literal content on line 315. File fetches external content - HIGH RISK.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/document_index/opensearch/opensearch_document_index.py",
          "line": 336,
          "category": "LLM05: Supply Chain Vulnerabilities",
          "title": "Code execution on external content",
          "description": "eval() on non-literal content on line 336. File fetches external content - HIGH RISK.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/document_index/opensearch/opensearch_document_index.py",
          "line": 346,
          "category": "LLM05: Supply Chain Vulnerabilities",
          "title": "Code execution on external content",
          "description": "eval() on non-literal content on line 346. File fetches external content - HIGH RISK.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/document_index/opensearch/opensearch_document_index.py",
          "line": 357,
          "category": "LLM05: Supply Chain Vulnerabilities",
          "title": "Code execution on external content",
          "description": "eval() on non-literal content on line 357. File fetches external content - HIGH RISK.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/document_index/opensearch/opensearch_document_index.py",
          "line": 362,
          "category": "LLM05: Supply Chain Vulnerabilities",
          "title": "Code execution on external content",
          "description": "eval() on non-literal content on line 362. File fetches external content - HIGH RISK.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/document_index/opensearch/opensearch_document_index.py",
          "line": 499,
          "category": "LLM05: Supply Chain Vulnerabilities",
          "title": "Code execution on external content",
          "description": "eval() on non-literal content on line 499. File fetches external content - HIGH RISK.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/document_index/opensearch/opensearch_document_index.py",
          "line": 535,
          "category": "LLM05: Supply Chain Vulnerabilities",
          "title": "Code execution on external content",
          "description": "eval() on non-literal content on line 535. File fetches external content - HIGH RISK.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/document_index/opensearch/opensearch_document_index.py",
          "line": 566,
          "category": "LLM05: Supply Chain Vulnerabilities",
          "title": "Code execution on external content",
          "description": "eval() on non-literal content on line 566. File fetches external content - HIGH RISK.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/document_index/opensearch/opensearch_document_index.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/document_index/opensearch/search.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/document_index/opensearch/schema.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/document_index/vespa/shared_utils/utils.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/document_index/vespa/shared_utils/vespa_request_builders.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/evals/providers/local.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/evals/providers/braintrust.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/evals/one_off/create_braintrust_dataset.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/tracing/framework/provider.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/tracing/framework/spans.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/freshdesk/connector.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/guru/connector.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/productboard/connector.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/imap/models.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/imap/connector.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/discord/connector.py",
          "line": 184,
          "category": "LLM04: Model Denial of Service",
          "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits",
          "description": "Function '_manage_async_retrieval' on line 184 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/discord/connector.py",
          "line": 227,
          "category": "LLM04: Model Denial of Service",
          "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits",
          "description": "Function 'run_and_yield' on line 227 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust m",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/discord/connector.py",
          "line": 184,
          "category": "LLM08: Excessive Agency",
          "title": "Direct execution of LLM-generated code in '_manage_async_retrieval'",
          "description": "Function '_manage_async_retrieval' on line 184 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or bugg",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/discord/connector.py",
          "line": 184,
          "category": "LLM09: Overreliance",
          "title": "Direct execution of LLM output in '_manage_async_retrieval'",
          "description": "Function '_manage_async_retrieval' on line 184 directly executes LLM-generated code using eval(. This is extremely dangerous and allows arbitrary code execution.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/discord/connector.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/gitlab/connector.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/drupal_wiki/models.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/drupal_wiki/connector.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/xenforo/connector.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/slab/connector.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/cross_connector_utils/miscellaneous_utils.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/cross_connector_utils/rate_limit_wrapper.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/file/connector.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/web/connector.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/google_utils/google_auth.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/google_utils/google_kv.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/google_utils/google_utils.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/google_utils/resources.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/google_utils/shared_constants.py",
          "line": 24,
          "category": "LLM06: Sensitive Information Disclosure",
          "title": "Hardcoded secret detected: Secret Keyword",
          "description": "detect-secrets found a potential Secret Keyword on line 24. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/google_utils/shared_constants.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/document360/connector.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/linear/connector.py",
          "line": 138,
          "category": "LLM06: Sensitive Information Disclosure",
          "title": "Hardcoded secret detected: Secret Keyword",
          "description": "detect-secrets found a potential Secret Keyword on line 138. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/linear/connector.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/highspot/client.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/highspot/utils.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/highspot/connector.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/bookstack/client.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/bookstack/connector.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/clickup/connector.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/zulip/utils.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/zulip/connector.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/testrail/connector.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/notion/connector.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/hubspot/rate_limit.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/hubspot/connector.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/fireflies/connector.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/jira/utils.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/jira/connector.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/github/utils.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/outline/client.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/outline/connector.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/zendesk/connector.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/confluence/onyx_confluence.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/confluence/utils.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/confluence/connector.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/sharepoint/connector.py",
          "line": 151,
          "category": "LLM06: Sensitive Information Disclosure",
          "title": "Hardcoded secret detected: Secret Keyword",
          "description": "detect-secrets found a potential Secret Keyword on line 151. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/google_site/connector.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/teams/models.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/teams/utils.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/teams/connector.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/discourse/connector.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/dropbox/connector.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/salesforce/doc_conversion.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/salesforce/sqlite_functions.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/salesforce/utils.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/salesforce/connector.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/salesforce/onyx_salesforce.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/slack/utils.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/slack/onyx_slack_web_client.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/slack/onyx_retry_handler.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/axero/connector.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/loopio/connector.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/gitbook/connector.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/bitbucket/utils.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/bitbucket/connector.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/blob/connector.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/gong/connector.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/asana/asana_api.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/asana/connector.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/mediawiki/family.py",
          "line": 122,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'generator.run' is used in 'run(' on line 122 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/mediawiki/family.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/mediawiki/wiki.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/coda/connector.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/gmail/connector.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/egnyte/connector.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/google_drive/section_extraction.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/google_drive/models.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/google_drive/doc_conversion.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/google_drive/constants.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/google_drive/connector.py",
          "line": 1494,
          "category": "LLM06: Sensitive Information Disclosure",
          "title": "Hardcoded secret detected: Secret Keyword",
          "description": "detect-secrets found a potential Secret Keyword on line 1494. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/google_drive/connector.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/google_drive/file_retrieval.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/salesforce/shelve_stuff/old_test_salesforce_shelves.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/salesforce/shelve_stuff/shelve_functions.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/celery/celery_redis.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/celery/celery_utils.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/celery/celery_k8s_probe.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/celery/memory_monitoring.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/indexing/job_client.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/indexing/run_docfetching.py",
          "line": 537,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'connector_runner.run' is used in 'run(' on line 537 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/indexing/run_docfetching.py",
          "line": 309,
          "category": "LLM04: Model Denial of Service",
          "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits",
          "description": "Function 'connector_document_extraction' on line 309 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attack",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/indexing/checkpointing_utils.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/indexing/memory_tracer.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/celery/tasks/models.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/celery/tasks/beat_schedule.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/celery/configs/base.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/celery/apps/kg_processing.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/celery/apps/light.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/celery/apps/user_file_processing.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/celery/apps/background.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/celery/apps/heavy.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/celery/apps/app_base.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/celery/apps/docprocessing.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/celery/apps/primary.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/celery/apps/beat.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/celery/apps/docfetching.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/celery/tasks/periodic/tasks.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/celery/tasks/evals/tasks.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/celery/tasks/docfetching/tasks.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/celery/tasks/docfetching/task_creation_utils.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/celery/tasks/vespa/tasks.py",
          "line": 253,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous sql_injection sink",
          "description": "LLM output from 'rds.generate_tasks' is used in 'UPDATE' on line 253 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application security.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/celery/tasks/vespa/tasks.py",
          "line": 329,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous sql_injection sink",
          "description": "LLM output from 'rug.generate_tasks' is used in 'UPDATE' on line 329 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application security.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/celery/tasks/vespa/tasks.py",
          "line": 287,
          "category": "LLM04: Model Denial of Service",
          "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
          "description": "Function 'try_generate_user_group_sync_tasks' on line 287 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections ",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/celery/tasks/vespa/document_sync.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/celery/tasks/user_file_processing/tasks.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/celery/tasks/shared/tasks.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/celery/tasks/connector_deletion/tasks.py",
          "line": 327,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous sql_injection sink",
          "description": "LLM output from 'redis_connector.delete.generate_tasks' is used in 'UPDATE' on line 327 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise appli",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/celery/tasks/kg_processing/tasks.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/celery/tasks/kg_processing/utils.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/celery/tasks/docprocessing/tasks.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/celery/tasks/docprocessing/utils.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/celery/tasks/docprocessing/heartbeat.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/celery/tasks/monitoring/tasks.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/external_permissions/post_query_censoring.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/access/access.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/auth/users.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/server/seeding.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/server/tenant_usage_limits.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/utils/encryption.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/utils/posthog_client.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/utils/license.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/configs/app_configs.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/db/user_group.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/db/usage_export.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/db/connector_credential_pair.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/db/token_limit.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/db/query_history.py",
          "line": 113,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/db/query_history.py",
          "line": 161,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/db/query_history.py",
          "line": 49,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/db/saml.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/db/standard_answer.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/db/external_perm.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/db/document_set.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/db/document.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/db/license.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/db/analytics.py",
          "line": 112,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous sql_injection sink",
          "description": "LLM output from 'ChatMessage.chat_session_id.label' is used in 'WHERE' on line 112 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise applicatio",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/db/analytics.py",
          "line": 29,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/db/analytics.py",
          "line": 64,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/db/analytics.py",
          "line": 111,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/db/analytics.py",
          "line": 131,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/db/analytics.py",
          "line": 140,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/db/analytics.py",
          "line": 193,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/db/analytics.py",
          "line": 222,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/db/analytics.py",
          "line": 253,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/db/analytics.py",
          "line": 284,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/db/analytics.py",
          "line": 316,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/db/analytics.py",
          "line": 132,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/db/analytics.py",
          "line": 112,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/onyxbot/slack/handlers/handle_standard_answers.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/server/middleware/tenant_tracking.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/server/reporting/usage_export_api.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/server/reporting/usage_export_generation.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/server/enterprise_settings/store.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/server/enterprise_settings/models.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/server/enterprise_settings/api.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/server/user_group/api.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/server/oauth/confluence_cloud.py",
          "line": 252,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/server/oauth/api.py",
          "line": 24,
          "category": "LLM04: Model Denial of Service",
          "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
          "description": "Function 'prepare_authorization_request' on line 24 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/server/oauth/api.py",
          "line": 49,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/server/oauth/api.py",
          "line": 58,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/server/oauth/api.py",
          "line": 66,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/server/oauth/google_drive.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/server/oauth/slack.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/server/license/models.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/server/license/api.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/server/tenants/team_membership_api.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/server/tenants/user_invitations_api.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/server/tenants/tenant_management_api.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/server/tenants/billing.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/server/tenants/anonymous_users_api.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/server/tenants/access.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/server/tenants/admin_api.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/server/tenants/user_mapping.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/server/tenants/schema_management.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/server/tenants/provisioning.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/server/tenants/billing_api.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/server/tenants/product_gating.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/server/manage/standard_answer.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/server/documents/cc_pair.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/server/query_history/models.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/server/query_history/api.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/server/token_rate_limits/api.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/server/analytics/api.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/external_permissions/jira/group_sync.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/external_permissions/jira/page_access.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/external_permissions/github/group_sync.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/external_permissions/github/doc_sync.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/external_permissions/github/utils.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/external_permissions/confluence/group_sync.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/external_permissions/confluence/doc_sync.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/external_permissions/confluence/page_access.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/external_permissions/confluence/space_access.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/external_permissions/sharepoint/group_sync.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/external_permissions/sharepoint/permission_utils.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/external_permissions/salesforce/utils.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/external_permissions/salesforce/postprocessing.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/external_permissions/slack/channel_access.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/external_permissions/slack/group_sync.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/external_permissions/slack/doc_sync.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/external_permissions/gmail/doc_sync.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/external_permissions/google_drive/permission_retrieval.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/external_permissions/google_drive/models.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/external_permissions/google_drive/group_sync.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/external_permissions/google_drive/doc_sync.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/background/celery/tasks/beat_schedule.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/background/celery/tasks/external_group_syncing/tasks.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/background/celery/tasks/vespa/tasks.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/background/celery/tasks/doc_permission_syncing/tasks.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/background/celery/tasks/cleanup/tasks.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/background/celery/tasks/cloud/tasks.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/background/celery/tasks/ttl_management/tasks.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/background/celery/tasks/query_history/tasks.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/tools/ods/hatch_build.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/tools/ods/internal/openapi/openapi_schema.py",
          "line": 139,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'subprocess.run' is used in 'run(' on line 139 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/tools/ods/internal/openapi/openapi_schema.py",
          "line": 100,
          "category": "LLM08: Excessive Agency",
          "title": "Direct execution of LLM-generated code in 'generate_client'",
          "description": "Function 'generate_client' on line 100 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM ou",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/tools/ods/internal/openapi/openapi_schema.py",
          "line": 100,
          "category": "LLM09: Overreliance",
          "title": "Direct execution of LLM output in 'generate_client'",
          "description": "Function 'generate_client' on line 100 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/mcp_server_main.py",
          "line": 12,
          "category": "LLM08: Excessive Agency",
          "title": "High-risk execute operation without confirmation in 'main'",
          "description": "Function 'main' on line 12 performs high-risk execute operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destruc",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/onyx_openapi_schema.py",
          "line": 19,
          "category": "LLM10: Model Theft",
          "title": "Model API without rate limiting in 'go'",
          "description": "API endpoint 'go' on line 19 provides model access without rate limiting. This allows attackers to make unlimited queries to extract model knowledge, potentially stealing intellectual property or sens",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/orphan_doc_cleanup_script.py",
          "line": 113,
          "category": "SQL Injection",
          "title": "SQL injection: f-string formatting",
          "description": "SQL query on line 113 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
          "is_semantic_taint": false
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/reset_postgres.py",
          "line": 53,
          "category": "SQL Injection",
          "title": "SQL injection: f-string formatting",
          "description": "SQL query on line 53 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
          "is_semantic_taint": false
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/chat_loadtest.py",
          "line": 188,
          "category": "LLM08: Excessive Agency",
          "title": "High-risk execute/admin operation without confirmation in 'main'",
          "description": "Function 'main' on line 188 performs high-risk execute/admin operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially ",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/get_wikidocs.py",
          "line": 80,
          "category": "LLM05: Supply Chain Vulnerabilities",
          "title": "Unsafe model loading pattern",
          "description": "trust_remote_code=True enables arbitrary code execution on line 80.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/env.py",
          "line": 223,
          "category": "LLM08: Excessive Agency",
          "title": "High-risk write/execute operation without confirmation in 'do_run_migrations'",
          "description": "Function 'do_run_migrations' on line 223 performs high-risk write/execute operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/env.py",
          "line": 369,
          "category": "LLM08: Excessive Agency",
          "title": "High-risk write/execute/network operation without confirmation in 'run_migrations_offline'",
          "description": "Function 'run_migrations_offline' on line 369 performs high-risk write/execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonom",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic_tenants/env.py",
          "line": 60,
          "category": "LLM08: Excessive Agency",
          "title": "High-risk execute/network operation without confirmation in 'run_migrations_offline'",
          "description": "Function 'run_migrations_offline' on line 60 performs high-risk execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously e",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic_tenants/env.py",
          "line": 84,
          "category": "LLM08: Excessive Agency",
          "title": "High-risk execute operation without confirmation in 'do_run_migrations'",
          "description": "Function 'do_run_migrations' on line 84 performs high-risk execute operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potent",
          "is_semantic_taint": true
        }
      ]
    },
    {
      "app": "gpt-pilot",
      "description": "AI developer",
      "repo": "https://github.com/Pythagora-io/gpt-pilot",
      "python_files": 151,
      "files_scanned": 0,
      "total_findings": 98,
      "critical": 68,
      "high": 12,
      "potential_vulns": [
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/main.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/proc/process_manager.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/ui/ipc_client.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/ui/console.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/ui/api_server.py",
          "line": 345,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/ui/api_server.py",
          "line": 320,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/ui/api_server.py",
          "line": 341,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/ui/base.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/llm/convo.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/llm/azure_client.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/llm/parser.py",
          "line": 156,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** Function with user ",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/llm/groq_client.py",
          "line": 46,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/llm/openai_client.py",
          "line": 51,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/llm/anthropic_client.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/llm/relace_client.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/config/user_settings.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/config/version.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/config/actions.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/config/__init__.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/config/magic_words.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/config/env_importer.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/agents/mixins.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/agents/importer.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/agents/tech_lead.py",
          "line": 415,
          "category": "LLM06: Sensitive Information Disclosure",
          "title": "Hardcoded secret detected: Secret Keyword",
          "description": "detect-secrets found a potential Secret Keyword on line 415. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/agents/tech_lead.py",
          "line": 425,
          "category": "LLM06: Sensitive Information Disclosure",
          "title": "Hardcoded secret detected: Secret Keyword",
          "description": "detect-secrets found a potential Secret Keyword on line 425. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/agents/tech_lead.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/agents/problem_solver.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/agents/response.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/agents/tech_writer.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/agents/human_input.py",
          "line": 26,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/agents/wizard.py",
          "line": 65,
          "category": "LLM06: Sensitive Information Disclosure",
          "title": "Hardcoded secret detected: Secret Keyword",
          "description": "detect-secrets found a potential Secret Keyword on line 65. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/agents/wizard.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/agents/external_docs.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/agents/error_handler.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/agents/troubleshooter.py",
          "line": 107,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/agents/troubleshooter.py",
          "line": 71,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/agents/base.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/agents/code_monkey.py",
          "line": 223,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/agents/spec_writer.py",
          "line": 320,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/utils/text.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/cli/helpers.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/state/state_manager.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/db/fix_migrations.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/db/setup.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/db/v0importer.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/templates/example_project.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/templates/render.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/templates/vite_react.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/templates/javascript_react.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/templates/node_express_mongoose.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/templates/vite_react_swagger.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/templates/base.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/templates/react_express.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/log/__init__.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/disk/vfs.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/telemetry/__init__.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/templates/tree/add_raw_tags.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/db/migrations/env.py",
          "line": 30,
          "category": "LLM09: Overreliance",
          "title": "Critical decision without oversight in 'run_migrations_offline'",
          "description": "Function 'run_migrations_offline' on line 30 makes critical financial decisions based on LLM output without human oversight or verification. Action edges detected (HTTP/file/DB/subprocess) - risk of a",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/db/migrations/env.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/db/models/user_input.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/db/models/project_state.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/db/models/branch.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/db/models/chat_convo.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/db/models/file.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/db/models/file_content.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/db/models/specification.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/db/models/project.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/db/migrations/versions/69e50fdaf067_move_knowledge_base_to_separate_table.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/db/migrations/env.py",
          "line": 52,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous xss sink",
          "description": "LLM output from 'context.run_migrations' is used in 'render' on line 52 without sanitization. This creates a xss vulnerability where malicious LLM output can compromise application security.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/db/migrations/env.py",
          "line": 77,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous xss sink",
          "description": "LLM output from 'context.run_migrations' is used in 'render' on line 77 without sanitization. This creates a xss vulnerability where malicious LLM output can compromise application security.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/db/migrations/env.py",
          "line": 30,
          "category": "LLM08: Excessive Agency",
          "title": "High-risk execute/network operation without confirmation in 'run_migrations_offline'",
          "description": "Function 'run_migrations_offline' on line 30 performs high-risk execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously e",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/db/migrations/env.py",
          "line": 55,
          "category": "LLM08: Excessive Agency",
          "title": "High-risk write/execute operation without confirmation in 'run_migrations_online'",
          "description": "Function 'run_migrations_online' on line 55 performs high-risk write/execute operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously exec",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/db/migrations/versions/0173e14719aa_vacuum_database.py",
          "line": 15,
          "category": "LLM06: Sensitive Information Disclosure",
          "title": "Hardcoded secret detected: Hex High Entropy String",
          "description": "detect-secrets found a potential Hex High Entropy String on line 15. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/db/migrations/versions/0a1bb637fa26_initial.py",
          "line": 15,
          "category": "LLM06: Sensitive Information Disclosure",
          "title": "Hardcoded secret detected: Hex High Entropy String",
          "description": "detect-secrets found a potential Hex High Entropy String on line 15. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/db/migrations/versions/08d71952ec2f_refactor_specification_template_to_.py",
          "line": 15,
          "category": "LLM06: Sensitive Information Disclosure",
          "title": "Hardcoded secret detected: Hex High Entropy String",
          "description": "detect-secrets found a potential Hex High Entropy String on line 15. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/db/migrations/versions/69e50fdaf067_move_knowledge_base_to_separate_table.py",
          "line": 18,
          "category": "LLM06: Sensitive Information Disclosure",
          "title": "Hardcoded secret detected: Hex High Entropy String",
          "description": "detect-secrets found a potential Hex High Entropy String on line 18. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/db/migrations/versions/b760f66138c0_add_docs_column_to_project_states.py",
          "line": 15,
          "category": "LLM06: Sensitive Information Disclosure",
          "title": "Hardcoded secret detected: Hex High Entropy String",
          "description": "detect-secrets found a potential Hex High Entropy String on line 15. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/db/migrations/versions/f352dbe45751_make_relevant_files_nullable.py",
          "line": 15,
          "category": "LLM06: Sensitive Information Disclosure",
          "title": "Hardcoded secret detected: Hex High Entropy String",
          "description": "detect-secrets found a potential Hex High Entropy String on line 15. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
          "is_semantic_taint": true
        }
      ]
    },
    {
      "app": "chatgpt-retrieval-plugin",
      "description": "OpenAI plugin",
      "repo": "https://github.com/openai/chatgpt-retrieval-plugin",
      "python_files": 51,
      "files_scanned": 0,
      "total_findings": 48,
      "critical": 32,
      "high": 12,
      "potential_vulns": [
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpgwxahnts/chatgpt-retrieval-plugin/server/main.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpgwxahnts/chatgpt-retrieval-plugin/local_server/main.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpgwxahnts/chatgpt-retrieval-plugin/datastore/datastore.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpgwxahnts/chatgpt-retrieval-plugin/services/extract_metadata.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpgwxahnts/chatgpt-retrieval-plugin/services/file.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpgwxahnts/chatgpt-retrieval-plugin/services/openai.py",
          "line": 66,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpgwxahnts/chatgpt-retrieval-plugin/services/openai.py",
          "line": 71,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpgwxahnts/chatgpt-retrieval-plugin/services/openai.py",
          "line": 44,
          "category": "LLM04: Model Denial of Service",
          "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
          "description": "Function 'get_chat_completion' on line 44 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpgwxahnts/chatgpt-retrieval-plugin/services/openai.py",
          "line": 32,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpgwxahnts/chatgpt-retrieval-plugin/services/openai.py",
          "line": 34,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpgwxahnts/chatgpt-retrieval-plugin/services/chunks.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpgwxahnts/chatgpt-retrieval-plugin/scripts/process_jsonl/process_jsonl.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpgwxahnts/chatgpt-retrieval-plugin/scripts/process_json/process_json.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpgwxahnts/chatgpt-retrieval-plugin/scripts/process_zip/process_zip.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpgwxahnts/chatgpt-retrieval-plugin/examples/memory/main.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpgwxahnts/chatgpt-retrieval-plugin/examples/authentication-methods/no-auth/main.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpgwxahnts/chatgpt-retrieval-plugin/datastore/providers/zilliz_datastore.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpgwxahnts/chatgpt-retrieval-plugin/datastore/providers/pgvector_datastore.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpgwxahnts/chatgpt-retrieval-plugin/datastore/providers/chroma_datastore.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpgwxahnts/chatgpt-retrieval-plugin/datastore/providers/weaviate_datastore.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpgwxahnts/chatgpt-retrieval-plugin/datastore/providers/llama_datastore.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpgwxahnts/chatgpt-retrieval-plugin/datastore/providers/redis_datastore.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpgwxahnts/chatgpt-retrieval-plugin/datastore/providers/analyticdb_datastore.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpgwxahnts/chatgpt-retrieval-plugin/datastore/providers/mongodb_atlas_datastore.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpgwxahnts/chatgpt-retrieval-plugin/datastore/providers/qdrant_datastore.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpgwxahnts/chatgpt-retrieval-plugin/datastore/providers/supabase_datastore.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpgwxahnts/chatgpt-retrieval-plugin/datastore/providers/azuresearch_datastore.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpgwxahnts/chatgpt-retrieval-plugin/datastore/providers/azurecosmosdb_datastore.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpgwxahnts/chatgpt-retrieval-plugin/datastore/providers/elasticsearch_datastore.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpgwxahnts/chatgpt-retrieval-plugin/datastore/providers/pinecone_datastore.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpgwxahnts/chatgpt-retrieval-plugin/datastore/providers/postgres_datastore.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpgwxahnts/chatgpt-retrieval-plugin/datastore/providers/milvus_datastore.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpgwxahnts/chatgpt-retrieval-plugin/datastore/providers/analyticdb_datastore.py",
          "line": 68,
          "category": "SQL Injection",
          "title": "SQL injection: f-string formatting",
          "description": "SQL query on line 68 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
          "is_semantic_taint": false
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpgwxahnts/chatgpt-retrieval-plugin/datastore/providers/analyticdb_datastore.py",
          "line": 85,
          "category": "SQL Injection",
          "title": "SQL injection: f-string formatting",
          "description": "SQL query on line 85 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
          "is_semantic_taint": false
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpgwxahnts/chatgpt-retrieval-plugin/datastore/providers/analyticdb_datastore.py",
          "line": 95,
          "category": "SQL Injection",
          "title": "SQL injection: f-string formatting",
          "description": "SQL query on line 95 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
          "is_semantic_taint": false
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpgwxahnts/chatgpt-retrieval-plugin/datastore/providers/analyticdb_datastore.py",
          "line": 147,
          "category": "SQL Injection",
          "title": "SQL injection: f-string formatting",
          "description": "SQL query on line 147 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
          "is_semantic_taint": false
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpgwxahnts/chatgpt-retrieval-plugin/datastore/providers/analyticdb_datastore.py",
          "line": 177,
          "category": "SQL Injection",
          "title": "SQL injection: f-string formatting",
          "description": "SQL query on line 177 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
          "is_semantic_taint": false
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpgwxahnts/chatgpt-retrieval-plugin/datastore/providers/analyticdb_datastore.py",
          "line": 285,
          "category": "SQL Injection",
          "title": "SQL injection: f-string formatting",
          "description": "SQL query on line 285 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
          "is_semantic_taint": false
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpgwxahnts/chatgpt-retrieval-plugin/datastore/providers/analyticdb_datastore.py",
          "line": 288,
          "category": "SQL Injection",
          "title": "SQL injection: f-string formatting",
          "description": "SQL query on line 288 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
          "is_semantic_taint": false
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpgwxahnts/chatgpt-retrieval-plugin/datastore/providers/analyticdb_datastore.py",
          "line": 311,
          "category": "SQL Injection",
          "title": "SQL injection: f-string formatting",
          "description": "SQL query on line 311 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
          "is_semantic_taint": false
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpgwxahnts/chatgpt-retrieval-plugin/datastore/providers/postgres_datastore.py",
          "line": 131,
          "category": "SQL Injection",
          "title": "SQL injection: f-string formatting",
          "description": "SQL query on line 131 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
          "is_semantic_taint": false
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpgwxahnts/chatgpt-retrieval-plugin/datastore/providers/postgres_datastore.py",
          "line": 50,
          "category": "SQL Injection",
          "title": "SQL injection: f-string formatting",
          "description": "SQL query on line 50 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
          "is_semantic_taint": false
        }
      ]
    },
    {
      "app": "embedchain",
      "description": "RAG framework",
      "repo": "https://github.com/embedchain/embedchain",
      "python_files": 542,
      "files_scanned": 0,
      "total_findings": 702,
      "critical": 406,
      "high": 96,
      "potential_vulns": [
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/server/main.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/evaluation/generate_scores.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/evaluation/run_experiments.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/exceptions.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/cookbooks/helper/mem0_teachability.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/openmemory/api/main.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/openmemory/api/app/models.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/openmemory/api/app/database.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/openmemory/api/app/mcp_server.py",
          "line": 172,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/openmemory/api/app/mcp_server.py",
          "line": 446,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/openmemory/api/alembic/env.py",
          "line": 37,
          "category": "LLM09: Overreliance",
          "title": "Critical decision without oversight in 'run_migrations_offline'",
          "description": "Function 'run_migrations_offline' on line 37 makes critical financial decisions based on LLM output without human oversight or verification. Action edges detected (HTTP/file/DB/subprocess) - risk of a",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/openmemory/api/alembic/env.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/openmemory/api/alembic/versions/add_config_table.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/openmemory/api/app/routers/config.py",
          "line": 62,
          "category": "LLM06: Sensitive Information Disclosure",
          "title": "Hardcoded secret detected: Secret Keyword",
          "description": "detect-secrets found a potential Secret Keyword on line 62. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/openmemory/api/app/routers/config.py",
          "line": 69,
          "category": "LLM06: Sensitive Information Disclosure",
          "title": "Hardcoded secret detected: Secret Keyword",
          "description": "detect-secrets found a potential Secret Keyword on line 69. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/openmemory/api/app/routers/config.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/openmemory/api/app/routers/apps.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/openmemory/api/app/routers/stats.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/openmemory/api/app/routers/memories.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/openmemory/api/app/utils/db.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/openmemory/api/app/utils/memory.py",
          "line": 250,
          "category": "LLM06: Sensitive Information Disclosure",
          "title": "Hardcoded secret detected: Secret Keyword",
          "description": "detect-secrets found a potential Secret Keyword on line 250. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/openmemory/api/app/utils/memory.py",
          "line": 257,
          "category": "LLM06: Sensitive Information Disclosure",
          "title": "Hardcoded secret detected: Secret Keyword",
          "description": "detect-secrets found a potential Secret Keyword on line 257. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/openmemory/api/app/utils/memory.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/reranker/cohere_reranker.py",
          "line": 32,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/reranker/huggingface_reranker.py",
          "line": 19,
          "category": "LLM08: Excessive Agency",
          "title": "Direct execution of LLM-generated code in '__init__'",
          "description": "Function '__init__' on line 19 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM outputs ca",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/reranker/huggingface_reranker.py",
          "line": 19,
          "category": "LLM09: Overreliance",
          "title": "Direct execution of LLM output in '__init__'",
          "description": "Function '__init__' on line 19 directly executes LLM-generated code using eval(. This is extremely dangerous and allows arbitrary code execution.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/reranker/llm_reranker.py",
          "line": 120,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output flows to sql_injection sink",
          "description": "LLM output variable 'response' flows to 'self._extract_score' on line 120 via direct flow. This creates a sql_injection vulnerability.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/reranker/zero_entropy_reranker.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/reranker/sentence_transformer_reranker.py",
          "line": 78,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/reranker/sentence_transformer_reranker.py",
          "line": 46,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** Function with user ",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/cassandra.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/valkey.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/s3_vectors.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/milvus.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/azure_mysql.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/configs.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/pinecone.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/qdrant.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/upstash_vector.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/mongodb.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/neptune_analytics.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/faiss.py",
          "line": 94,
          "category": "LLM03: Training Data Poisoning",
          "title": "Unsafe data loading with pickle.load in training context",
          "description": "Function '_load' uses pickle.load on line 94. Pickle-based deserialization can execute arbitrary code, allowing attackers to inject malicious code through poisoned training data or models.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/faiss.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/pgvector.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/redis.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/weaviate.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/azure_ai_search.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/databricks.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/baidu.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/vertex_ai_vector_search.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/embeddings/gemini.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/embeddings/aws_bedrock.py",
          "line": 59,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/embeddings/aws_bedrock.py",
          "line": 72,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/embeddings/aws_bedrock.py",
          "line": 55,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** Function with user ",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/embeddings/azure_openai.py",
          "line": 34,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous sql_injection sink",
          "description": "LLM output from 'AzureOpenAI' is used in 'UPDATE' on line 34 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application security.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/embeddings/azure_openai.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/embeddings/huggingface.py",
          "line": 27,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous sql_injection sink",
          "description": "LLM output from 'self.model.get_sentence_embedding_dimension' is used in 'UPDATE' on line 27 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise ",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/embeddings/huggingface.py",
          "line": 44,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/embeddings/huggingface.py",
          "line": 29,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** Function with user ",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/embeddings/ollama.py",
          "line": 39,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous sql_injection sink",
          "description": "LLM output from 'model.get' is used in 'UPDATE' on line 39 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application security.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/memgraph_memory.py",
          "line": 204,
          "category": "LLM01: Prompt Injection",
          "title": "User input 'data' embedded in LLM prompt",
          "description": "User input parameter 'data' is directly passed to LLM API call 'self.llm.generate_response'. This is a high-confidence prompt injection vector.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/memgraph_memory.py",
          "line": 347,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous sql_injection sink",
          "description": "LLM output from 'self.llm.generate_response' is used in 'DELETE' on line 347 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application secu",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/memgraph_memory.py",
          "line": 278,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous sql_injection sink",
          "description": "LLM output from 'self.embedding_model.embed' is used in 'WHERE' on line 278 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application secur",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/memgraph_memory.py",
          "line": 273,
          "category": "LLM04: Model Denial of Service",
          "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits",
          "description": "Function '_search_graph_db' on line 273 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaus",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/kuzu_memory.py",
          "line": 227,
          "category": "LLM01: Prompt Injection",
          "title": "User input 'data' embedded in LLM prompt",
          "description": "User input parameter 'data' is directly passed to LLM API call 'self.llm.generate_response'. This is a high-confidence prompt injection vector.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/kuzu_memory.py",
          "line": 368,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous sql_injection sink",
          "description": "LLM output from 'self.llm.generate_response' is used in 'DELETE' on line 368 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application secu",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/kuzu_memory.py",
          "line": 317,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous sql_injection sink",
          "description": "LLM output from 'self.embedding_model.embed' is used in 'execute(' on line 317 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application se",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/kuzu_memory.py",
          "line": 629,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous sql_injection sink",
          "description": "LLM output from 'self.embedding_model.embed' is used in 'execute(' on line 629 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application se",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/kuzu_memory.py",
          "line": 297,
          "category": "LLM04: Model Denial of Service",
          "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits",
          "description": "Function '_search_graph_db' on line 297 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaus",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/telemetry.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/setup.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/utils.py",
          "line": 107,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/storage.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/main.py",
          "line": 1114,
          "category": "LLM01: Prompt Injection",
          "title": "User input 'messages' embedded in LLM prompt",
          "description": "User input 'messages' flows to LLM call via assignment in variable 'parsed_messages'. Function '_create_procedural_memory' may be vulnerable to prompt injection attacks.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/main.py",
          "line": 452,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output flows to sql_injection sink",
          "description": "LLM output variable 'response' flows to 'extract_json' on line 452 via direct flow. This creates a sql_injection vulnerability.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/main.py",
          "line": 1087,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output flows to sql_injection sink",
          "description": "LLM output variable 'embeddings' flows to 'self.vector_store.insert' on line 1087 via direct flow. This creates a sql_injection vulnerability.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/main.py",
          "line": 1177,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output flows to sql_injection sink",
          "description": "LLM output variable 'embeddings' flows to 'self.vector_store.update' on line 1177 via direct flow. This creates a sql_injection vulnerability.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/main.py",
          "line": 281,
          "category": "LLM04: Model Denial of Service",
          "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
          "description": "Function 'add' on line 281 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust mod",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/main.py",
          "line": 1103,
          "category": "LLM04: Model Denial of Service",
          "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
          "description": "Function '_create_procedural_memory' on line 1103 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable a",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/graph_memory.py",
          "line": 201,
          "category": "LLM01: Prompt Injection",
          "title": "User input 'data' embedded in LLM prompt",
          "description": "User input parameter 'data' is directly passed to LLM API call 'self.llm.generate_response'. This is a high-confidence prompt injection vector.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/graph_memory.py",
          "line": 341,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous sql_injection sink",
          "description": "LLM output from 'self.llm.generate_response' is used in 'DELETE' on line 341 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application secu",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/graph_memory.py",
          "line": 284,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous sql_injection sink",
          "description": "LLM output from 'self.embedding_model.embed' is used in 'WHERE' on line 284 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application secur",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/graph_memory.py",
          "line": 271,
          "category": "LLM04: Model Denial of Service",
          "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits",
          "description": "Function '_search_graph_db' on line 271 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaus",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/utils/factory.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/configs/prompts.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/graphs/tools.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/llms/vllm.py",
          "line": 73,
          "category": "LLM04: Model Denial of Service",
          "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
          "description": "Function 'generate_response' on line 73 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers t",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/llms/vllm.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/llms/lmstudio.py",
          "line": 73,
          "category": "LLM04: Model Denial of Service",
          "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
          "description": "Function 'generate_response' on line 73 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers t",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/llms/lmstudio.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/llms/azure_openai_structured.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/llms/aws_bedrock.py",
          "line": 600,
          "category": "LLM04: Model Denial of Service",
          "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits",
          "description": "Function 'list_available_models' on line 600 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to e",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/llms/openai.py",
          "line": 142,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output flows to command_injection sink",
          "description": "LLM output variable 'response' flows to 'self.config.response_callback' on line 142 via direct flow. This creates a command_injection vulnerability.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/llms/openai.py",
          "line": 83,
          "category": "LLM04: Model Denial of Service",
          "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting, No input length validation, No timeout configuration, No token/context limits",
          "description": "Function 'generate_response' on line 83 has 5 DoS risk(s): LLM calls in loops, No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protection",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/llms/openai.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/llms/azure_openai.py",
          "line": 100,
          "category": "LLM04: Model Denial of Service",
          "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
          "description": "Function 'generate_response' on line 100 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers ",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/llms/azure_openai.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/llms/deepseek.py",
          "line": 73,
          "category": "LLM04: Model Denial of Service",
          "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
          "description": "Function 'generate_response' on line 73 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers t",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/llms/deepseek.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/llms/sarvam.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/llms/anthropic.py",
          "line": 43,
          "category": "LLM04: Model Denial of Service",
          "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
          "description": "Function 'generate_response' on line 43 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers t",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/llms/anthropic.py",
          "line": 86,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/llms/xai.py",
          "line": 51,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/llms/langchain.py",
          "line": 54,
          "category": "LLM04: Model Denial of Service",
          "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting, No input length validation, No timeout configuration, No token/context limits",
          "description": "Function 'generate_response' on line 54 has 5 DoS risk(s): LLM calls in loops, No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protection",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/llms/langchain.py",
          "line": 80,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/llms/langchain.py",
          "line": 82,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/llms/langchain.py",
          "line": 84,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/llms/openai_structured.py",
          "line": 21,
          "category": "LLM04: Model Denial of Service",
          "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
          "description": "Function 'generate_response' on line 21 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers t",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/llms/openai_structured.py",
          "line": 51,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/llms/base.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/client/utils.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/client/main.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/client/project.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/graphs/neptune/neptunedb.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/graphs/neptune/neptunegraph.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/graphs/neptune/base.py",
          "line": 83,
          "category": "LLM01: Prompt Injection",
          "title": "User input 'data' embedded in LLM prompt",
          "description": "User input parameter 'data' is directly passed to LLM API call 'self.llm.generate_response'. This is a high-confidence prompt injection vector.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/graphs/neptune/base.py",
          "line": 174,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous sql_injection sink",
          "description": "LLM output from 'self.llm.generate_response' is used in 'DELETE' on line 174 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application secu",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/graphs/neptune/base.py",
          "line": 216,
          "category": "LLM04: Model Denial of Service",
          "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
          "description": "Function '_add_entities' on line 216 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to e",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/graphs/neptune/base.py",
          "line": 461,
          "category": "LLM04: Model Denial of Service",
          "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits",
          "description": "Function '_search_graph_db' on line 461 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaus",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/configs/vector_stores/cassandra.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/configs/vector_stores/elasticsearch.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/configs/vector_stores/s3_vectors.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/configs/vector_stores/milvus.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/configs/vector_stores/azure_mysql.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/configs/vector_stores/pinecone.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/configs/vector_stores/supabase.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/configs/vector_stores/qdrant.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/configs/vector_stores/faiss.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/configs/vector_stores/redis.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/configs/vector_stores/weaviate.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/configs/vector_stores/azure_ai_search.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/configs/vector_stores/databricks.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/configs/vector_stores/langchain.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/configs/vector_stores/baidu.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/configs/llms/aws_bedrock.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/evaluation/metrics/utils.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/evaluation/metrics/llm_judge.py",
          "line": 41,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/evaluation/metrics/llm_judge.py",
          "line": 54,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output flows to sql_injection sink",
          "description": "LLM output variable 'response' flows to 'extract_json' on line 54 via direct flow. This creates a sql_injection vulnerability.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/evaluation/src/langmem.py",
          "line": 26,
          "category": "LLM01: Prompt Injection",
          "title": "User input 'question' embedded in LLM prompt",
          "description": "User input 'question' flows to LLM call via call in variable 'prompt'. Function 'get_answer' may be vulnerable to prompt injection attacks.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/evaluation/src/langmem.py",
          "line": 83,
          "category": "LLM01: Prompt Injection",
          "title": "User input 'message' embedded in LLM prompt",
          "description": "User input parameter 'message' is directly passed to LLM API call 'self.agent.invoke'. This is a high-confidence prompt injection vector.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/evaluation/src/langmem.py",
          "line": 88,
          "category": "LLM01: Prompt Injection",
          "title": "User input 'query' embedded in LLM prompt",
          "description": "User input parameter 'query' is directly passed to LLM API call 'self.agent.invoke'. This is a high-confidence prompt injection vector.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/evaluation/src/langmem.py",
          "line": 25,
          "category": "LLM04: Model Denial of Service",
          "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
          "description": "Function 'get_answer' on line 25 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhau",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/evaluation/src/langmem.py",
          "line": 82,
          "category": "LLM04: Model Denial of Service",
          "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
          "description": "Function 'add_memory' on line 82 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhau",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/evaluation/src/langmem.py",
          "line": 85,
          "category": "LLM04: Model Denial of Service",
          "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
          "description": "Function 'search_memory' on line 85 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to ex",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/evaluation/src/rag.py",
          "line": 34,
          "category": "LLM04: Model Denial of Service",
          "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits",
          "description": "Function 'generate_response' on line 34 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaus",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/evaluation/src/rag.py",
          "line": 144,
          "category": "LLM04: Model Denial of Service",
          "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits",
          "description": "Function 'process_all_conversations' on line 144 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers ",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/evaluation/src/rag.py",
          "line": 44,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/evaluation/src/zep/add.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/evaluation/src/zep/search.py",
          "line": 103,
          "category": "LLM01: Prompt Injection",
          "title": "User input 'question' embedded in LLM prompt",
          "description": "User input 'question' flows to LLM call via call in variable 'answer_prompt'. Function 'answer_question' may be vulnerable to prompt injection attacks.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/evaluation/src/zep/search.py",
          "line": 106,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/evaluation/src/memzero/search.py",
          "line": 102,
          "category": "LLM01: Prompt Injection",
          "title": "User input 'question' embedded in LLM prompt",
          "description": "User input 'question' flows to LLM call via call in variable 'answer_prompt'. Function 'answer_question' may be vulnerable to prompt injection attacks.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/evaluation/src/memzero/search.py",
          "line": 90,
          "category": "LLM04: Model Denial of Service",
          "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
          "description": "Function 'answer_question' on line 90 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to ",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/evaluation/src/memzero/search.py",
          "line": 113,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/evaluation/src/openai/predict.py",
          "line": 94,
          "category": "LLM01: Prompt Injection",
          "title": "User input 'question' embedded in LLM prompt",
          "description": "User input 'question' flows to LLM call via call in variable 'answer_prompt'. Function 'answer_question' may be vulnerable to prompt injection attacks.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/evaluation/src/openai/predict.py",
          "line": 97,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/examples/misc/movie_recommendation_grok3.py",
          "line": 50,
          "category": "LLM01: Prompt Injection",
          "title": "User input 'user_query' embedded in LLM prompt",
          "description": "User input 'user_query' flows to LLM call via assignment in variable 'prompt'. Function 'recommend_movie_with_memory' may be vulnerable to prompt injection attacks.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/examples/misc/movie_recommendation_grok3.py",
          "line": 7,
          "category": "LLM06: Sensitive Information Disclosure",
          "title": "Hardcoded secret detected: Secret Keyword",
          "description": "detect-secrets found a potential Secret Keyword on line 7. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/examples/misc/movie_recommendation_grok3.py",
          "line": 8,
          "category": "LLM06: Sensitive Information Disclosure",
          "title": "Hardcoded secret detected: Secret Keyword",
          "description": "detect-secrets found a potential Secret Keyword on line 8. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/examples/misc/personal_assistant_agno.py",
          "line": 69,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'agent.run' is used in 'run(' on line 69 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/examples/misc/personal_assistant_agno.py",
          "line": 71,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'agent.run' is used in 'run(' on line 71 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/examples/misc/personal_assistant_agno.py",
          "line": 5,
          "category": "LLM06: Sensitive Information Disclosure",
          "title": "Hardcoded secret detected: Secret Keyword",
          "description": "detect-secrets found a potential Secret Keyword on line 5. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/examples/misc/personal_assistant_agno.py",
          "line": 6,
          "category": "LLM06: Sensitive Information Disclosure",
          "title": "Hardcoded secret detected: Secret Keyword",
          "description": "detect-secrets found a potential Secret Keyword on line 6. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/examples/misc/diet_assistant_voice_cartesia.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/examples/misc/voice_assistant_elevenlabs.py",
          "line": 12,
          "category": "LLM06: Sensitive Information Disclosure",
          "title": "Hardcoded secret detected: Secret Keyword",
          "description": "detect-secrets found a potential Secret Keyword on line 12. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/examples/misc/voice_assistant_elevenlabs.py",
          "line": 13,
          "category": "LLM06: Sensitive Information Disclosure",
          "title": "Hardcoded secret detected: Secret Keyword",
          "description": "detect-secrets found a potential Secret Keyword on line 13. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/examples/misc/voice_assistant_elevenlabs.py",
          "line": 14,
          "category": "LLM06: Sensitive Information Disclosure",
          "title": "Hardcoded secret detected: Secret Keyword",
          "description": "detect-secrets found a potential Secret Keyword on line 14. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/examples/misc/voice_assistant_elevenlabs.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/examples/misc/test.py",
          "line": 74,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/examples/misc/test.py",
          "line": 62,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** Function with user ",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/examples/misc/personalized_search.py",
          "line": 172,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/examples/misc/personalized_search.py",
          "line": 38,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/examples/misc/personalized_search.py",
          "line": 155,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** Function with user ",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/examples/misc/vllm_example.py",
          "line": 22,
          "category": "LLM06: Sensitive Information Disclosure",
          "title": "Hardcoded secret detected: Secret Keyword",
          "description": "detect-secrets found a potential Secret Keyword on line 22. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/examples/misc/vllm_example.py",
          "line": 34,
          "category": "LLM06: Sensitive Information Disclosure",
          "title": "Hardcoded secret detected: Secret Keyword",
          "description": "detect-secrets found a potential Secret Keyword on line 34. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/examples/misc/study_buddy.py",
          "line": 7,
          "category": "LLM06: Sensitive Information Disclosure",
          "title": "Hardcoded secret detected: Secret Keyword",
          "description": "detect-secrets found a potential Secret Keyword on line 7. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/examples/misc/study_buddy.py",
          "line": 8,
          "category": "LLM06: Sensitive Information Disclosure",
          "title": "Hardcoded secret detected: Secret Keyword",
          "description": "detect-secrets found a potential Secret Keyword on line 8. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/examples/misc/multillm_memory.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential indirect prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Indirect prompt injection: Malicious instructions from external data sources\n\n**Location:** ML model detected",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/examples/misc/fitness_checker.py",
          "line": 38,
          "category": "LLM01: Prompt Injection",
          "title": "User input 'user_input' embedded in LLM prompt",
          "description": "User input 'user_input' flows to LLM call via f-string in variable 'prompt'. Function 'fitness_coach' may be vulnerable to prompt injection attacks.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/examples/misc/fitness_checker.py",
          "line": 47,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'agent.run' is used in 'run(' on line 47 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/examples/misc/fitness_checker.py",
          "line": 6,
          "category": "LLM06: Sensitive Information Disclosure",
          "title": "Hardcoded secret detected: Secret Keyword",
          "description": "detect-secrets found a potential Secret Keyword on line 6. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/examples/misc/fitness_checker.py",
          "line": 7,
          "category": "LLM06: Sensitive Information Disclosure",
          "title": "Hardcoded secret detected: Secret Keyword",
          "description": "detect-secrets found a potential Secret Keyword on line 7. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/embedchain.py",
          "line": 90,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous sql_injection sink",
          "description": "LLM output from 'self.llm.update_history' is used in 'UPDATE' on line 90 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application security",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/embedchain.py",
          "line": 618,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous sql_injection sink",
          "description": "LLM output from 'self.llm.update_history' is used in 'UPDATE' on line 618 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application securit",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/embedchain.py",
          "line": 755,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous sql_injection sink",
          "description": "LLM output from 'self.llm.memory.get' is used in 'DELETE' on line 755 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application security.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/embedchain.py",
          "line": 765,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous sql_injection sink",
          "description": "LLM output from 'self.llm.memory.delete' is used in 'DELETE' on line 765 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application security",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/embedchain.py",
          "line": 766,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous sql_injection sink",
          "description": "LLM output from 'self.llm.update_history' is used in 'DELETE' on line 766 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application securit",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/client.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/constants.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/factory.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/cli.py",
          "line": 111,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'subprocess.run' is used in 'run(' on line 111 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/cli.py",
          "line": 113,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'ctx.invoke' is used in 'run(' on line 113 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/cli.py",
          "line": 122,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'subprocess.run' is used in 'run(' on line 122 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/cli.py",
          "line": 132,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'subprocess.run' is used in 'run(' on line 132 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/cli.py",
          "line": 144,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'subprocess.run' is used in 'run(' on line 144 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/cli.py",
          "line": 168,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'subprocess.run' is used in 'run(' on line 168 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/cli.py",
          "line": 229,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'subprocess.run' is used in 'run(' on line 229 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/cli.py",
          "line": 240,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'subprocess.run' is used in 'run(' on line 240 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/cli.py",
          "line": 251,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'subprocess.run' is used in 'run(' on line 251 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/cli.py",
          "line": 268,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'subprocess.run' is used in 'run(' on line 268 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/cli.py",
          "line": 279,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'subprocess.run' is used in 'run(' on line 279 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/cli.py",
          "line": 111,
          "category": "LLM05: Supply Chain Vulnerabilities",
          "title": "Network fetch combined with code execution",
          "description": "This file downloads external content (lines [76, 82]) and executes code (lines [111, 122, 132]). This pattern enables remote code execution attacks if the fetched content is not properly validated.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/cli.py",
          "line": 62,
          "category": "LLM08: Excessive Agency",
          "title": "Direct execution of LLM-generated code in 'create_app'",
          "description": "Function 'create_app' on line 62 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM outputs ",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/cli.py",
          "line": 117,
          "category": "LLM08: Excessive Agency",
          "title": "Direct execution of LLM-generated code in 'install_reqs'",
          "description": "Function 'install_reqs' on line 117 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM outpu",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/cli.py",
          "line": 142,
          "category": "LLM08: Excessive Agency",
          "title": "Direct execution of LLM-generated code in 'start'",
          "description": "Function 'start' on line 142 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM outputs can ",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/cli.py",
          "line": 219,
          "category": "LLM08: Excessive Agency",
          "title": "Direct execution of LLM-generated code in 'run_dev_fly_io'",
          "description": "Function 'run_dev_fly_io' on line 219 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM out",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/cli.py",
          "line": 236,
          "category": "LLM08: Excessive Agency",
          "title": "Direct execution of LLM-generated code in 'run_dev_modal_com'",
          "description": "Function 'run_dev_modal_com' on line 236 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM ",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/cli.py",
          "line": 247,
          "category": "LLM08: Excessive Agency",
          "title": "Direct execution of LLM-generated code in 'run_dev_streamlit_io'",
          "description": "Function 'run_dev_streamlit_io' on line 247 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy L",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/cli.py",
          "line": 258,
          "category": "LLM08: Excessive Agency",
          "title": "Direct execution of LLM-generated code in 'run_dev_render_com'",
          "description": "Function 'run_dev_render_com' on line 258 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/cli.py",
          "line": 275,
          "category": "LLM08: Excessive Agency",
          "title": "Direct execution of LLM-generated code in 'run_dev_gradio'",
          "description": "Function 'run_dev_gradio' on line 275 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM out",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/cli.py",
          "line": 62,
          "category": "LLM09: Overreliance",
          "title": "Direct execution of LLM output in 'create_app'",
          "description": "Function 'create_app' on line 62 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/cli.py",
          "line": 117,
          "category": "LLM09: Overreliance",
          "title": "Direct execution of LLM output in 'install_reqs'",
          "description": "Function 'install_reqs' on line 117 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/cli.py",
          "line": 142,
          "category": "LLM09: Overreliance",
          "title": "Direct execution of LLM output in 'start'",
          "description": "Function 'start' on line 142 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/cli.py",
          "line": 219,
          "category": "LLM09: Overreliance",
          "title": "Direct execution of LLM output in 'run_dev_fly_io'",
          "description": "Function 'run_dev_fly_io' on line 219 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/cli.py",
          "line": 236,
          "category": "LLM09: Overreliance",
          "title": "Direct execution of LLM output in 'run_dev_modal_com'",
          "description": "Function 'run_dev_modal_com' on line 236 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/cli.py",
          "line": 247,
          "category": "LLM09: Overreliance",
          "title": "Direct execution of LLM output in 'run_dev_streamlit_io'",
          "description": "Function 'run_dev_streamlit_io' on line 247 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/cli.py",
          "line": 258,
          "category": "LLM09: Overreliance",
          "title": "Direct execution of LLM output in 'run_dev_render_com'",
          "description": "Function 'run_dev_render_com' on line 258 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/cli.py",
          "line": 275,
          "category": "LLM09: Overreliance",
          "title": "Direct execution of LLM output in 'run_dev_gradio'",
          "description": "Function 'run_dev_gradio' on line 275 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/app.py",
          "line": 14,
          "category": "LLM05: Supply Chain Vulnerabilities",
          "title": "Network fetch combined with code execution",
          "description": "This file downloads external content (lines [191, 215, 235]) and executes code (lines [14, 15, 27]). This pattern enables remote code execution attacks if the fetched content is not properly validated",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/app.py",
          "line": 288,
          "category": "LLM05: Supply Chain Vulnerabilities",
          "title": "Code execution on external content",
          "description": "eval() on non-literal content on line 288. File fetches external content - HIGH RISK.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/examples/telegram_bot/telegram_bot.py",
          "line": 51,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/examples/api_server/api_server.py",
          "line": 48,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'App().chat' is used in 'run(' on line 48 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/examples/unacademy-ai/app.py",
          "line": 69,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential indirect prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Indirect prompt injection: Malicious instructions from external data sources\n\n**Location:** LLM call with pot",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/examples/unacademy-ai/app.py",
          "line": 66,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential indirect prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Indirect prompt injection: Malicious instructions from external data sources\n\n**Location:** LLM call with pot",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/examples/unacademy-ai/app.py",
          "line": 70,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential indirect prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Indirect prompt injection: Malicious instructions from external data sources\n\n**Location:** LLM call with pot",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/examples/unacademy-ai/app.py",
          "line": 74,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential indirect prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Indirect prompt injection: Malicious instructions from external data sources\n\n**Location:** LLM call with pot",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/examples/whatsapp_bot/whatsapp_bot.py",
          "line": 44,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/examples/whatsapp_bot/whatsapp_bot.py",
          "line": 44,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'chat_bot.chat' is used in 'run(' on line 44 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/examples/whatsapp_bot/whatsapp_bot.py",
          "line": 42,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** Function with user ",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/examples/rest-api/models.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/examples/rest-api/database.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/examples/rest-api/utils.py",
          "line": 3,
          "category": "LLM06: Sensitive Information Disclosure",
          "title": "Hardcoded secret detected: Secret Keyword",
          "description": "detect-secrets found a potential Secret Keyword on line 3. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/examples/rest-api/utils.py",
          "line": 7,
          "category": "LLM06: Sensitive Information Disclosure",
          "title": "Hardcoded secret detected: Secret Keyword",
          "description": "detect-secrets found a potential Secret Keyword on line 7. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/examples/rest-api/utils.py",
          "line": 8,
          "category": "LLM06: Sensitive Information Disclosure",
          "title": "Hardcoded secret detected: Secret Keyword",
          "description": "detect-secrets found a potential Secret Keyword on line 8. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/examples/rest-api/utils.py",
          "line": 9,
          "category": "LLM06: Sensitive Information Disclosure",
          "title": "Hardcoded secret detected: Secret Keyword",
          "description": "detect-secrets found a potential Secret Keyword on line 9. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/examples/rest-api/utils.py",
          "line": 10,
          "category": "LLM06: Sensitive Information Disclosure",
          "title": "Hardcoded secret detected: Secret Keyword",
          "description": "detect-secrets found a potential Secret Keyword on line 10. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/examples/rest-api/main.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/examples/full_stack/backend/models.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/examples/full_stack/backend/routes/dashboard.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/examples/full_stack/backend/routes/sources.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/examples/full_stack/backend/routes/chat_response.py",
          "line": 28,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/examples/nextjs/ec_app/app.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/examples/nextjs/nextjs_discord/app.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/examples/nextjs/nextjs_slack/app.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/bots/whatsapp.py",
          "line": 69,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'app.run' is used in 'run(' on line 69 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/bots/whatsapp.py",
          "line": 69,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/bots/poe.py",
          "line": 73,
          "category": "LLM04: Model Denial of Service",
          "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
          "description": "Function 'ask_bot' on line 73 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust ",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/bots/discord.py",
          "line": 54,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'client.run' is used in 'run(' on line 54 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/bots/slack.py",
          "line": 66,
          "category": "LLM01: Prompt Injection",
          "title": "User input 'message' embedded in LLM prompt",
          "description": "User input parameter 'message' is directly passed to LLM API call 'self.client.chat_postMessage'. This is a high-confidence prompt injection vector.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/bots/slack.py",
          "line": 87,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'app.run' is used in 'run(' on line 87 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/bots/slack.py",
          "line": 65,
          "category": "LLM04: Model Denial of Service",
          "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
          "description": "Function 'send_slack_message' on line 65 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers ",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/migrations/env.py",
          "line": 21,
          "category": "LLM09: Overreliance",
          "title": "Critical decision without oversight in 'run_migrations_offline'",
          "description": "Function 'run_migrations_offline' on line 21 makes critical financial decisions based on LLM output without human oversight or verification. Action edges detected (HTTP/file/DB/subprocess) - risk of a",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/migrations/env.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/nvidia.py",
          "line": 65,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/vllm.py",
          "line": 40,
          "category": "LLM01: Prompt Injection",
          "title": "User input 'prompt' embedded in LLM prompt",
          "description": "User input parameter 'prompt' is directly passed to LLM API call 'llm.invoke'. This is a high-confidence prompt injection vector.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/vllm.py",
          "line": 40,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous sql_injection sink",
          "description": "LLM output from 'llm.invoke' is used in 'UPDATE' on line 40 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application security.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/jina.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/google.py",
          "line": 52,
          "category": "LLM01: Prompt Injection",
          "title": "User input 'prompt' embedded in LLM prompt",
          "description": "User input parameter 'prompt' is directly passed to LLM API call 'model.generate_content'. This is a high-confidence prompt injection vector.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/gpt4all.py",
          "line": 44,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** Function with user ",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/aws_bedrock.py",
          "line": 57,
          "category": "LLM01: Prompt Injection",
          "title": "User input 'prompt' embedded in LLM prompt",
          "description": "User input parameter 'prompt' is directly passed to LLM API call 'llm.invoke'. This is a high-confidence prompt injection vector.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/groq.py",
          "line": 64,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/groq.py",
          "line": 47,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** Function with user ",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/openai.py",
          "line": 118,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/openai.py",
          "line": 99,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output flows to command_injection sink",
          "description": "LLM output variable 'chat' flows to 'self._query_function_call' on line 99 via direct flow. This creates a command_injection vulnerability.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/openai.py",
          "line": 106,
          "category": "LLM04: Model Denial of Service",
          "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
          "description": "Function '_query_function_call' on line 106 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attacke",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/openai.py",
          "line": 101,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/openai.py",
          "line": 50,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** Function with user ",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/azure_openai.py",
          "line": 40,
          "category": "LLM01: Prompt Injection",
          "title": "User input 'prompt' embedded in LLM prompt",
          "description": "User input 'prompt' flows to LLM call via call in variable 'messages'. Function '_get_answer' may be vulnerable to prompt injection attacks.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/vertex_ai.py",
          "line": 64,
          "category": "LLM01: Prompt Injection",
          "title": "User input 'prompt' embedded in LLM prompt",
          "description": "User input 'prompt' flows to LLM call via call in variable 'messages'. Function '_get_answer' may be vulnerable to prompt injection attacks.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/vertex_ai.py",
          "line": 65,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/vertex_ai.py",
          "line": 62,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/huggingface.py",
          "line": 69,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/huggingface.py",
          "line": 80,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/huggingface.py",
          "line": 99,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/huggingface.py",
          "line": 50,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** Function with user ",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/huggingface.py",
          "line": 72,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** Function with user ",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/huggingface.py",
          "line": 83,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** Function with user ",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/anthropic.py",
          "line": 54,
          "category": "LLM01: Prompt Injection",
          "title": "User input 'prompt' embedded in LLM prompt",
          "description": "User input 'prompt' flows to LLM call via call in variable 'messages'. Function '_get_answer' may be vulnerable to prompt injection attacks.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/anthropic.py",
          "line": 49,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/anthropic.py",
          "line": 56,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/anthropic.py",
          "line": 47,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** Function with user ",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/together.py",
          "line": 68,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/together.py",
          "line": 58,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** Function with user ",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/mistralai.py",
          "line": 69,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/mistralai.py",
          "line": 39,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** Function with user ",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/clarifai.py",
          "line": 38,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/clarifai.py",
          "line": 21,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** Function with user ",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/cohere.py",
          "line": 63,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/cohere.py",
          "line": 53,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** Function with user ",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/llama2.py",
          "line": 53,
          "category": "LLM01: Prompt Injection",
          "title": "User input 'prompt' embedded in LLM prompt",
          "description": "User input parameter 'prompt' is directly passed to LLM API call 'llm.invoke'. This is a high-confidence prompt injection vector.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/ollama.py",
          "line": 54,
          "category": "LLM01: Prompt Injection",
          "title": "User input 'prompt' embedded in LLM prompt",
          "description": "User input parameter 'prompt' is directly passed to LLM API call 'llm.invoke'. This is a high-confidence prompt injection vector.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/base.py",
          "line": 251,
          "category": "LLM01: Prompt Injection",
          "title": "User input 'contexts' embedded in LLM prompt",
          "description": "User input parameter 'contexts' is directly passed to LLM API call 'self.generate_prompt'. This is a high-confidence prompt injection vector.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/base.py",
          "line": 314,
          "category": "LLM01: Prompt Injection",
          "title": "User input 'contexts' embedded in LLM prompt",
          "description": "User input parameter 'contexts' is directly passed to LLM API call 'self.generate_prompt'. This is a high-confidence prompt injection vector.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/base.py",
          "line": 195,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'search.run' is used in 'run(' on line 195 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/base.py",
          "line": 178,
          "category": "LLM04: Model Denial of Service",
          "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
          "description": "Function 'access_search_and_get_results' on line 178 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enabl",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/base.py",
          "line": 214,
          "category": "LLM04: Model Denial of Service",
          "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
          "description": "Function 'query' on line 214 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust m",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/base.py",
          "line": 274,
          "category": "LLM04: Model Denial of Service",
          "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
          "description": "Function 'chat' on line 274 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust mo",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/memory/base.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/chunkers/base_chunker.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/vectordb/opensearch.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/vectordb/qdrant.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/vectordb/chroma.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/vectordb/zilliz.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/data_formatter/data_formatter.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/embedder/google.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/embedder/gpt4all.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/embedder/openai.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/embedder/azure_openai.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/embedder/huggingface.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/embedder/mistralai.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/misc.py",
          "line": 134,
          "category": "LLM05: Supply Chain Vulnerabilities",
          "title": "Network fetch combined with code execution",
          "description": "This file downloads external content (lines [249]) and executes code (lines [134, 135, 517]). This pattern enables remote code execution attacks if the fetched content is not properly validated.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/misc.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/cli.py",
          "line": 100,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'subprocess.run' is used in 'run(' on line 100 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/cli.py",
          "line": 35,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'subprocess.run' is used in 'run(' on line 35 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/cli.py",
          "line": 55,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'subprocess.run' is used in 'run(' on line 55 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/cli.py",
          "line": 77,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'subprocess.run' is used in 'run(' on line 77 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/cli.py",
          "line": 128,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'subprocess.run' is used in 'run(' on line 128 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/cli.py",
          "line": 139,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'subprocess.run' is used in 'run(' on line 139 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/cli.py",
          "line": 150,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'subprocess.run' is used in 'run(' on line 150 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/cli.py",
          "line": 167,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'subprocess.run' is used in 'run(' on line 167 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/cli.py",
          "line": 178,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'subprocess.run' is used in 'run(' on line 178 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/cli.py",
          "line": 229,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'subprocess.run' is used in 'run(' on line 229 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/cli.py",
          "line": 233,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'subprocess.run' is used in 'run(' on line 233 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/cli.py",
          "line": 248,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'subprocess.run' is used in 'run(' on line 248 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/cli.py",
          "line": 269,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'subprocess.run' is used in 'run(' on line 269 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/cli.py",
          "line": 284,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'subprocess.run' is used in 'run(' on line 284 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/cli.py",
          "line": 299,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'subprocess.run' is used in 'run(' on line 299 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/cli.py",
          "line": 317,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'subprocess.run' is used in 'run(' on line 317 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/cli.py",
          "line": 258,
          "category": "LLM04: Model Denial of Service",
          "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits",
          "description": "Function 'deploy_streamlit' on line 258 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaus",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/cli.py",
          "line": 30,
          "category": "LLM08: Excessive Agency",
          "title": "Direct execution of LLM-generated code in 'setup_fly_io_app'",
          "description": "Function 'setup_fly_io_app' on line 30 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM ou",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/cli.py",
          "line": 135,
          "category": "LLM08: Excessive Agency",
          "title": "Direct execution of LLM-generated code in 'run_dev_modal_com'",
          "description": "Function 'run_dev_modal_com' on line 135 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM ",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/cli.py",
          "line": 146,
          "category": "LLM08: Excessive Agency",
          "title": "Direct execution of LLM-generated code in 'run_dev_streamlit_io'",
          "description": "Function 'run_dev_streamlit_io' on line 146 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy L",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/cli.py",
          "line": 157,
          "category": "LLM08: Excessive Agency",
          "title": "Direct execution of LLM-generated code in 'run_dev_render_com'",
          "description": "Function 'run_dev_render_com' on line 157 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/cli.py",
          "line": 174,
          "category": "LLM08: Excessive Agency",
          "title": "Direct execution of LLM-generated code in 'run_dev_gradio'",
          "description": "Function 'run_dev_gradio' on line 174 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM out",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/cli.py",
          "line": 211,
          "category": "LLM08: Excessive Agency",
          "title": "Direct execution of LLM-generated code in 'deploy_fly'",
          "description": "Function 'deploy_fly' on line 211 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM outputs",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/cli.py",
          "line": 244,
          "category": "LLM08: Excessive Agency",
          "title": "Direct execution of LLM-generated code in 'deploy_modal'",
          "description": "Function 'deploy_modal' on line 244 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM outpu",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/cli.py",
          "line": 258,
          "category": "LLM08: Excessive Agency",
          "title": "Direct execution of LLM-generated code in 'deploy_streamlit'",
          "description": "Function 'deploy_streamlit' on line 258 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM o",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/cli.py",
          "line": 279,
          "category": "LLM08: Excessive Agency",
          "title": "Direct execution of LLM-generated code in 'deploy_render'",
          "description": "Function 'deploy_render' on line 279 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM outp",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/cli.py",
          "line": 294,
          "category": "LLM08: Excessive Agency",
          "title": "Direct execution of LLM-generated code in 'deploy_gradio_app'",
          "description": "Function 'deploy_gradio_app' on line 294 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM ",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/cli.py",
          "line": 309,
          "category": "LLM08: Excessive Agency",
          "title": "Direct execution of LLM-generated code in 'deploy_hf_spaces'",
          "description": "Function 'deploy_hf_spaces' on line 309 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM o",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/cli.py",
          "line": 30,
          "category": "LLM09: Overreliance",
          "title": "Direct execution of LLM output in 'setup_fly_io_app'",
          "description": "Function 'setup_fly_io_app' on line 30 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/cli.py",
          "line": 45,
          "category": "LLM09: Overreliance",
          "title": "Direct execution of LLM output in 'setup_modal_com_app'",
          "description": "Function 'setup_modal_com_app' on line 45 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/cli.py",
          "line": 67,
          "category": "LLM09: Overreliance",
          "title": "Direct execution of LLM output in 'setup_render_com_app'",
          "description": "Function 'setup_render_com_app' on line 67 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/cli.py",
          "line": 99,
          "category": "LLM09: Overreliance",
          "title": "Direct execution of LLM output in 'setup_hf_app'",
          "description": "Function 'setup_hf_app' on line 99 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/cli.py",
          "line": 118,
          "category": "LLM09: Overreliance",
          "title": "Direct execution of LLM output in 'run_dev_fly_io'",
          "description": "Function 'run_dev_fly_io' on line 118 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/cli.py",
          "line": 135,
          "category": "LLM09: Overreliance",
          "title": "Direct execution of LLM output in 'run_dev_modal_com'",
          "description": "Function 'run_dev_modal_com' on line 135 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/cli.py",
          "line": 146,
          "category": "LLM09: Overreliance",
          "title": "Direct execution of LLM output in 'run_dev_streamlit_io'",
          "description": "Function 'run_dev_streamlit_io' on line 146 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/cli.py",
          "line": 157,
          "category": "LLM09: Overreliance",
          "title": "Direct execution of LLM output in 'run_dev_render_com'",
          "description": "Function 'run_dev_render_com' on line 157 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/cli.py",
          "line": 174,
          "category": "LLM09: Overreliance",
          "title": "Direct execution of LLM output in 'run_dev_gradio'",
          "description": "Function 'run_dev_gradio' on line 174 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/cli.py",
          "line": 211,
          "category": "LLM09: Overreliance",
          "title": "Direct execution of LLM output in 'deploy_fly'",
          "description": "Function 'deploy_fly' on line 211 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/cli.py",
          "line": 244,
          "category": "LLM09: Overreliance",
          "title": "Direct execution of LLM output in 'deploy_modal'",
          "description": "Function 'deploy_modal' on line 244 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/cli.py",
          "line": 258,
          "category": "LLM09: Overreliance",
          "title": "Direct execution of LLM output in 'deploy_streamlit'",
          "description": "Function 'deploy_streamlit' on line 258 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/cli.py",
          "line": 279,
          "category": "LLM09: Overreliance",
          "title": "Direct execution of LLM output in 'deploy_render'",
          "description": "Function 'deploy_render' on line 279 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/cli.py",
          "line": 294,
          "category": "LLM09: Overreliance",
          "title": "Direct execution of LLM output in 'deploy_gradio_app'",
          "description": "Function 'deploy_gradio_app' on line 294 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/cli.py",
          "line": 309,
          "category": "LLM09: Overreliance",
          "title": "Direct execution of LLM output in 'deploy_hf_spaces'",
          "description": "Function 'deploy_hf_spaces' on line 309 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/models/vector_dimensions.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/models/embedding_functions.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/models/data_type.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/models/providers.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/telemetry/posthog.py",
          "line": 14,
          "category": "LLM06: Sensitive Information Disclosure",
          "title": "Hardcoded secret detected: Base64 High Entropy String",
          "description": "detect-secrets found a potential Base64 High Entropy String on line 14. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/telemetry/posthog.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/loaders/pdf_file.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/loaders/substack.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/loaders/openapi.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/loaders/discord.py",
          "line": 134,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous command_injection sink",
          "description": "LLM output from 'client.run' is used in 'run(' on line 134 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/loaders/discord.py",
          "line": 134,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/loaders/web_page.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/loaders/local_qna_pair.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/loaders/docs_site_loader.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/loaders/beehiiv.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/loaders/dropbox.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/loaders/gmail.py",
          "line": 57,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output used in dangerous sql_injection sink",
          "description": "LLM output from 'flow.run_local_server' is used in 'execute(' on line 57 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application security",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/loaders/gmail.py",
          "line": 57,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/loaders/discourse.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/loaders/csv.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/loaders/postgres.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/loaders/directory_loader.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/loaders/audio.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/loaders/notion.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/loaders/json.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/loaders/image.py",
          "line": 29,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/loaders/image.py",
          "line": 28,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** Function with user ",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/loaders/slack.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/loaders/rss_feed.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/loaders/mysql.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/loaders/unstructured_file.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/helpers/json_serializable.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/store/assistants.py",
          "line": 105,
          "category": "LLM01: Prompt Injection",
          "title": "User input 'message' embedded in LLM prompt",
          "description": "User input parameter 'message' is directly passed to LLM API call 'self._client.beta.threads.messages.create'. This is a high-confidence prompt injection vector.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/store/assistants.py",
          "line": 203,
          "category": "LLM01: Prompt Injection",
          "title": "User input 'query' embedded in LLM prompt",
          "description": "User input parameter 'query' is directly passed to LLM API call 'self.pipeline.chat'. This is a high-confidence prompt injection vector.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/store/assistants.py",
          "line": 119,
          "category": "LLM02: Insecure Output Handling",
          "title": "LLM output flows to command_injection sink",
          "description": "LLM output variable 'run_id' flows to 'self._client.beta.threads.runs.retrieve' on line 119 via direct flow. This creates a command_injection vulnerability.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/store/assistants.py",
          "line": 104,
          "category": "LLM04: Model Denial of Service",
          "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
          "description": "Function '_send_message' on line 104 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to e",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/store/assistants.py",
          "line": 108,
          "category": "LLM04: Model Denial of Service",
          "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits",
          "description": "Function '_wait_for_completion' on line 108 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to ex",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/evaluation/metrics/groundedness.py",
          "line": 42,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential prompt injection vulnerability (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  ",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/evaluation/metrics/groundedness.py",
          "line": 63,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential prompt injection vulnerability (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  ",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/evaluation/metrics/context_relevancy.py",
          "line": 42,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/evaluation/metrics/answer_relevancy.py",
          "line": 43,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/evaluation/metrics/answer_relevancy.py",
          "line": 39,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** Function with user ",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/deployment/render.com/app.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/deployment/modal.com/app.py",
          "line": 74,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with poten",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/deployment/gradio.app/app.py",
          "line": 13,
          "category": "LLM01: Prompt Injection",
          "title": "User input 'message' embedded in LLM prompt",
          "description": "User input parameter 'message' is directly passed to LLM API call 'app.chat'. This is a high-confidence prompt injection vector.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/deployment/gradio.app/app.py",
          "line": 12,
          "category": "LLM04: Model Denial of Service",
          "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
          "description": "Function 'query' on line 12 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust mo",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/deployment/fly.io/app.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/config/llm/base.py",
          "line": 105,
          "category": "LLM05: Supply Chain Vulnerabilities",
          "title": "Network fetch combined with code execution",
          "description": "This file downloads external content (lines [228]) and executes code (lines [105, 106, 107]). This pattern enables remote code execution attacks if the fetched content is not properly validated.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/config/llm/base.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/config/evaluation/base.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/core/db/models.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/core/db/database.py",
          "line": 1,
          "category": "LLM01: Prompt Injection (ML-Detected)",
          "title": "Potential direct prompt injection (ML-detected)",
          "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected v",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/openmemory/api/alembic/env.py",
          "line": 37,
          "category": "LLM08: Excessive Agency",
          "title": "High-risk execute/network operation without confirmation in 'run_migrations_offline'",
          "description": "Function 'run_migrations_offline' on line 37 performs high-risk execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously e",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/openmemory/api/alembic/env.py",
          "line": 61,
          "category": "LLM08: Excessive Agency",
          "title": "High-risk write/execute operation without confirmation in 'run_migrations_online'",
          "description": "Function 'run_migrations_online' on line 61 performs high-risk write/execute operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously exec",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/openmemory/api/alembic/versions/add_config_table.py",
          "line": 15,
          "category": "LLM06: Sensitive Information Disclosure",
          "title": "Hardcoded secret detected: Hex High Entropy String",
          "description": "detect-secrets found a potential Hex High Entropy String on line 15. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/openmemory/api/alembic/versions/0b53c747049a_initial_migration.py",
          "line": 14,
          "category": "LLM06: Sensitive Information Disclosure",
          "title": "Hardcoded secret detected: Hex High Entropy String",
          "description": "detect-secrets found a potential Hex High Entropy String on line 14. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
          "is_semantic_taint": true
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/cassandra.py",
          "line": 137,
          "category": "SQL Injection",
          "title": "SQL injection: f-string formatting",
          "description": "SQL query on line 137 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
          "is_semantic_taint": false
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/cassandra.py",
          "line": 163,
          "category": "SQL Injection",
          "title": "SQL injection: f-string formatting",
          "description": "SQL query on line 163 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
          "is_semantic_taint": false
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/cassandra.py",
          "line": 198,
          "category": "SQL Injection",
          "title": "SQL injection: f-string formatting",
          "description": "SQL query on line 198 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
          "is_semantic_taint": false
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/cassandra.py",
          "line": 234,
          "category": "SQL Injection",
          "title": "SQL injection: f-string formatting",
          "description": "SQL query on line 234 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
          "is_semantic_taint": false
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/cassandra.py",
          "line": 290,
          "category": "SQL Injection",
          "title": "SQL injection: f-string formatting",
          "description": "SQL query on line 290 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
          "is_semantic_taint": false
        },
        {
          "file": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/cassandra.py",
          "line": 317,
          "category": "SQL Injection",
          "title": "SQL injection: f-string formatting",
          "description": "SQL query on line 317 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
          "is_semantic_taint": false
        }
      ]
    }
  ]
}