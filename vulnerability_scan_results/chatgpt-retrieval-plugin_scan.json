{
  "report_type": "static_scan",
  "generated_at": "2026-01-14T14:16:44.449820Z",
  "summary": {
    "target": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpgwxahnts/chatgpt-retrieval-plugin",
    "files_scanned": 51,
    "overall_score": 2.95,
    "confidence": 0.68,
    "duration_seconds": 1.516,
    "findings_count": 48,
    "severity_breakdown": {
      "CRITICAL": 32,
      "HIGH": 12,
      "MEDIUM": 2,
      "LOW": 2,
      "INFO": 0
    }
  },
  "category_scores": [
    {
      "category_id": "1_prompt_security",
      "category_name": "Prompt Security",
      "score": 16,
      "confidence": 0.48,
      "subscores": {
        "input_validation": 0,
        "jailbreak_prevention": 30,
        "monitoring_response": 25,
        "prompt_injection_defense": 0,
        "context_isolation": 75,
        "prompt_templates": 0,
        "output_filtering": 0,
        "jailbreak_detection": 0
      },
      "detected_controls": [
        "Pattern Detection input validation detected",
        "1 jailbreak prevention mechanisms active",
        "Context protection: Token Limiting"
      ],
      "gaps": [
        "Missing behavioral analysis/guardrails library",
        "No red team testing detected",
        "No attack detection tools found"
      ]
    },
    {
      "category_id": "2_model_security",
      "category_name": "Model Security & Integrity",
      "score": 0,
      "confidence": 0.39,
      "subscores": {
        "model_protection": 41,
        "extraction_defense": 35,
        "supply_chain_security": 0,
        "api_key_security": 0,
        "model_access_control": 0,
        "rate_limiting_defense": 0,
        "model_encryption": 0,
        "model_provenance": 0,
        "supply_chain_verification": 0
      },
      "detected_controls": [
        "Encryption in transit"
      ],
      "gaps": [
        "Extraction defense is weak",
        "Implement rate limiting and output protections",
        "Supply chain security needs attention",
        "Add model verification and dependency scanning"
      ]
    },
    {
      "category_id": "3_data_privacy",
      "category_name": "Data Privacy & PII Protection",
      "score": 1,
      "confidence": 0.42,
      "subscores": {
        "pii_handling": 0,
        "consent_management": 0,
        "compliance_auditing": 0,
        "pii_detection": 0,
        "pii_redaction": 0,
        "data_encryption": 0,
        "consent_management_comprehensive": 0,
        "data_lifecycle": 0,
        "gdpr_compliance": 0,
        "audit_logging": 30
      },
      "detected_controls": [
        "PII pattern matching",
        "Audit logging"
      ],
      "gaps": [
        "PII detection and handling needs improvement",
        "Consider implementing PII detection tools (Presidio, spaCy NER)",
        "Consent management is weak",
        "Implement user consent mechanisms and user rights endpoints",
        "Regulatory compliance controls missing",
        "Add audit logging and compliance documentation"
      ]
    },
    {
      "category_id": "4_hallucination_mitigation",
      "category_name": "Hallucination Detection & Mitigation",
      "score": 0,
      "confidence": 0.38,
      "subscores": {
        "factual_consistency": 0,
        "output_validation": 0,
        "user_feedback": 0,
        "rag_integration": 0,
        "citation_tracking": 0,
        "confidence_scoring": 0,
        "fact_checking": 0,
        "consistency_checks": 0,
        "grounding": 0
      },
      "detected_controls": [],
      "gaps": [
        "Missing controls: Citation tracking / RAG, Confidence scoring, Consistency checks, Constraint checking, Cross-validation",
        "Plus 9 more controls need implementation"
      ]
    },
    {
      "category_id": "5_ethical_ai",
      "category_name": "Ethical AI & Bias Detection",
      "score": 0,
      "confidence": 0.4,
      "subscores": {
        "bias_testing": 0,
        "fairness_metrics": 0,
        "explainability": 0,
        "harmful_content_filtering": 0,
        "diverse_training": 0,
        "transparency": 0,
        "legacy_bias_detection": 0,
        "legacy_fairness": 0
      },
      "detected_controls": [],
      "gaps": []
    },
    {
      "category_id": "6_governance",
      "category_name": "Governance & Compliance",
      "score": 5,
      "confidence": 0.46,
      "subscores": {
        "plugin_security": 0,
        "authorization_controls": 0,
        "audit_logging": 0,
        "incident_response": 0,
        "model_documentation": 50,
        "risk_assessment": 0,
        "user_consent": 0,
        "legacy_governance": 0
      },
      "detected_controls": [],
      "gaps": []
    },
    {
      "category_id": "0_owasp_vulnerabilities",
      "category_name": "OWASP LLM Vulnerabilities",
      "score": 0,
      "confidence": 0.97,
      "subscores": {
        "LLM01": 100,
        "LLM02": 100,
        "LLM03": 100,
        "LLM04": 68,
        "LLM05": 93,
        "LLM06": 100,
        "LLM07": 100,
        "LLM08": 86,
        "LLM09": 100,
        "LLM10": 100
      },
      "detected_controls": [
        "Prompt Injection (no vulnerabilities found)",
        "Insecure Output Handling (no vulnerabilities found)",
        "Training Data Poisoning (no vulnerabilities found)",
        "Sensitive Information Disclosure (no vulnerabilities found)",
        "Insecure Plugin Design (no vulnerabilities found)",
        "Overreliance (no vulnerabilities found)",
        "Model Theft (no vulnerabilities found)"
      ],
      "gaps": [
        "Model Denial of Service: 1 critical",
        "Supply Chain Vulnerabilities: 2 low",
        "Excessive Agency: 2 medium",
        "ML: 31 critical",
        "SQLI: 12 high"
      ]
    }
  ],
  "findings": [
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpgwxahnts/chatgpt-retrieval-plugin/server/main.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpgwxahnts/chatgpt-retrieval-plugin/server/main.py",
      "line_number": 1,
      "code_snippet": "import os\nfrom typing import Optional\nimport uvicorn\nfrom fastapi import FastAPI, File, Form, HTTPException, Depends, Body, UploadFile\nfrom fastapi.security import HTTPBearer, HTTPAuthorizationCredentials\nfrom fastapi.staticfiles import StaticFiles\nfrom loguru import logger\n\nfrom models.api import (\n    DeleteRequest,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpgwxahnts/chatgpt-retrieval-plugin/local_server/main.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpgwxahnts/chatgpt-retrieval-plugin/local_server/main.py",
      "line_number": 1,
      "code_snippet": "# This is a version of the main.py file found in ../../../server/main.py for testing the plugin locally.\n# Use the command `poetry run dev` to run this.\nfrom typing import Optional\nimport uvicorn\nfrom fastapi import FastAPI, File, Form, HTTPException, Body, UploadFile\nfrom loguru import logger\n\nfrom models.api import (\n    DeleteRequest,\n    DeleteResponse,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpgwxahnts/chatgpt-retrieval-plugin/datastore/datastore.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpgwxahnts/chatgpt-retrieval-plugin/datastore/datastore.py",
      "line_number": 1,
      "code_snippet": "from abc import ABC, abstractmethod\nfrom typing import Dict, List, Optional\nimport asyncio\n\nfrom models.models import (\n    Document,\n    DocumentChunk,\n    DocumentMetadataFilter,\n    Query,\n    QueryResult,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpgwxahnts/chatgpt-retrieval-plugin/services/extract_metadata.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpgwxahnts/chatgpt-retrieval-plugin/services/extract_metadata.py",
      "line_number": 1,
      "code_snippet": "from models.models import Source\nfrom services.openai import get_chat_completion\nimport json\nfrom typing import Dict\nimport os\nfrom loguru import logger\n\n\ndef extract_metadata_from_document(text: str) -> Dict[str, str]:\n    sources = Source.__members__.keys()",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpgwxahnts/chatgpt-retrieval-plugin/services/file.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpgwxahnts/chatgpt-retrieval-plugin/services/file.py",
      "line_number": 1,
      "code_snippet": "import os\nfrom io import BufferedReader\nfrom typing import Optional\nfrom fastapi import UploadFile\nimport mimetypes\nfrom PyPDF2 import PdfReader\nimport docx2txt\nimport csv\nimport pptx\nfrom loguru import logger",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpgwxahnts/chatgpt-retrieval-plugin/services/openai.py_66",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpgwxahnts/chatgpt-retrieval-plugin/services/openai.py",
      "line_number": 66,
      "code_snippet": "    # Note: Azure Open AI requires deployment id\n    response = {}\n    if deployment_id == None:\n        response = openai.ChatCompletion.create(\n            model=model,\n            messages=messages,\n        )",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML format)\n4. Apply input validation and length limits\n5. Use allowlists for expected input formats\n6. Consider prompt injection detection libraries\n\n1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpgwxahnts/chatgpt-retrieval-plugin/services/openai.py_71",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpgwxahnts/chatgpt-retrieval-plugin/services/openai.py",
      "line_number": 71,
      "code_snippet": "            messages=messages,\n        )\n    else:\n        response = openai.ChatCompletion.create(\n            deployment_id=deployment_id,\n            messages=messages,\n        )",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML format)\n4. Apply input validation and length limits\n5. Use allowlists for expected input formats\n6. Consider prompt injection detection libraries\n\n1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM04_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpgwxahnts/chatgpt-retrieval-plugin/services/openai.py_44",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
      "description": "Function 'get_chat_completion' on line 44 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpgwxahnts/chatgpt-retrieval-plugin/services/openai.py",
      "line_number": 44,
      "code_snippet": "def get_chat_completion(\n    messages,\n    model=\"gpt-3.5-turbo\",  # use \"gpt-4\" for better results\n    deployment_id=None,\n):\n    \"\"\"\n    Generate a chat completion using OpenAI's chat completion API.\n\n    Args:\n        messages: The list of messages in the chat history.\n        model: The name of the model to use for the completion. Default is gpt-3.5-turbo, which is a fast, cheap and versatile model. Use gpt-4 for higher quality but slower results.",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpgwxahnts/chatgpt-retrieval-plugin/services/openai.py_32_unpinned",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "LOW",
      "confidence": 0.75,
      "title": "Unpinned model version in API call",
      "description": "Model 'EMBEDDING_MODEL' is used without version pinning on line 32. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. (Advisory: no dynamic execution detected in this file.)",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpgwxahnts/chatgpt-retrieval-plugin/services/openai.py",
      "line_number": 32,
      "code_snippet": "    response = {}\n    if deployment is None:\n        response = openai.Embedding.create(input=texts, model=EMBEDDING_MODEL, dimensions=EMBEDDING_DIMENSION)\n    else:\n        response = openai.Embedding.create(input=texts, deployment_id=deployment)\n",
      "recommendation": "Supply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpgwxahnts/chatgpt-retrieval-plugin/services/openai.py_66_unpinned",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "LOW",
      "confidence": 0.75,
      "title": "Unpinned model version in API call",
      "description": "Model 'model' is used without version pinning on line 66. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. (Advisory: no dynamic execution detected in this file.)",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpgwxahnts/chatgpt-retrieval-plugin/services/openai.py",
      "line_number": 66,
      "code_snippet": "    response = {}\n    if deployment_id == None:\n        response = openai.ChatCompletion.create(\n            model=model,\n            messages=messages,\n        )",
      "recommendation": "Supply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpgwxahnts/chatgpt-retrieval-plugin/services/openai.py_13_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "High-risk write/network operation without confirmation in 'get_embeddings'",
      "description": "Function 'get_embeddings' on line 13 performs high-risk write/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpgwxahnts/chatgpt-retrieval-plugin/services/openai.py",
      "line_number": 13,
      "code_snippet": "\n@retry(wait=wait_random_exponential(min=1, max=20), stop=stop_after_attempt(3))\ndef get_embeddings(texts: List[str]) -> List[List[float]]:\n    \"\"\"\n    Embed texts using OpenAI's ada model.\n",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpgwxahnts/chatgpt-retrieval-plugin/services/openai.py_44_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "High-risk write operation without confirmation in 'get_chat_completion'",
      "description": "Function 'get_chat_completion' on line 44 performs high-risk write operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpgwxahnts/chatgpt-retrieval-plugin/services/openai.py",
      "line_number": 44,
      "code_snippet": "\n@retry(wait=wait_random_exponential(min=1, max=20), stop=stop_after_attempt(3))\ndef get_chat_completion(\n    messages,\n    model=\"gpt-3.5-turbo\",  # use \"gpt-4\" for better results\n    deployment_id=None,",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpgwxahnts/chatgpt-retrieval-plugin/services/openai.py_32",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpgwxahnts/chatgpt-retrieval-plugin/services/openai.py",
      "line_number": 32,
      "code_snippet": "\n    response = {}\n    if deployment is None:\n        response = openai.Embedding.create(input=texts, model=EMBEDDING_MODEL, dimensions=EMBEDDING_DIMENSION)\n    else:\n        response = openai.Embedding.create(input=texts, deployment_id=deployment)\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpgwxahnts/chatgpt-retrieval-plugin/services/openai.py_34",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpgwxahnts/chatgpt-retrieval-plugin/services/openai.py",
      "line_number": 34,
      "code_snippet": "    if deployment is None:\n        response = openai.Embedding.create(input=texts, model=EMBEDDING_MODEL, dimensions=EMBEDDING_DIMENSION)\n    else:\n        response = openai.Embedding.create(input=texts, deployment_id=deployment)\n\n    # Extract the embedding data from the response\n    data = response[\"data\"]  # type: ignore",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpgwxahnts/chatgpt-retrieval-plugin/services/chunks.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpgwxahnts/chatgpt-retrieval-plugin/services/chunks.py",
      "line_number": 1,
      "code_snippet": "from typing import Dict, List, Optional, Tuple\nimport uuid\nimport os\nfrom models.models import Document, DocumentChunk, DocumentChunkMetadata\n\nimport tiktoken\n\nfrom services.openai import get_embeddings\n\n# Global variables",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpgwxahnts/chatgpt-retrieval-plugin/scripts/process_jsonl/process_jsonl.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpgwxahnts/chatgpt-retrieval-plugin/scripts/process_jsonl/process_jsonl.py",
      "line_number": 1,
      "code_snippet": "import uuid\nimport json\nimport argparse\nimport asyncio\n\nfrom loguru import logger\nfrom models.models import Document, DocumentMetadata\nfrom datastore.datastore import DataStore\nfrom datastore.factory import get_datastore\nfrom services.extract_metadata import extract_metadata_from_document",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpgwxahnts/chatgpt-retrieval-plugin/scripts/process_json/process_json.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpgwxahnts/chatgpt-retrieval-plugin/scripts/process_json/process_json.py",
      "line_number": 1,
      "code_snippet": "import uuid\nimport json\nimport argparse\nimport asyncio\n\nfrom loguru import logger\nfrom models.models import Document, DocumentMetadata\nfrom datastore.datastore import DataStore\nfrom datastore.factory import get_datastore\nfrom services.extract_metadata import extract_metadata_from_document",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpgwxahnts/chatgpt-retrieval-plugin/scripts/process_zip/process_zip.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpgwxahnts/chatgpt-retrieval-plugin/scripts/process_zip/process_zip.py",
      "line_number": 1,
      "code_snippet": "import uuid\nimport zipfile\nimport os\nimport json\nimport argparse\nimport asyncio\n\nfrom loguru import logger\nfrom models.models import Document, DocumentMetadata, Source\nfrom datastore.datastore import DataStore",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpgwxahnts/chatgpt-retrieval-plugin/examples/memory/main.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpgwxahnts/chatgpt-retrieval-plugin/examples/memory/main.py",
      "line_number": 1,
      "code_snippet": "# This is a version of the main.py file found in ../../server/main.py that also gives ChatGPT access to the upsert endpoint\n# (allowing it to save information from the chat back to the vector) database.\n# Copy and paste this into the main file at ../../server/main.py if you choose to give the model access to the upsert endpoint\n# and want to access the openapi.json when you run the app locally at http://0.0.0.0:8000/sub/openapi.json.\nimport os\nfrom typing import Optional\nimport uvicorn\nfrom fastapi import FastAPI, File, Form, HTTPException, Depends, Body, UploadFile\nfrom fastapi.security import HTTPBearer, HTTPAuthorizationCredentials\nfrom fastapi.staticfiles import StaticFiles",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpgwxahnts/chatgpt-retrieval-plugin/examples/authentication-methods/no-auth/main.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpgwxahnts/chatgpt-retrieval-plugin/examples/authentication-methods/no-auth/main.py",
      "line_number": 1,
      "code_snippet": "# This is a version of the main.py file found in ../../../server/main.py without authentication.\n# Copy and paste this into the main file at ../../../server/main.py if you choose to use no authentication for your retrieval plugin.\nfrom typing import Optional\nimport uvicorn\nfrom fastapi import FastAPI, File, Form, HTTPException, Body, UploadFile\nfrom fastapi.staticfiles import StaticFiles\nfrom loguru import logger\n\nfrom models.api import (\n    DeleteRequest,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpgwxahnts/chatgpt-retrieval-plugin/datastore/providers/zilliz_datastore.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpgwxahnts/chatgpt-retrieval-plugin/datastore/providers/zilliz_datastore.py",
      "line_number": 1,
      "code_snippet": "import os\n\nfrom loguru import logger\nfrom typing import Optional\nfrom pymilvus import (\n    connections,\n)\nfrom uuid import uuid4\n\nfrom datastore.providers.milvus_datastore import (",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpgwxahnts/chatgpt-retrieval-plugin/datastore/providers/pgvector_datastore.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpgwxahnts/chatgpt-retrieval-plugin/datastore/providers/pgvector_datastore.py",
      "line_number": 1,
      "code_snippet": "from abc import ABC, abstractmethod\nfrom typing import Any, Dict, List, Optional\nfrom datetime import datetime\nfrom loguru import logger\n\nfrom services.date import to_unix_timestamp\nfrom datastore.datastore import DataStore\nfrom models.models import (\n    DocumentChunk,\n    DocumentChunkMetadata,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpgwxahnts/chatgpt-retrieval-plugin/datastore/providers/chroma_datastore.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpgwxahnts/chatgpt-retrieval-plugin/datastore/providers/chroma_datastore.py",
      "line_number": 1,
      "code_snippet": "\"\"\"\nChroma datastore support for the ChatGPT retrieval plugin.\n\nConsult the Chroma docs and GitHub repo for more information:\n- https://docs.trychroma.com/usage-guide?lang=py\n- https://github.com/chroma-core/chroma\n- https://www.trychroma.com/\n\"\"\"\n\nimport os",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpgwxahnts/chatgpt-retrieval-plugin/datastore/providers/weaviate_datastore.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpgwxahnts/chatgpt-retrieval-plugin/datastore/providers/weaviate_datastore.py",
      "line_number": 1,
      "code_snippet": "import asyncio\nimport os\nimport re\nimport uuid\nfrom typing import Dict, List, Optional\n\nimport weaviate\nfrom loguru import logger\nfrom weaviate import Client\nfrom weaviate.util import generate_uuid5",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpgwxahnts/chatgpt-retrieval-plugin/datastore/providers/llama_datastore.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpgwxahnts/chatgpt-retrieval-plugin/datastore/providers/llama_datastore.py",
      "line_number": 1,
      "code_snippet": "import json\nimport os\nfrom typing import Dict, List, Optional, Type\nfrom loguru import logger\nfrom datastore.datastore import DataStore\nfrom models.models import (\n    DocumentChunk,\n    DocumentChunkMetadata,\n    DocumentChunkWithScore,\n    DocumentMetadataFilter,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpgwxahnts/chatgpt-retrieval-plugin/datastore/providers/redis_datastore.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpgwxahnts/chatgpt-retrieval-plugin/datastore/providers/redis_datastore.py",
      "line_number": 1,
      "code_snippet": "import asyncio\nimport os\nimport re\nimport json\nimport redis.asyncio as redis\nimport numpy as np\n\nfrom redis.commands.search.query import Query as RediSearchQuery\nfrom redis.commands.search.indexDefinition import IndexDefinition, IndexType\nfrom redis.commands.search.field import (",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpgwxahnts/chatgpt-retrieval-plugin/datastore/providers/analyticdb_datastore.py_68",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 68 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpgwxahnts/chatgpt-retrieval-plugin/datastore/providers/analyticdb_datastore.py",
      "line_number": 68,
      "code_snippet": "\n    def _create_table(self, cur: psycopg2.extensions.cursor):\n        cur.execute(\n            f\"\"\"\n              CREATE TABLE IF NOT EXISTS {self.collection_name} (",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpgwxahnts/chatgpt-retrieval-plugin/datastore/providers/analyticdb_datastore.py_85",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 85 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpgwxahnts/chatgpt-retrieval-plugin/datastore/providers/analyticdb_datastore.py",
      "line_number": 85,
      "code_snippet": "\n    def _create_embedding_index(self, cur: psycopg2.extensions.cursor):\n        cur.execute(\n            f\"\"\"\n            SELECT * FROM pg_indexes WHERE tablename='{self.collection_name}';",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpgwxahnts/chatgpt-retrieval-plugin/datastore/providers/analyticdb_datastore.py_95",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 95 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpgwxahnts/chatgpt-retrieval-plugin/datastore/providers/analyticdb_datastore.py",
      "line_number": 95,
      "code_snippet": "        )\n        if not index_exists:\n            cur.execute(\n                f\"\"\"\n                CREATE INDEX {self.collection_name}_embedding_idx",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpgwxahnts/chatgpt-retrieval-plugin/datastore/providers/analyticdb_datastore.py_147",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 147 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpgwxahnts/chatgpt-retrieval-plugin/datastore/providers/analyticdb_datastore.py",
      "line_number": 147,
      "code_snippet": "            with conn.cursor() as cur:\n                # Construct the SQL query and data\n                query = f\"\"\"\n                        INSERT INTO {self.collection_name} (id, content, embedding, document_id, source, source_id, url, author, created_at)\n                        VALUES (%s::text, %s::text, %s::real[], %s::text, %s::text, %s::text, %s::text, %s::text, %s::timestamp with time zone)",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpgwxahnts/chatgpt-retrieval-plugin/datastore/providers/analyticdb_datastore.py_177",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 177 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpgwxahnts/chatgpt-retrieval-plugin/datastore/providers/analyticdb_datastore.py",
      "line_number": 177,
      "code_snippet": "        def generate_query(query: QueryWithEmbedding) -> Tuple[str, List[Any]]:\n            embedding = \"[\" + \", \".join(str(x) for x in query.embedding) + \"]\"\n            q = f\"\"\"\n                SELECT\n                    id,",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpgwxahnts/chatgpt-retrieval-plugin/datastore/providers/analyticdb_datastore.py_285",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 285 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpgwxahnts/chatgpt-retrieval-plugin/datastore/providers/analyticdb_datastore.py",
      "line_number": 285,
      "code_snippet": "\n        if delete_all:\n            query = f\"DELETE FROM {self.collection_name} WHERE document_id LIKE %s;\"\n            return await execute_delete(query, [\"%\"])\n        elif ids:",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpgwxahnts/chatgpt-retrieval-plugin/datastore/providers/analyticdb_datastore.py_288",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 288 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpgwxahnts/chatgpt-retrieval-plugin/datastore/providers/analyticdb_datastore.py",
      "line_number": 288,
      "code_snippet": "            return await execute_delete(query, [\"%\"])\n        elif ids:\n            query = f\"DELETE FROM {self.collection_name} WHERE document_id IN ({','.join(['%s'] * len(ids))});\"\n            return await execute_delete(query, ids)\n        elif filter is not None:",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpgwxahnts/chatgpt-retrieval-plugin/datastore/providers/analyticdb_datastore.py_311",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 311 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpgwxahnts/chatgpt-retrieval-plugin/datastore/providers/analyticdb_datastore.py",
      "line_number": 311,
      "code_snippet": "        where_values = [value for value, _ in conditions if value]\n\n        query = f\"DELETE FROM {self.collection_name} WHERE {' AND '.join(where_conditions)};\"\n        return query, where_values",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpgwxahnts/chatgpt-retrieval-plugin/datastore/providers/analyticdb_datastore.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpgwxahnts/chatgpt-retrieval-plugin/datastore/providers/analyticdb_datastore.py",
      "line_number": 1,
      "code_snippet": "import os\nimport asyncio\nfrom typing import Dict, List, Optional, Tuple, Any\nfrom datetime import datetime\nfrom loguru import logger\n\nfrom psycopg2cffi import compat\n\ncompat.register()\nimport psycopg2",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpgwxahnts/chatgpt-retrieval-plugin/datastore/providers/mongodb_atlas_datastore.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpgwxahnts/chatgpt-retrieval-plugin/datastore/providers/mongodb_atlas_datastore.py",
      "line_number": 1,
      "code_snippet": "import os\nfrom typing import Dict, List, Any, Optional\nfrom loguru import logger\nfrom importlib.metadata import version\nfrom motor.motor_asyncio import AsyncIOMotorClient\nfrom pymongo.driver_info import DriverInfo\nfrom pymongo import UpdateOne\n\nfrom datastore.datastore import DataStore\nfrom functools import cached_property",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpgwxahnts/chatgpt-retrieval-plugin/datastore/providers/qdrant_datastore.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpgwxahnts/chatgpt-retrieval-plugin/datastore/providers/qdrant_datastore.py",
      "line_number": 1,
      "code_snippet": "import os\nimport uuid\nfrom typing import Dict, List, Optional\n\nfrom grpc._channel import _InactiveRpcError\nfrom qdrant_client.http.exceptions import UnexpectedResponse\nfrom qdrant_client.http.models import PayloadSchemaType\n\nfrom datastore.datastore import DataStore\nfrom models.models import (",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpgwxahnts/chatgpt-retrieval-plugin/datastore/providers/supabase_datastore.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpgwxahnts/chatgpt-retrieval-plugin/datastore/providers/supabase_datastore.py",
      "line_number": 1,
      "code_snippet": "import os\nfrom typing import Any, List\nfrom datetime import datetime\n\nfrom supabase import Client\n\nfrom datastore.providers.pgvector_datastore import PGClient, PgVectorDataStore\nfrom models.models import (\n    DocumentMetadataFilter,\n)",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpgwxahnts/chatgpt-retrieval-plugin/datastore/providers/azuresearch_datastore.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpgwxahnts/chatgpt-retrieval-plugin/datastore/providers/azuresearch_datastore.py",
      "line_number": 1,
      "code_snippet": "import asyncio\nimport base64\nimport os\nimport re\nimport time\nfrom typing import Dict, List, Optional, Union\n\nfrom azure.core.credentials import AzureKeyCredential\nfrom azure.identity import DefaultAzureCredential as DefaultAzureCredentialSync\nfrom azure.identity.aio import DefaultAzureCredential",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpgwxahnts/chatgpt-retrieval-plugin/datastore/providers/azurecosmosdb_datastore.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpgwxahnts/chatgpt-retrieval-plugin/datastore/providers/azurecosmosdb_datastore.py",
      "line_number": 1,
      "code_snippet": "import logging\nimport os\n\nimport certifi\nimport numpy as np\nimport pymongo\n\nfrom pymongo.mongo_client import MongoClient\nfrom abc import ABC, abstractmethod\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpgwxahnts/chatgpt-retrieval-plugin/datastore/providers/elasticsearch_datastore.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpgwxahnts/chatgpt-retrieval-plugin/datastore/providers/elasticsearch_datastore.py",
      "line_number": 1,
      "code_snippet": "import os\nfrom typing import Dict, List, Any, Optional\n\nimport elasticsearch\nfrom elasticsearch import Elasticsearch, helpers\nfrom loguru import logger\n\nfrom datastore.datastore import DataStore\nfrom models.models import (\n    DocumentChunk,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpgwxahnts/chatgpt-retrieval-plugin/datastore/providers/pinecone_datastore.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpgwxahnts/chatgpt-retrieval-plugin/datastore/providers/pinecone_datastore.py",
      "line_number": 1,
      "code_snippet": "import os\nfrom typing import Any, Dict, List, Optional\nimport pinecone\nfrom tenacity import retry, wait_random_exponential, stop_after_attempt\nimport asyncio\nfrom loguru import logger\n\nfrom datastore.datastore import DataStore\nfrom models.models import (\n    DocumentChunk,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpgwxahnts/chatgpt-retrieval-plugin/datastore/providers/postgres_datastore.py_131",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 131 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpgwxahnts/chatgpt-retrieval-plugin/datastore/providers/postgres_datastore.py",
      "line_number": 131,
      "code_snippet": "\n        with self.client.cursor() as cur:\n            cur.execute(f\"DELETE FROM {table} {filters}\")\n            self.client.commit()",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpgwxahnts/chatgpt-retrieval-plugin/datastore/providers/postgres_datastore.py_50",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 50 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpgwxahnts/chatgpt-retrieval-plugin/datastore/providers/postgres_datastore.py",
      "line_number": 50,
      "code_snippet": "            json[\"embedding\"] = np.array(json[\"embedding\"])\n            cur.execute(\n                f\"INSERT INTO {table} (id, content, embedding, document_id, source, source_id, url, author, created_at) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s) ON CONFLICT (id) DO UPDATE SET content = %s, embedding = %s, document_id = %s, source = %s, source_id = %s, url = %s, author = %s, created_at = %s\",\n                (\n                    json[\"id\"],",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpgwxahnts/chatgpt-retrieval-plugin/datastore/providers/postgres_datastore.py_94",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 94 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpgwxahnts/chatgpt-retrieval-plugin/datastore/providers/postgres_datastore.py",
      "line_number": 94,
      "code_snippet": "        with self.client.cursor() as cur:\n            cur.execute(\n                f\"DELETE FROM {table} WHERE {column} LIKE %s\",\n                (f\"%{pattern}%\",),\n            )",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpgwxahnts/chatgpt-retrieval-plugin/datastore/providers/postgres_datastore.py_105",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 105 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpgwxahnts/chatgpt-retrieval-plugin/datastore/providers/postgres_datastore.py",
      "line_number": 105,
      "code_snippet": "        with self.client.cursor() as cur:\n            cur.execute(\n                f\"DELETE FROM {table} WHERE {column} IN %s\",\n                (tuple(ids),),\n            )",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpgwxahnts/chatgpt-retrieval-plugin/datastore/providers/postgres_datastore.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpgwxahnts/chatgpt-retrieval-plugin/datastore/providers/postgres_datastore.py",
      "line_number": 1,
      "code_snippet": "import os\nfrom typing import Any, List\nfrom datetime import datetime\nimport numpy as np\n\nfrom psycopg2 import connect\nfrom psycopg2.extras import DictCursor\nfrom pgvector.psycopg2 import register_vector\n\nfrom services.date import to_unix_timestamp",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpgwxahnts/chatgpt-retrieval-plugin/datastore/providers/milvus_datastore.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpgwxahnts/chatgpt-retrieval-plugin/datastore/providers/milvus_datastore.py",
      "line_number": 1,
      "code_snippet": "import json\nimport os\nimport asyncio\n\nfrom loguru import logger\nfrom typing import Dict, List, Optional\nfrom pymilvus import (\n    Collection,\n    connections,\n    utility,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    }
  ],
  "metadata": {
    "fp_reduction": {
      "total": 71,
      "kept": 48,
      "filtered": 23,
      "reduction_pct": 32.4,
      "avg_tp_probability": 0.514,
      "filter_reasons": {
        "high severity with context": 23,
        "test file": 18,
        "build tool subprocess": 12,
        "server runner": 8,
        "example file": 4,
        "env variable reference": 1
      }
    }
  }
}